--- 
title: "Notes for Statistical Computing & Machine Learning"
author: "Daniel Kaplan"
date: "Math 253 Macalester College"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: rstudio/bookdown-demo
description: "Notes and other materials for Math 253 at Macalester College."
---

# Placeholder

<!--chapter:end:index.Rmd-->


# Preface {-} 

<!--chapter:end:010-Preface.Rmd-->


# Introduction {#introduction}

<!--chapter:end:110-Introduction.Rmd-->


# (PART) Topic I: Linear Regression {-}
# Placeholder

<!--chapter:end:200-Linear-Regression.Rmd-->


# Notes

<!--chapter:end:210-Linear-Regression.Rmd-->


# Foundations: linear algebra, likelihood and Bayes' rule

<!--chapter:end:310-Foundations.Rmd-->


# Classifiers

<!--chapter:end:410-Classifiers.Rmd-->

---
title: "Logistic Regression"
---


```{r include = FALSE}
library(mosaic)
library(ggplot2)
library(ISLR)
library(statisticalModeling)
library(ggformula)
```


## Probability and odds

Probability $p(event)$ is a number between zero and one.

Simple way to make a probability model for yes/no variable: encode outcome as zero and one, use regression.
```{r}
Whickham$alive <- as.numeric(with(Whickham, outcome == "Alive"))
```

Model of mortality in Whickham
```{r}
res <- mean( alive ~ smoker, data=Whickham)
res
res / (1-res)
```

```{r}
mod2 <- glm(alive ~ age, data=Whickham, family = "binomial")

gmodel(mod2) %>%
  gf_jitter(alive ~ age, data = Whickham, alpha = .2)
plotFun(f(age) ~ age, age.lim = c(20,100))
plotPoints(jitter(alive) ~ age, data=Whickham, add=TRUE,
           pch=20, alpha=.3) 
```

If we're going to use likelihood to fit, the estimated probability can't be $\leq 0$.

## Log Odds

Gerolamo Cardano (1501-1576) defined *odds* as the ratio of favorable to unfavorable outcomes.

For an event whose *probability* is $p$, it's *odds* are $w = \frac{p}{1-p}$.

A probability is a number between 0 and one.

An odds is a ratio of two positive numbers. 5:9, 9:5, etc.

"Odds are against it," could be taken to mean that the odds is less than 1.  More unfavorable outcomes than favorable ones.

Given odds $w$, the probability is $p = \frac{w}{1+w}$.  There's a one-to-one correspondence between probability and odds.

The log odds is a number between $-\infty$ and $\infty$.  

## Why use odds?

**Making Book**

Several horses in a race.  People bet on each one amounts $H_i$.

What should be the winnings when horse $j$ wins? Payoff means you get your original stake back plus your winnings.

If it's arranged to pay winnings of     
$\sum{i \neq j} \frac{H_i}{H_j}$ + the amount $H_j$   
the net income will be zero for the bookie.

*Shaving the odds* means to pay less than the zero-net-income winnings.  

**Link function**

You can build a linear regression to predict the log odds, $\ln w$.  The output of the linear regression is free to range from $-\infty$ to $\infty$.  Then, to measure likelihood, unlog to get odds $w$, then $p = \frac{w}{1+w}$.

## Use of glm() 

Response should be 0 or 1.  We don't take the log odds of the response.  Instead, the likelihood is    
- $p$ if the outcome is 1
- $1-p$ if the outcome is 0

Multiply these together of all the cases to get the total likelihood.

## Interpretation of coefficients

Each adds to the log odds in the normal, linear regression way.  Negative means less likely; positive more likely.  

## Example: Logistic regression of default

```{r}
names(Default)
ggplot(Default, 
       aes(x = income, y = balance, alpha = default, color = default)) + 
  geom_point() #+ facet_wrap( ~ student)
```

```{r fig.keep = "all"}
model_of_default <-
  glm(default == "Yes" ~ balance + income, data = Default, family = "binomial")
gmodel(model_of_default)
gmodel(model_of_default, ~ income + balance)

summary(model_of_default)

logodds <- predict(model_of_default, newdata = list(balance = 1000, income = 40000)) #,
                   # type = "response")
logodds
odds <- exp(logodds)
odds / (1 + odds)
logistic <- function(x) {exp(x) / (1 + exp(x))}
logistic(-3.36)
table(Default$default)
```














<!--chapter:end:411-Logistic-Regression.Rmd-->


# Linear and Quadratic Discriminant Analysis

<!--chapter:end:412-LDA-QDA.Rmd-->


# Cross-Validation and Bootstrapping

<!--chapter:end:510-Cross-Validation.Rmd-->


# Regularization, shrinkage and dimension reduction

<!--chapter:end:610-Regularization-Shrinkage.Rmd-->


# Nonlinearity in linear models

<!--chapter:end:710-nonlinear-functions.Rmd-->


# Trees for Regression and Classification

<!--chapter:end:711-Trees.Rmd-->


# Support Vector Classifiers

<!--chapter:end:715-support-vector-classifiers.Rmd-->


# Programming Basics

<!--chapter:end:850-Programming.Rmd-->


# Appendices {-}

<!--chapter:end:902-Appendices.Rmd-->


# Connecting RStudio to your GitHub repository {-}

<!--chapter:end:910-github.Rmd-->


# Instructions for the publishing system: Bookdown {-}

<!--chapter:end:950-Bookdown-Instructions.Rmd-->

