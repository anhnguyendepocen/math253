<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Notes for Math 253: Statistical Computing and Machine Learning</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Notes and other materials for Math 253 at Macalester College.">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Notes for Math 253: Statistical Computing and Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notes and other materials for Math 253 at Macalester College." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Notes for Math 253: Statistical Computing and Machine Learning" />
  
  <meta name="twitter:description" content="Notes and other materials for Math 253 at Macalester College." />
  

<meta name="author" content="Daniel Kaplan">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="placeholder-1.html">
<link rel="next" href="foundations-linear-algebra-likelihood-and-bayes-rule.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Math 253 Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Placeholder</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a></li>
<li class="part"><span><b>I Topic I: Linear Regression</b></span></li>
<li class="chapter" data-level="3" data-path="placeholder-1.html"><a href="placeholder-1.html"><i class="fa fa-check"></i><b>3</b> Placeholder</a></li>
<li class="chapter" data-level="4" data-path="notes.html"><a href="notes.html"><i class="fa fa-check"></i><b>4</b> Notes</a><ul>
<li class="chapter" data-level="4.1" data-path="notes.html"><a href="notes.html#review-of-day-4-sept-15-2016"><i class="fa fa-check"></i><b>4.1</b> Review of Day 4, Sept 15, 2016</a></li>
<li class="chapter" data-level="4.2" data-path="notes.html"><a href="notes.html#regression-and-interpretability"><i class="fa fa-check"></i><b>4.2</b> Regression and Interpretability</a></li>
<li class="chapter" data-level="4.3" data-path="notes.html"><a href="notes.html#toward-an-automated-regression-process"><i class="fa fa-check"></i><b>4.3</b> Toward an automated regression process</a></li>
<li class="chapter" data-level="4.4" data-path="notes.html"><a href="notes.html#selecting-model-terms"><i class="fa fa-check"></i><b>4.4</b> Selecting model terms</a></li>
<li class="chapter" data-level="4.5" data-path="notes.html"><a href="notes.html#programming-basics-graphics"><i class="fa fa-check"></i><b>4.5</b> Programming basics: Graphics</a></li>
<li class="chapter" data-level="4.6" data-path="notes.html"><a href="notes.html#in-class-programming-activity"><i class="fa fa-check"></i><b>4.6</b> In-class programming activity</a></li>
<li class="chapter" data-level="4.7" data-path="notes.html"><a href="notes.html#day-5-summary"><i class="fa fa-check"></i><b>4.7</b> Day 5 Summary</a><ul>
<li class="chapter" data-level="4.7.1" data-path="notes.html"><a href="notes.html#linear-regression"><i class="fa fa-check"></i><b>4.7.1</b> Linear regression</a></li>
<li class="chapter" data-level="4.7.2" data-path="notes.html"><a href="notes.html#coefficients-as-quantities"><i class="fa fa-check"></i><b>4.7.2</b> Coefficients as quantities</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="notes.html"><a href="notes.html#in-class-programming-activity-1"><i class="fa fa-check"></i><b>4.8</b> In-class programming activity</a></li>
<li class="chapter" data-level="4.9" data-path="notes.html"><a href="notes.html#day-6-summary"><i class="fa fa-check"></i><b>4.9</b> Day 6 Summary</a></li>
<li class="chapter" data-level="4.10" data-path="notes.html"><a href="notes.html#measuring-accuracy-of-the-model"><i class="fa fa-check"></i><b>4.10</b> Measuring Accuracy of the Model</a></li>
<li class="chapter" data-level="4.11" data-path="notes.html"><a href="notes.html#bias-of-the-model"><i class="fa fa-check"></i><b>4.11</b> Bias of the model</a><ul>
<li class="chapter" data-level="4.11.1" data-path="notes.html"><a href="notes.html#theory-of-whole-model-anova."><i class="fa fa-check"></i><b>4.11.1</b> Theory of whole-model ANOVA.</a></li>
</ul></li>
<li class="chapter" data-level="4.12" data-path="notes.html"><a href="notes.html#forward-backward-and-mixed-selection"><i class="fa fa-check"></i><b>4.12</b> Forward, backward and mixed selection</a></li>
<li class="chapter" data-level="4.13" data-path="notes.html"><a href="notes.html#programming-basics-functions"><i class="fa fa-check"></i><b>4.13</b> Programming Basics: Functions</a></li>
<li class="chapter" data-level="4.14" data-path="notes.html"><a href="notes.html#in-class-programming-activity-2"><i class="fa fa-check"></i><b>4.14</b> In-class programming activity</a></li>
<li class="chapter" data-level="4.15" data-path="notes.html"><a href="notes.html#review-of-day-7"><i class="fa fa-check"></i><b>4.15</b> Review of Day 7</a></li>
<li class="chapter" data-level="4.16" data-path="notes.html"><a href="notes.html#using-predict-to-calculate-precision"><i class="fa fa-check"></i><b>4.16</b> Using predict() to calculate precision</a></li>
<li class="chapter" data-level="4.17" data-path="notes.html"><a href="notes.html#conclusion"><i class="fa fa-check"></i><b>4.17</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="foundations-linear-algebra-likelihood-and-bayes-rule.html"><a href="foundations-linear-algebra-likelihood-and-bayes-rule.html"><i class="fa fa-check"></i><b>5</b> Foundations: linear algebra, likelihood and Bayes’ rule</a></li>
<li class="chapter" data-level="6" data-path="classifiers.html"><a href="classifiers.html"><i class="fa fa-check"></i><b>6</b> Classifiers</a></li>
<li class="chapter" data-level="7" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>7</b> Logistic Regression</a></li>
<li class="chapter" data-level="8" data-path="linear-and-quadratic-discriminant-analysis.html"><a href="linear-and-quadratic-discriminant-analysis.html"><i class="fa fa-check"></i><b>8</b> Linear and Quadratic Discriminant Analysis</a></li>
<li class="chapter" data-level="9" data-path="cross-validation-and-bootstrapping.html"><a href="cross-validation-and-bootstrapping.html"><i class="fa fa-check"></i><b>9</b> Cross-Validation and Bootstrapping</a></li>
<li class="chapter" data-level="10" data-path="regularization-shrinkage-and-dimension-reduction.html"><a href="regularization-shrinkage-and-dimension-reduction.html"><i class="fa fa-check"></i><b>10</b> Regularization, shrinkage and dimension reduction</a></li>
<li class="chapter" data-level="11" data-path="nonlinearity-in-linear-models.html"><a href="nonlinearity-in-linear-models.html"><i class="fa fa-check"></i><b>11</b> Nonlinearity in linear models</a></li>
<li class="chapter" data-level="12" data-path="trees-for-regression-and-classification.html"><a href="trees-for-regression-and-classification.html"><i class="fa fa-check"></i><b>12</b> Trees for Regression and Classification</a></li>
<li class="chapter" data-level="13" data-path="support-vector-classifiers.html"><a href="support-vector-classifiers.html"><i class="fa fa-check"></i><b>13</b> Support Vector Classifiers</a></li>
<li class="chapter" data-level="14" data-path="programming-basics.html"><a href="programming-basics.html"><i class="fa fa-check"></i><b>14</b> Programming Basics</a></li>
<li class="chapter" data-level="" data-path="appendices.html"><a href="appendices.html"><i class="fa fa-check"></i>Appendices</a></li>
<li class="chapter" data-level="" data-path="connecting-rstudio-to-your-github-repository.html"><a href="connecting-rstudio-to-your-github-repository.html"><i class="fa fa-check"></i>Connecting RStudio to your GitHub repository</a></li>
<li class="chapter" data-level="" data-path="instructions-for-the-publishing-system-bookdown.html"><a href="instructions-for-the-publishing-system-bookdown.html"><i class="fa fa-check"></i>Instructions for the publishing system: Bookdown</a></li>
<li class="divider"></li>
<li><a href="https://github.com/dtkaplan/math253" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes for Math 253: Statistical Computing and Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="notes" class="section level1">
<h1><span class="header-section-number">Topic 4</span> Notes</h1>
<div id="review-of-day-4-sept-15-2016" class="section level2">
<h2><span class="header-section-number">4.1</span> Review of Day 4, Sept 15, 2016</h2>
<ul>
<li>Discussed the linear regression architecture and how it relates to machine learning.
<ul>
<li>linear regression is designed to work even in “small data” situations where
<ol style="list-style-type: lower-alpha">
<li>cross-validation is not appropriate</li>
<li>the number of model degrees of freedom may be almost as large as <span class="math inline">\(n\)</span></li>
</ol></li>
<li>provides a ready definition of the “size” of a model: the number of coefficients.</li>
</ul></li>
<li>Introduced the main software used in linear regression, <code>lm()</code> and <code>predict()</code></li>
<li>Programming basics: indexing of vectors, matrices and data frames.</li>
</ul>
</div>
<div id="regression-and-interpretability" class="section level2">
<h2><span class="header-section-number">4.2</span> Regression and Interpretability</h2>
<p>Regression models are generally constructed for the sake of interpretability:</p>
<ul>
<li>Global linearity</li>
<li>Coefficients are indication of effect size. The coefficients have physical units.</li>
<li>Term by term indication of statistical significance</li>
</ul>
<p>An example on <code>College</code> data from <code>ISLR</code> package</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(College, <span class="dt">package=</span><span class="st">&quot;ISLR&quot;</span>)
College$Yield &lt;-<span class="st"> </span><span class="kw">with</span>(College, Enroll/Accept)
mod1 &lt;-<span class="st"> </span><span class="kw">lm</span>(Yield ~<span class="st"> </span>Outstate +<span class="st"> </span>Grad.Rate +<span class="st"> </span>Top25perc, <span class="dt">data =</span> College)
mosaic::<span class="kw">rsquared</span>(mod1)</code></pre></div>
<pre><code>## [1] 0.2170221</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod2 &lt;-<span class="st"> </span><span class="kw">lm</span>(Yield ~<span class="st"> </span>. -<span class="st"> </span>Grad.Rate, <span class="dt">data =</span> College)
mosaic::<span class="kw">rsquared</span>(mod2)</code></pre></div>
<pre><code>## [1] 0.5004599</code></pre>
<ul>
<li>What variables matter?</li>
<li>How good are the predictions?</li>
<li>How strong are the effects?</li>
</ul>
</div>
<div id="toward-an-automated-regression-process" class="section level2">
<h2><span class="header-section-number">4.3</span> Toward an automated regression process</h2>
<p>In machine learning, we ask the computer to identify patterns in the data.</p>
<ul>
<li>In “traditional” regression (which is still very important), we specify the explanatory terms and the computer finds the “best” model with those terms: least squares.</li>
<li>In machine learning, we want the computer to figure out which terms, of all the possibilities, will lead to the “best” model.</li>
</ul>
</div>
<div id="selecting-model-terms" class="section level2">
<h2><span class="header-section-number">4.4</span> Selecting model terms</h2>
<p>The regression techniques</p>
<ul>
<li>Traditional regression:
<ul>
<li>Our knowledge of the system being studied.</li>
<li>Heirarchical principal
<ul>
<li>main effects, then</li>
<li>interaction.</li>
</ul></li>
</ul></li>
<li>Machine learning
<ul>
<li>Look at all combinations of variables?</li>
<li>Activity 1:
<ul>
<li>Write a statement that will pull 2 random variables from a data frame <em>and</em> the explanatory variable.</li>
<li>Use <code>Yield ~ .</code> as the formula.</li>
</ul></li>
</ul></li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">explanatory_vars &lt;-<span class="st"> </span><span class="kw">names</span>(College)[-<span class="dv">19</span>]

my_vars &lt;-<span class="st"> </span><span class="kw">sample</span>(explanatory_vars, <span class="dt">size =</span> <span class="dv">12</span>)
new_data &lt;-<span class="st"> </span>College[ , <span class="kw">c</span>(my_vars, <span class="st">&quot;Yield&quot;</span>)]
mod &lt;-<span class="st"> </span><span class="kw">lm</span>(Yield ~<span class="st"> </span>., <span class="dt">data =</span> new_data)
mosaic::<span class="kw">rsquared</span>(mod)</code></pre></div>
<pre><code>## [1] 0.4820961</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Yield ~ ., data = new_data)
## 
## Coefficients:
## (Intercept)         Apps    Top10perc     Outstate       Enroll  
##   5.580e-01    1.212e-05    1.049e-03   -1.043e-05    1.741e-04  
##         PhD     Terminal    Top25perc       Accept    S.F.Ratio  
##  -5.869e-04   -4.375e-04   -4.634e-04   -8.551e-05    1.272e-03  
##  PrivateYes  perc.alumni        Books  
##   8.053e-03   -1.104e-04    3.029e-05</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">2</span>^<span class="dv">18</span></code></pre></div>
<pre><code>## [1] 262144</code></pre>
<ul>
<li>Activity 2: * How many combinations are there of <span class="math inline">\(k\)</span> explanatory variables? Calculate this for <span class="math inline">\(k = 5, 10, 15, 20\)</span>. How many are there in the <code>College</code>? * What about with interactions?</li>
<li>How long does it take to fit a model?</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">system.time</span>( <span class="kw">do</span>(<span class="dv">1000</span>) *<span class="st"> </span><span class="kw">lm</span>(Yield ~<span class="st"> </span>., <span class="dt">data=</span>College))</code></pre></div>
<pre><code>##    user  system elapsed 
##   6.060   0.038   6.122</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">256</span>*<span class="dv">7</span>/<span class="dv">3600</span></code></pre></div>
<pre><code>## [1] 0.4977778</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">names</span>(College)</code></pre></div>
<pre><code>##  [1] &quot;Private&quot;     &quot;Apps&quot;        &quot;Accept&quot;      &quot;Enroll&quot;      &quot;Top10perc&quot;  
##  [6] &quot;Top25perc&quot;   &quot;F.Undergrad&quot; &quot;P.Undergrad&quot; &quot;Outstate&quot;    &quot;Room.Board&quot; 
## [11] &quot;Books&quot;       &quot;Personal&quot;    &quot;PhD&quot;         &quot;Terminal&quot;    &quot;S.F.Ratio&quot;  
## [16] &quot;perc.alumni&quot; &quot;Expend&quot;      &quot;Grad.Rate&quot;   &quot;Yield&quot;</code></pre>
<p>With <span class="math inline">\(k\)</span> explanatory variables, <span class="math inline">\(2^k\)</span> possibilities, not even including interactions. Including first-order interactions, it’s <span class="math inline">\(2^k + 2^{k(k-1)/2}\)</span>. Calculate this for $k=3. - Increase in <span class="math inline">\(R^2\)</span>? Problem: <span class="math inline">\(R^2\)</span> will always go up as we add a new term. - Some other measure that takes into account how much <span class="math inline">\(R^2\)</span> should go up.</p>
</div>
<div id="programming-basics-graphics" class="section level2">
<h2><span class="header-section-number">4.5</span> Programming basics: Graphics</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="dv">1</span>, <span class="dt">type =</span> <span class="st">&quot;n&quot;</span>, <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">100</span>,<span class="dv">200</span>), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">300</span>,<span class="dv">500</span>))</code></pre></div>
<p><img src="Class_Notes__Statistical_Computing___Machine_Learning_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Basic functions:</p>
<ol style="list-style-type: decimal">
<li>Create a frame: <code>plot()</code>. Blank frame: <code>plot( , type=&quot;n&quot;)</code>
<ul>
<li>set axis limits,</li>
</ul></li>
<li>Dots: <code>points(x, y)</code>, <code>pch=20</code></li>
<li>Lines: <code>lines(x, y)</code> — with <code>NA</code> for line breaks</li>
<li>Polygons: <code>polygon(x, y)</code> — like lines but connects first to last.
<ul>
<li>fill</li>
</ul></li>
<li>Color, size, … <code>rgb(r, g, b, alpha)</code>, “tomato”</li>
</ol>
</div>
<div id="in-class-programming-activity" class="section level2">
<h2><span class="header-section-number">4.6</span> In-class programming activity</h2>
<p><a href="../Daily-Programming/Day-4-Programming-Task.pdf">Programming Activity 4</a>: Drawing a histogram.</p>
</div>
<div id="day-5-summary" class="section level2">
<h2><span class="header-section-number">4.7</span> Day 5 Summary</h2>
<div id="linear-regression" class="section level3">
<h3><span class="header-section-number">4.7.1</span> Linear regression</h3>
<ul>
<li>Discussed “interpretability” of linear models, e.g. meaning of coefficients, confidence intervals, R^2, etc.
<ul>
<li>which variables are “important” via ANOVA and mean sum of squares</li>
</ul></li>
<li>Discussed metrics to compare models
<ul>
<li>R^2 – not fair, since “bigger” models are always better</li>
<li>Punishment: Two criteria for judging
<ul>
<li>R^2</li>
<li>How big the model is.</li>
<li>These two are somehow combined together into “adjusted R^2.” We’ll say more about that today.</li>
</ul></li>
<li>Cross-validation. Judge each model on its “out of sample” prediction performance.</li>
</ul></li>
</ul>
</div>
<div id="coefficients-as-quantities" class="section level3">
<h3><span class="header-section-number">4.7.2</span> Coefficients as quantities</h3>
<p>Coefficients in linear models are not just numbers, they are physical quantities with dimensions and units.</p>
<ul>
<li>Dimensions are always (dim of response)/(dim of this term)</li>
<li>The model doesn’t depend on the units of these quantities. The units only set the magnitude to the numerical part of the coefficient, but as a quantity a coefficient is the same thing regardless of units.</li>
<li>Conversion from one unit to another by multiplying by 1, but expressed in different units, e.g. 60 seconds per minute, 2.2 pounds per kilogram.</li>
</ul>
</div>
</div>
<div id="in-class-programming-activity-1" class="section level2">
<h2><span class="header-section-number">4.8</span> In-class programming activity</h2>
<p><a href="../Daily-Programming/Day-05-Programming-Task.pdf">Day 5 activity</a></p>
<p>Drawing a histogram.</p>
</div>
<div id="day-6-summary" class="section level2">
<h2><span class="header-section-number">4.9</span> Day 6 Summary</h2>
<ul>
<li><span class="math inline">\(R^2\)</span>
<ul>
<li>var(fitted) / var(response)</li>
<li>partitioning of variance:
<ul>
<li>var(fitted) + var(resid) = var(response)</li>
<li>same with sum of squares: SS(fitted) + SS(resid) = SS(response)</li>
</ul></li>
</ul></li>
<li>Adjusted <span class="math inline">\(R^2\)</span>
<ul>
<li><span class="math inline">\(R^2\)</span> vs <span class="math inline">\(p\)</span> picture</li>
<li>Derive a formula from the picture: we’ve got <span class="math inline">\(p+1\)</span> df to get from <span class="math inline">\(R^2 = 0\)</span> to our observed <span class="math inline">\(R^2\)</span>, so <span class="math inline">\(n - (p+1)\)</span> df left for the residuals. Rate of increase due to junk is <span class="math inline">\((1 - R^2) / (n - p - 1)\)</span>. Projecting back <span class="math inline">\(p\)</span> terms gives an adjustment of <span class="math inline">\((1 - R^2) / \frac{p}{n - p - 1}\)</span>. Subtract this from <span class="math inline">\(R^2\)</span>.</li>
<li>Wikipedia gives two formulas:
<ul>
<li><span class="math inline">\(Adj R^2 = {1-(1-R^{2}){n-1 \over n-p-1}}\)</span> – this projects back <span class="math inline">\(n-1\)</span> terms from 1.</li>
<li><span class="math inline">\(Adj R^2 = {R^{2}-(1-R^{2}){p \over n-p-1}}\)</span> — projects back <span class="math inline">\(p\)</span> terms from <span class="math inline">\(R^2\)</span>.</li>
</ul></li>
</ul></li>
<li>Adjusted <span class="math inline">\(R^2\)</span></li>
<li>Whole model ANOVA.</li>
<li>ANOVA on model parts</li>
</ul>
</div>
<div id="measuring-accuracy-of-the-model" class="section level2">
<h2><span class="header-section-number">4.10</span> Measuring Accuracy of the Model</h2>
<ul>
<li><span class="math inline">\(R^2\)</span> = Var(fitted)/Var(response)</li>
<li>Adjusted <span class="math inline">\(R^2\)</span> - takes into account estimate of average increase in <span class="math inline">\(R^2\)</span> per junk degree of freedom</li>
<li>Residual Standard Error - Sqrt of average square error per residual degree of freedom. The sqrt of the mean square for residuals in ANOVA.</li>
</ul>
</div>
<div id="bias-of-the-model" class="section level2">
<h2><span class="header-section-number">4.11</span> Bias of the model</h2>
<p>You need to know the “truth” to calculate the bias. We don’t.</p>
<p><img src="Images/Chapter-2/2.1.png" width="400" /></p>
<ul>
<li>Perhaps effect of TV goes as sqrt(money) as media get saturated?</li>
<li>Perhaps there is a synergy that wasn’t included in the model?</li>
</ul>
<div id="theory-of-whole-model-anova." class="section level3">
<h3><span class="header-section-number">4.11.1</span> Theory of whole-model ANOVA.</h3>
<p>Standard measure: <span class="math inline">\(\frac{\mbox{Explained amount}}{\mbox{Unexplained amount}}\)</span></p>
<p>Examples:</p>
<ul>
<li>Standard error of mean: <span class="math inline">\(\frac{\hat{\mu}}{\sigma / n}\)</span> – note the <span class="math inline">\(n\)</span>.</li>
<li>t statistic on difference between two means: <span class="math inline">\(\frac{\hat{\mu}_1 - \hat{\mu}_2}{\sigma / (n-1)}\)</span></li>
<li>F statistic: <span class="math inline">\(\frac{SS / df1}{SSR / df2}\)</span>
<ul>
<li>df1 is the number of degrees of freedom involved by the model or model term under consideration.</li>
<li>df2 is <span class="math inline">\(n - (p - 1)\)</span> where <span class="math inline">\(p\)</span> is the total degrees of freedom in the model. (I called this <span class="math inline">\(m\)</span> in the Math 155 book.) The intercept is what the <span class="math inline">\(-1\)</span> is about: the intercept <em>can never</em> account for case-to-case variation.</li>
</ul></li>
</ul>
<p>Trade-off between eating variance and consuming degrees of freedom.</p>
</div>
</div>
<div id="forward-backward-and-mixed-selection" class="section level2">
<h2><span class="header-section-number">4.12</span> Forward, backward and mixed selection</h2>
<p>Use the <code>College</code> model to demonstrate each of the approaches by hand. Start with <code>pairs()</code> or write an <code>lapply()</code> for the correlation with <code>Yield</code>?</p>
<p>Create a whole bunch of model terms</p>
<ul>
<li>“main” effects</li>
<li>“interaction” effects</li>
<li>nonlinear transformations: powers, logs, sqrt, steps, …</li>
<li>categorical variables</li>
</ul>
<p>Result: a set of <span class="math inline">\(k\)</span> vectors that we’re interested to use in our model.</p>
<p>Considerations:</p>
<ul>
<li>not all of the <span class="math inline">\(k\)</span> vectors may pull their weight</li>
<li>two or more vectors may overlap in how they eat up variance</li>
</ul>
<p>Algorithmic approaches:</p>
<ul>
<li>Try all combinations, pick the best one.
<ul>
<li>computationally expensive/impossible <span class="math inline">\(2^k\)</span> possibilities</li>
<li>what’s the sensitivity of the process to the choice of training data?</li>
</ul></li>
<li>“Greedy” approaches</li>
</ul>
</div>
<div id="programming-basics-functions" class="section level2">
<h2><span class="header-section-number">4.13</span> Programming Basics: Functions</h2>
<ol style="list-style-type: decimal">
<li><p>Syntax of functions:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">name &lt;-<span class="st"> </span>function(arg1, arg2, ...) {
  body of function. Can use arg1, arg2, etc. 
}</code></pre></div></li>
</ol>
<ul>
<li>typically you will return a value. The value calculated by the last command line in the body is what’s returned. Or you can use <code>return()</code> at any point in the function.</li>
<li>Often functions are designed to produce “side effects”, e.g. graphics.<br />
</li>
<li>Scope: what happens in functions stays in functions.</li>
</ul>
<ol style="list-style-type: decimal">
<li>Create a plotting frame: <code>plot()</code>
<ul>
<li>Write a function that makes this more convenient to use. What features would you like.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">blank_frame &lt;-<span class="st"> </span>function(xlim, ylim) {

}</code></pre></div></li>
<li>Write a function to draw a circle.
<ul>
<li>What do you want the interface to look like? What arguments are essential? What options are nice to have?</li>
</ul></li>
</ol>
</div>
<div id="in-class-programming-activity-2" class="section level2">
<h2><span class="header-section-number">4.14</span> In-class programming activity</h2>
<p>Histogram and density functions</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">101</span>)
n =<span class="st"> </span><span class="dv">20</span>
X &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">vals =</span> <span class="kw">runif</span>(n), 
                <span class="dt">group =</span> <span class="kw">as.character</span>((<span class="dv">1</span>:n) %%<span class="st"> </span><span class="dv">2</span>))
<span class="kw">ggplot</span>(<span class="kw">head</span>(X, <span class="dv">6</span>), <span class="kw">aes</span>(<span class="dt">x =</span> vals)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">bw =</span> <span class="dv">1</span>, <span class="dt">position =</span> <span class="st">&quot;stack&quot;</span>, <span class="kw">aes</span>(<span class="dt">color =</span> group, <span class="dt">fill =</span> group)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_rug</span>(<span class="kw">aes</span>(<span class="dt">color =</span> group)) +<span class="st"> </span>
<span class="st"> </span><span class="co"># facet_grid( . ~ group) +</span>
<span class="st">  </span><span class="kw">xlim</span>(-<span class="fl">0.5</span>, <span class="fl">1.5</span>)</code></pre></div>
<p><img src="Class_Notes__Statistical_Computing___Machine_Learning_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p><a href="../Daily-Programming/Day-06-Programming-Task.pdf">Day 6 activity</a></p>
</div>
<div id="review-of-day-7" class="section level2">
<h2><span class="header-section-number">4.15</span> Review of Day 7</h2>
<ul>
<li>We finished reviewing adjusted R^2 and ANOVA.</li>
<li>Started talking about linear algebra.</li>
</ul>
<p>We only got through the first few elements in our review of linear algebra. Let’s go through them again</p>
</div>
<div id="using-predict-to-calculate-precision" class="section level2">
<h2><span class="header-section-number">4.16</span> Using predict() to calculate precision</h2>
<ul>
<li>confidence intervals</li>
<li>prediction intervals</li>
</ul>
</div>
<div id="conclusion" class="section level2">
<h2><span class="header-section-number">4.17</span> Conclusion</h2>
<p>This wraps up our look at linear regression. Main points:</p>
<ul>
<li>model output is a linear combination of the inputs.</li>
<li><code>lm()</code> finds the “best” linear combination.</li>
<li>rich theory relating to precision of coefficients and the residuals.</li>
<li>traditional ways of applying that theory: F tests and t tests.</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="placeholder-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="foundations-linear-algebra-likelihood-and-bayes-rule.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dtkaplan/math253/edit/master/210-Linear-Regression.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
