[
["preface.html", "Notes for Statistical Computing &amp; Machine Learning Preface Math 253 and the Macalester statistics curriculum These notes are written in Bookdown", " Notes for Statistical Computing &amp; Machine Learning Daniel Kaplan Math 253 Macalester College Preface Math 253 and the Macalester statistics curriculum Math 253, Statistical Computing and Machine Learning, is a course at Macalester College. The course comes early in our curriculum for the statistics major. Currently, that curriculum looks like: Comp 110, Data Computing, a no-prerequisite course in data wrangling and visualization. The main tools in the course are R, dplyr, and ggplot. Math 125, Epidemiology. This course also has no pre-requisites. It’s designed to teach quantitative and statistical literacy in the context of public health and decision making. Unlike the other courses in our curriculum, Math 125 is not computer intensive. This is an elective for the statistics major. Math 155, Introduction to Statistical Modeling. This is our main entry-level statistics course. It also has no pre-requisites, but we encourage students to take a course in our calculus sequence: Applied Multivariate Calculus I, II, and III. The course covers important concepts in modeling: model architectures and fitting using mainly linear and logistic regression, covariation and adjustment, interpretation of model coefficients, inference techniques such as analysis of variance and co-variance and the ways these techniques can inform the decisions needed to build useful models, causality including techniques for making reasonable conclusions about causation from observational data. The course makes very extensive and intensive use of R and the mosaic package for R. This course, Math 253, Statistical Computing and Machine Learning. Math 253 introduces a broader set of model architectures (e.g. those associated with “machine learning” such as support vector machines) and the trade-offs that make machine learning a human skill rather than a push button mechanism. The main text is Introduction to Statistical Learning in R. Computing in R is used intensively in the course. Math 155 is a pre-requisite. The computing used in that course is the only pre-requisite. A small part of Math 253 is given to basic programming in R, but the large majority is about the mathematical and statistical concepts involved in learning from data and the exercise of a variety of architectures for classification and regression models. Math 253 is placed intentionally in the middle of our statistics curriculum to make it accessible to many students who are interested and may have use for the techniques, but whom are not primarily interested in statistics per se. Upper level courses including: Math 454: Bayesian statistics Math 453: Biostatistics Math 454: Mathematical statistics Supporting courses including: Math 354: Probability Math 135: Applied Multivariate Calculus I Math 236: Linear algebra Comp 123: Core conceptsin computer science Electives such as Math 432: Mathematical modeling Math/Comp 365: Numerical linear algebra Comp 302: Introduction to database management systems These notes are written in Bookdown The document uses an elaboration on R/Markdown, called “Bookdown.” I don’t yet know if this will be a good way to maintain and distribute class notes. Some advantages: All the notes are in one place. Students and other instructors can clone the notes for their own uses. People spotting mistakes can contribute corrections via the “pull request” mechanism supported by Bookdown working with GitHub. Multiple instructors can contribute to the notes via GitHub. This is the stated purpose behind Bookdown — allowing book-length publications to be authored by many authors. Disadvantages: The notes are always a work in progress. Don’t be mislead by the polish that RMarkdown and Bookdown give to the notes. "],
["introduction.html", "Topic 1 Introduction 1.1 Statistical and Machine Learning 1.2 Review of Day 1 1.3 Theoretical concepts ISL §2.1 1.4 Many techniques 1.5 Basic dicotomies in machine learning 1.6 Programming Activity 1 1.7 Review of Day 2 1.8 A Classifier example 1.9 Programming Activity 2 1.10 Day 3 theory: accuracy, precision, and bias 1.11 Programming Activity 3 1.12 Review of Day 3 1.13 Start Thursday 15 Sept. 1.14 Day 4 Preview 1.15 Small data", " Topic 1 Introduction Subjects Overview of statistical learning. Getting started with R, RStudio, and RMarkdown Reading: Chapter 2 of ISL Programming basics 1: Names, classes, and objects 1.1 Statistical and Machine Learning The two terms, “statistical learning” and “machine learning,” reflect mainly the artificialities of academic disciplines. Statisticians focus on the statistical aspects of the problems Computer scientists are interested in “machines”, including hardware and software. “Data Science” is a new term that reflects the reality that both statistical and machine learning are about data. Techniques and concepts from both statistics and computer science are essential. 1.1.1 Example 1: Machine translation of natural languages Computer scientists took this on. Identification of grammatical structures and tagging text. Dictionaries of single-word equivalents, common phrases. Story from early days of machine translation: Start with English: “The spirit is willing, but the flesh is weak.” Translate into Russian Translate back into English. Result: “The vodka is good, but the meat is rotten.” Statistical approach: Take a large sample of short phrases in language A and their human translation into language B: the dictionary Find simple measures of similarity between phrases in language A (e.g. de-stemmed words in common) Take new phrase in language A, look up it’s closest match in the dictionary phrases in language A. Translation is the corresponding dictionary entry in language B Where did the sample of phrases come from? European Union documents routinely translated into all the member languages. Humans mark correspondence. “Mechanical Turk” dispersal of small work tasks. Result: Google translate. 1.1.2 Example 2: From library catalogs to latent semantic indexing Early days: computer systems with key words and search systems (as in library catalogs) Now: dimension reduction (e.g. singular value decomposition), angle between specific documents and what might be called “eigen-documents” Result: Google search 1.1.3 Computing technique Each student in the class as a personal repository on GitHub. The instructor is also a contributor to this repository and can see anything in it. Complete instructions for doing this are in the appendix. Set up some communications and security systems (e.g. an RSA key) Clone your repository from GitHub. It is at an address like github.com/dtkaplan/math253-bobama. Day 1 Programming Activity 1.2 Review of Day 1 We discussed what “machine learning” means and saw some examples of situations where machine-learning techniques have been used successfully to solve problems that had at best clumbsy solutions before. (Natural language translation, catalogs of large collections of documents.) We worked through the process of connecting RStudio to GitHub, so that you can use your personal repository for organizing, backing up, and handing in your work. The Day-1 programming activity introduced some basic components of R: assignments, strings, vectors, etc. 1.3 Theoretical concepts ISL §2.1 “Data science” lies at the intersection of statistics and computer science. 1.3.1 Statistics concepts Sampling variability Bias and variance Characterization of precision Function estimation frameworks, e.g. generalized linear models Assumed probability models Prior and posterior probabilities (Bayesian framework) 1.3.2 Computing concepts Algorithms Iteration Simulation Function estimation frameworks, e.g. classification trees, support vector machines, artificial intelligence techniques Kalman filters 1.3.3 Cross fertilization Assumed probability models supplanted by simulation Randomization and iteration Cross validation Bootstrapping Model interpretibility Rather than an emphasis on the output of a function, interest in what the function has to say about how the world works. 1.4 Many techniques “Learning” is an attractive word and suggests that “machine learning” is an equivalent for what humans do. Perhaps it is to some extent … But “modeling” is a more precise term. We will be building models of various aspects of the world based on data. Model: A representation for a purpose. Blueprints, dolls, model airplanes. Mathematical model: A model built of mathematical stuff polynomials: Math 155 functions more generally: e.g. splines, smoothers, … trees geometry of distance: e.g. which exemplar are the inputs closest to? projection onto subspaces Statistical model: A mathematical model founded on data. 1.4.1 Unsupervised learning Wait until the end of the semester. We will be doing only supervised learning until late in the course. 1.4.2 Supervised learning: We have a set of cases, \\(i = 1, 2, 3, \\ldots, n\\), called the training data. For each case, we have an input \\({\\mathbf X_i}\\) consisting potentially of several variables measured on that case. The subscript \\(i\\) in \\({\\mathbf X_i}\\) means that we have one \\({\\mathbf X}\\) for each case. The boldface \\({\\bf X}\\) means that the input can be multi-dimensional, that is, consisting of multiple variables. For each case, we have an output \\(Y_i\\). We want to learn the overall pattern of the relationship between the inputs \\({\\mathbf X}\\) and the outputs \\(Y\\), not just for our \\(n\\) training cases, but for potential cases that we have not encountered yet. These as yet unencountered cases are thought of as the testing data. We are going to represent the pattern with a function \\(\\hat{f} : {\\bf X} \\rightarrow Y\\). Sometimes I’ll use the word model instead of function. A model is a representation for a purpose. A function is a kind of representation. So some models involve functions. That’s the kind we’ll focus on in this course. I say “model” out of habit, but it’s a good habit that reminds us that the purpose of the function is important and we should know what that purpose is when we undertake learning. 1.5 Basic dicotomies in machine learning There are fundamental trade-offs that describe the structure of learning from data. There are also trade-offs that arise between different methods of learning. Finally, there are dicotomies that stem from the different purposes for learning. These dicotomies provide a kind of road map to tell you where are are and identify where you might want to go. And, as always, it’s important to know why you are doing what you’re doing: your purpose. 1.5.1 Purposes for learning: Make predictions. Given new inputs, e.g. data about an event, predict what the result of the event will be. e.g. weather forecasting, credit card fraud, success in college, …. In statistics, this is sometimes thought of as “analyzing data from an observational study.” Anticipate the effects of an intervention that we impose, e.g., giving a patient a drug, changing funding for schools, … Traditionally in statistics, this has been tied exclusively to data from experiments. There is now greater acceptance that experiments are not always possible, and it’s important to be able to make reasonable inferences about causation from observational studies. Find structure in a mass of data. 1.5.2 Dicotomies make predictions vs capture causal mechanism (in this course: common sense. There are also formal techniques to guide causal reasoning.) flexibility vs variance (need some tools for this) black box vs interpretable models (comparing model architectures) reducible vs irreducible error (“bias” vs “residuals”) regression vs classification (easy!) supervised vs unsupervised learning (easy!) 1.5.3 Prediction versus mechanism Example: Malignancy of cancer from appearance of cells. Works for guiding treatment. Does it matter why malignant cells have the appearance they do? Story: Mid-1980s. Heart rate variability spectral analysis and holter monitors. (Holters were cassette tape recorders set to record ECG very, very slowly. Spectral analysis breaks down the overal signal into periodic components.) Very large spike at 0.03 Hz seen in people who will soon die. Could use for prediction, but researchers were also interested in the underlying physiological mechanism. Causal influences. We want to use observations to inform our understanding of what influences what. Story continued: The very large spike was the “wow and flutter” in the cassette tape mechanism. This had an exact periodicity: a spike in the spectrum. If the person was sick, their heart rate was steady: they had no capacity to vary it as other conditions in the body (arterial compliance, venus tone) called for. Understanding what happens in cardiac illness is, in part, about understanding how the various control systems interact. 1.5.4 Flexibility versus variance In traditional statistics, this is often tied up with the concept of “degrees of freedom.” Not flexible: Figure 1.1: Individual fits miss how the explanatory variables interact. ISL Figure 2.1 Flexible: Figure 1.2: Such detailed patterns are more closely associated with physical science data than with social/economic data. ISL Figure 2.2 And in multiple variables: Not flexible: Figure 1.3: ISL Figure 2.4 Flexible: Figure 1.4: ISL Figure 2.6 1.5.5 Black box vs interpretable models Many learning techniques produce models that are not easily interpreted in terms of the working of the system. Examples: neural networks, random forests, etc. The role of input variables is implicit. Characterizing it requires experimenting on the model. In other learning techniques, the role of the various inputs and their interactions is explicit (e.g. model coefficients). The reason to use a black-box model is that it can be flexible. So this tradeoff might be called “flexibility vs interpretability.” A quick characterization of several model architectures (which they call “statistical learning methods”) Figure 1.5: ISL Figure 2.7 1.5.6 Reducible versus irreducible error How good can we make a model? How do we describe how good it is? What does this mean? (from p. 19) \\[\\begin{array}{rcl} E(Y - \\hat{Y})^2 &amp; = &amp; E[f(X) + \\epsilon - \\hat{f}(X)]^2\\\\ &amp; = &amp; \\underbrace{[f(X) - \\hat{f}(X)]^2}_{Reducible} + \\underbrace{Var(\\epsilon)}_{Irreducible}\\\\ \\end{array}\\] Notation: \\(X\\) — the inputs that determine the output \\(Y\\). \\(Y\\) — the output, that is, the quantity we want to predict \\(\\hat{Y}\\) — our prediction hat means estimated, no hat means “real” (whatever that might mean) \\(E(Y - \\hat{Y})^2\\) — the mean of the square difference between our prediction and the “real” value. \\(E\\) means “expectation value.” \\(f(X)\\) — what \\(Y\\) would be, ideally, for a given \\(X\\) \\(\\hat{f}(X)\\) — our estimate of \\(f(X)\\) \\(\\epsilon\\) — but \\(Y\\) is subject to other, “random” influences. \\(\\epsilon\\) represents these. \\(\\epsilon\\) is a misleading notation because it may not be at all small in practice. But \\(\\epsilon\\) is alway centered on zero (by definition). \\(|f(X) - \\hat{f}(X)|\\) — the magnitude of the difference between the “real” \\(f()\\) and our estimate. This can be made small by collecting more data using a more flexible model expanding the set of inputs considered \\(Var(\\epsilon)\\) — the “variance” of \\(\\epsilon\\). This is the mean square of \\(\\epsilon\\), that is, \\(E(\\epsilon^2)\\). 1.5.7 Regression versus classification Regression: quantitative response (value, probability, count, …) Classification: categorical response with more than two categories. (When there are just two categories, regression (e.g. logistic regression) does the job.) 1.5.8 Supervised versus unsupervised Demographics groups in marketing. Poverty vs middle-class Political beliefs … left vs right? Figure 1.6: ISL Figure 2.8 1.6 Programming Activity 1 Using R/Markdown 1.7 Review of Day 2 1.7.1 Trade-offs/Dicotomies Regression vs classification Different kinds of functions. A classifier has output as a categorical level. A regression has output as a number. Many classifiers are arranged to produce as output a set of numbers: the probability of each of the possible levels of the categorical output. When there are just two such levels, only one probability is needed. (The other is simply \\((1-p)\\).) So for two-level classifiers, there’s not necessarily a distinction between regression and classification. Thus, “logistic regression.” Supervised vs unsupervised learning In supervised learning, with have an output (response variable) \\(Y\\) which we want to generate from input \\(\\mathbf{X}\\). We train a function \\(\\hat{f}: \\mathbf{X} \\rightarrow Y\\) In unsupervised learning, there is no identified response variable. Instead of modeling the response as a function of \\(\\mathbf{X}\\), we look for patterns within \\(\\mathbf{X}\\). Prediction vs causal mechanism Two different kinds of purpose. There may well be different kinds of functions best suited to each purpose. Accuracy (flexibility) vs interpretability We always want models to be accurate. Whether we need to be able to interpret the model depends on our overall purpose. Reducible error vs irreducible error It’s good to know how accurate our models can get. That gives a goal for trying out different types of models to know when we don’t need to keep searching. 1.8 A Classifier example A classification setting: Blood cell counts. Build a machine which takes a small blood sample and examines and classifies individual white blood cells. Figure 1.7: Blood cell classification The classification is to be based on two measured inputs, shown on the x- and y-axes. Training data has been developed where the cell was classified “by hand.” In medicine, this is sometimes called the gold standard. The gold standard is sometimes not very accurate. Here, each cell is one dot. The color is the type of the cell: granulocytes, lymphocytes, monocytes, … 1.9 Programming Activity 2 Some basics with data 1.10 Day 3 theory: accuracy, precision, and bias 1.10.1 Figure 2.10 In constructing a theory, it’s good to have a system you can play with where you know exactly what is going on: e.g. a simulation. The dark blue line in the left panel is a function the authors created for use in a simulation: Figure 1.8: ISL Figure 2.9 The dots are data the textbook authors generated from evaluating the function at a few dozen values of \\(x\\) and adding noise to each result. The difference between the dots’ vertical position and the function value is the residual, which they are calling the error. The mean square error MSE is \\[\\mbox{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - f(x_i))^2\\] Take this notation apart. What’s \\(n\\)? What’s \\(i\\)? Suppose that \\(f(x)\\) were constant. In that situation, what kind of statistical quantity does this resemble? In actual practice, we don’t know \\(f(x)\\). (Indeed, it’s a matter of philosophy whether this is an \\(f(x)\\) — it’s a kind of Platonic form.) Here we know \\(f(x)\\) because we are playing a game: running a simulation. Looking again at the left panel in Figure 2.9, you can see three different functions that they have fitted to the data. It’s not important right now, but you might as well know what these model architectures are: Linear regression line (orange) Smoothing splines (green and light blue). A smoothing spline is a functional form with a parameter: the smoothness. The green function is less smooth than the light blue function. That smoothness measure can also be applied to the linear regression form Each of these three functions were fitted to the data. Another word for fitted is trained. As such, we use the term training error for the difference between the data points and the fitted functions. Also, because the functions are not the Platonic \\(f(x)\\), they are written \\(\\hat{f}(x)\\). For each of the functions, the training MSE is \\[\\mbox{Training MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{f}(x_i))^2\\] Right panel of the graph is something completely different: both the axes are different than in the left panel. x-axis: the smoothness of the functions. This is labelled flexibility. The three x positions correspond to the smoothness of the three models. This is measured as the effective number of parameters of the function. Why does the straight-line function have a smoothness of 2? y-axis: the MSE The dots connected by the gray curve show the training MSE of the three models. The dots connected by the orange curve show the testing MSE of the three models. The continuous curves were constructed by calculating the MSE for many more values of smoothness than shown in the left panel. How did they measure the training MSE? 1.10.2 Another example: A smoother simulated \\(f(x)\\). Figure 1.9: ISL Figure 2.10 What’s different between the right panel of 2.9 and that of 2.10? 1.10.3 What’s the “best” of these models? When examining training MSE, the more flexible model has the smaller MSE. This answer is pre-ordained, regardless of the actual shape of the Platonic \\(f(x)\\). In traditional regression, we use ANOVA or adjusted$ \\(R^2\\) to help avoid this inevitability that more complicated models will be closer to the training data. Both of those traditional methods inflate* the estimate of the MSE by taking into account the “degrees of freedom,” df, in the model and how that compares to the number of cases \\(n\\) in the training dataset. The inflation looks like \\[ \\frac{n}{n - \\mbox{df}} \\] So when \\(\\mbox{df} \\rightarrow n\\), we inflate the MSE quite a lot. Another approach to this is to use testing MSE rather than training MSE. So pick the model with flexibility at the bottom of the U-shaped testing MSE curve. 1.10.4 Why is testing MSE U-shaped? Bias: how far \\(\\hat{f}(x)\\) is from \\(f(x)\\) Variance: how much \\(\\hat{f}\\) would vary among different randomly selected possible training samples. In traditional regression, we get at the variance by using confidence intervals on parameters. The broader the confidence interval, the higher the variation from random sample to random sample. These confidence intervals come from normal theory or from bootstrapping. Bootstrapping is a simulation of the variation in model fit due to training data. Bias decreases with higher flexibility. Variance tends to increase with higher flexibility. Irreducible error is constant. Figure 1.10: ISL Figure 2.12 1.10.5 Measuring the variance of independent sources of variation Simulation: Make and edit a file Day-03.Rmd. 1.10.5.1 Explore Add three different sources of variation. The width of the individual sources is measured by the standard deviation sd=. n &lt;- 1000 sd( rnorm(1000, sd=3) + rnorm(1000, sd=1) + rnorm(1000, sd=2) ) ## [1] 3.649603 Divide into small groups and construct a theory about how the variation in the individual components relates to the variation in the whole. test whether your theory works for other random distributions, e.g. rexp() 1.10.5.2 Result (Don’t read until you’ve drawn your own conclusions!) The variance of the sum of independent random variables is the sum of the variances of the individual random variables. 1.10.6 Equation 2.7 \\[E( y - \\hat{f}(x) )^2 = \\mbox{Var}(\\hat{f}(x)) + [\\mbox{Bias}(\\hat{f}(x))]^2 + \\mbox{Var}(\\epsilon)\\] Breaks down the total “error” into three independent sources of variation: How \\(y_i\\) differs from \\(f(x_i)\\). This is the irreducible noise: \\(\\epsilon\\) How \\(\\hat{f}(x_i)\\) (if fitted to the testing data) differs from \\(f(x_i)\\). This is the bias. How the particular \\(\\hat{f}(x_i)\\) fitted to the training data differs from the \\(\\hat{f}(x_i)\\) that would be the best fit to the testing data. \\[\\underbrace{E( y - \\hat{f}(x) )^2}_{\\mbox{Total error}} = \\underbrace{\\mbox{Var}(\\hat{f}(x))}_{\\mbox{source 3.}} + \\underbrace{[\\mbox{Bias}(\\hat{f}(x))]^2}_{\\mbox{source 2.}} + \\underbrace{\\mbox{Var}(\\epsilon)}_{\\mbox{source 1.}}\\] 1.11 Programming Activity 3 Indexing on data: training and testing data sets 1.12 Review of Day 3 \\(f({\\mathbf X})\\) versus \\(\\hat{f}({\\mbox{X}})\\): Platonic idea versus what we get out of the training data. Quip: “The hat means there’s a person underneath the model.” Mean Square Error — like the standard deviation of residuals Training vs testing data Smoothness, a.k.a. flexibility, model degrees of freedom More flexibility \\(\\rightarrow\\) better training MSE Components of MSE Irreducible random noise: \\(\\epsilon\\) Bias: \\(f({\\mathbf X}) - \\hat{f}({\\mathbf X})\\) Caused by too much smoothness Caused by omitting a relevant variable Caused by including an irrelevant variable \\(Var(\\hat{f}({\\mathbf X}))\\) — how much \\(\\hat{f}\\) varies from one possible training set to another. Increased by too many degrees of freedom: overfitting Increased by collinearity and multi-collinearity. Increased by large \\(\\epsilon\\) Decreased by large \\(n\\) 1.13 Start Thursday 15 Sept. Programming Basics I Indexing on data: training and testing data sets 1.14 Day 4 Preview The linear model (e.g. what lm() does) A variety of questions relevant to different purposes, e.g. how good will a prediction be? what’s the strength of an effect? is there synergy between different factors? ISL book’s statement on why to study linear regression “Though it may seem somewhat dull compared to some of the more modern statistical learning approaches described … later …, linear regression is still a useful and widely used statistical learning method. Moreover, it serves as a good jumping-off point for newer approaches…. Consequently, the importance of having a good understanding of linear regression before studying more complex learning methods cannot be overstated.” Concepts from linear regression: Compact representation of model form: polynomial coefficients. Much of inference (confidence intervals, hypothesis tests) can be expressed in terms of a polynomial coefficient. “Size” of model quantifiable as an integer: number of coefficients: degrees of freedom. Highly efficient estimation (when doing least squares) 1.15 Small data The regression techniques were developed in an era of small data, such as that that might be written in a lab notebook or field journal. As a result: Emphasis on very simple descriptions, such as means, differences between means, simple regression. Theoretical concern with details of distributions, such as the t-distribution. the difference between z- and t-distributions are of no consequence for moderate DF and higher. No division into training and testing data. Data are too valuable to test! (Ironic, given the importance of replicability in the theory of the scientific method.) As a consequence of (3), there’s a great deal of concern about assumptions, e.g. linearity of \\(f({\\mathbf X})\\) structure of \\(\\epsilon\\): IID — Independent and Identically Distributed uncorrelated between cases each is a draw from the same distribution. "],
["notes.html", "Topic 2 Notes 2.1 Review of Day 4, Sept 15, 2017 2.2 Regression and Interpretability 2.3 Toward an automated regression process 2.4 Selecting model terms 2.5 Programming basics: Graphics 2.6 In-class programming activity 2.7 Day 5 Summary 2.8 In-class programming activity 2.9 Day 6 Summary 2.10 Measuring Accuracy of the Model 2.11 Bias of the model 2.12 Forward, backward and mixed selection 2.13 Programming Basics: Functions 2.14 In-class programming activity 2.15 Review of Day 7 2.16 Using predict() to calculate precision 2.17 Conclusion", " Topic 2 Notes 2.1 Review of Day 4, Sept 15, 2017 How the linear regression architecture relates to machine learning. linear regression is designed to work even in “small data” situations where cross-validation is not appropriate the number of model degrees of freedom may be almost as large as \\(n\\) provides a ready definition of the “size” of a model: the number of coefficients. Main software used in linear regression, lm() and predict() Programming basics: indexing of vectors, matrices and data frames. 2.2 Regression and Interpretability Regression models are generally constructed for the sake of interpretability: Global linearity Coefficients are indication of effect size. The coefficients have physical units. Term by term indication of statistical significance An example on College data from ISLR package data(College, package=&quot;ISLR&quot;) College$Yield &lt;- with(College, Enroll/Accept) mod1 &lt;- lm(Yield ~ Outstate + Grad.Rate + Top25perc, data = College) mosaic::rsquared(mod1) ## [1] 0.2170221 mod2 &lt;- lm(Yield ~ . - Grad.Rate, data = College) mosaic::rsquared(mod2) ## [1] 0.5004599 What variables matter? How good are the predictions? How strong are the effects? 2.3 Toward an automated regression process In machine learning, we ask the computer to identify patterns in the data. In “traditional” regression (which is still very important), we specify the explanatory terms and the computer finds the “best” model with those terms: least squares. In machine learning, we want the computer to figure out which terms, of all the possibilities, will lead to the “best” model. 2.4 Selecting model terms The regression techniques Traditional regression: Our knowledge of the system being studied. Heirarchical principal main effects, then interaction. Machine learning Look at all combinations of variables? Activity 1: Write a statement that will pull 2 random variables from a data frame and the explanatory variable. Use Yield ~ . as the formula. explanatory_vars &lt;- names(College)[-19] my_vars &lt;- sample(explanatory_vars, size = 12) new_data &lt;- College[ , c(my_vars, &quot;Yield&quot;)] mod &lt;- lm(Yield ~ ., data = new_data) mosaic::rsquared(mod) ## [1] 0.3353474 mod ## ## Call: ## lm(formula = Yield ~ ., data = new_data) ## ## Coefficients: ## (Intercept) Grad.Rate Outstate S.F.Ratio Enroll ## 4.528e-01 -2.297e-04 -1.804e-05 2.087e-03 1.077e-04 ## perc.alumni Books Personal Expend Apps ## 4.508e-04 3.304e-05 7.720e-06 5.771e-06 -2.238e-05 ## PrivateYes Top25perc F.Undergrad ## 1.156e-02 6.683e-04 -6.454e-06 2^18 ## [1] 262144 Activity 2: * How many combinations are there of \\(k\\) explanatory variables? Calculate this for \\(k = 5, 10, 15, 20\\). How many are there in the College? * What about with interactions? How long does it take to fit a model? system.time( do(1000) * lm(Yield ~ ., data=College)) ## user system elapsed ## 6.188 0.031 6.242 256*7/3600 ## [1] 0.4977778 names(College) ## [1] &quot;Private&quot; &quot;Apps&quot; &quot;Accept&quot; &quot;Enroll&quot; &quot;Top10perc&quot; ## [6] &quot;Top25perc&quot; &quot;F.Undergrad&quot; &quot;P.Undergrad&quot; &quot;Outstate&quot; &quot;Room.Board&quot; ## [11] &quot;Books&quot; &quot;Personal&quot; &quot;PhD&quot; &quot;Terminal&quot; &quot;S.F.Ratio&quot; ## [16] &quot;perc.alumni&quot; &quot;Expend&quot; &quot;Grad.Rate&quot; &quot;Yield&quot; With \\(k\\) explanatory variables, \\(2^k\\) possibilities, not even including interactions. Including first-order interactions, it’s \\(2^k + 2^{k(k-1)/2}\\). Calculate this for $k=3. - Increase in \\(R^2\\)? Problem: \\(R^2\\) will always go up as we add a new term. - Some other measure that takes into account how much \\(R^2\\) should go up. 2.5 Programming basics: Graphics plot(1, type = &quot;n&quot;, xlim = c(100,200), ylim = c(300,500)) Basic functions: Create a frame: plot(). Blank frame: plot( , type=&quot;n&quot;) set axis limits, Dots: points(x, y), pch=20 Lines: lines(x, y) — with NA for line breaks Polygons: polygon(x, y) — like lines but connects first to last. fill Color, size, … rgb(r, g, b, alpha), “tomato” 2.6 In-class programming activity Programming Activity 4: Drawing a histogram. 2.7 Day 5 Summary 2.7.1 Linear regression Discussed “interpretability” of linear models, e.g. meaning of coefficients, confidence intervals, R^2, etc. which variables are “important” via ANOVA and mean sum of squares Discussed metrics to compare models R^2 – not fair, since “bigger” models are always better Punishment: Two criteria for judging R^2 How big the model is. These two are somehow combined together into “adjusted R^2.” We’ll say more about that today. Cross-validation. Judge each model on its “out of sample” prediction performance. 2.7.2 Coefficients as quantities Coefficients in linear models are not just numbers, they are physical quantities with dimensions and units. Dimensions are always (dim of response)/(dim of this term) The model doesn’t depend on the units of these quantities. The units only set the magnitude to the numerical part of the coefficient, but as a quantity a coefficient is the same thing regardless of units. Conversion from one unit to another by multiplying by 1, but expressed in different units, e.g. 60 seconds per minute, 2.2 pounds per kilogram. 2.8 In-class programming activity Day 5 activity Drawing a histogram. 2.9 Day 6 Summary \\(R^2\\) var(fitted) / var(response) partitioning of variance: var(fitted) + var(resid) = var(response) same with sum of squares: SS(fitted) + SS(resid) = SS(response) Adjusted \\(R^2\\) \\(R^2\\) vs \\(p\\) picture Derive a formula from the picture: we’ve got \\(p+1\\) df to get from \\(R^2 = 0\\) to our observed \\(R^2\\), so \\(n - (p+1)\\) df left for the residuals. Rate of increase due to junk is \\((1 - R^2) / (n - p - 1)\\). Projecting back \\(p\\) terms gives an adjustment of \\((1 - R^2) / \\frac{p}{n - p - 1}\\). Subtract this from \\(R^2\\). Wikipedia gives two formulas: \\(Adj R^2 = {1-(1-R^{2}){n-1 \\over n-p-1}}\\) – this projects back \\(n-1\\) terms from 1. \\(Adj R^2 = {R^{2}-(1-R^{2}){p \\over n-p-1}}\\) — projects back \\(p\\) terms from \\(R^2\\). Adjusted \\(R^2\\) Whole model ANOVA. ANOVA on model parts 2.10 Measuring Accuracy of the Model \\(R^2\\) = Var(fitted)/Var(response) Adjusted \\(R^2\\) - takes into account estimate of average increase in \\(R^2\\) per junk degree of freedom Residual Standard Error - Sqrt of average square error per residual degree of freedom. The sqrt of the mean square for residuals in ANOVA. 2.11 Bias of the model You need to know the “truth” to calculate the bias. We don’t. Perhaps effect of TV goes as sqrt(money) as media get saturated? Perhaps there is a synergy that wasn’t included in the model? 2.11.1 Theory of whole-model ANOVA. Standard measure: \\(\\frac{\\mbox{Explained amount}}{\\mbox{Unexplained amount}}\\) Examples: Standard error of mean: \\(\\frac{\\hat{\\mu}}{\\sigma / n}\\) – note the \\(n\\). t statistic on difference between two means: \\(\\frac{\\hat{\\mu}_1 - \\hat{\\mu}_2}{\\sigma / (n-1)}\\) F statistic: \\(\\frac{SS / df1}{SSR / df2}\\) df1 is the number of degrees of freedom involved by the model or model term under consideration. df2 is \\(n - (p - 1)\\) where \\(p\\) is the total degrees of freedom in the model. (I called this \\(m\\) in the Math 155 book.) The intercept is what the \\(-1\\) is about: the intercept can never account for case-to-case variation. Trade-off between eating variance and consuming degrees of freedom. 2.12 Forward, backward and mixed selection Use the College model to demonstrate each of the approaches by hand. Start with pairs() or write an lapply() for the correlation with Yield? Create a whole bunch of model terms “main” effects “interaction” effects nonlinear transformations: powers, logs, sqrt, steps, … categorical variables Result: a set of \\(k\\) vectors that we’re interested to use in our model. Considerations: not all of the \\(k\\) vectors may pull their weight two or more vectors may overlap in how they eat up variance Algorithmic approaches: Try all combinations, pick the best one. computationally expensive/impossible \\(2^k\\) possibilities what’s the sensitivity of the process to the choice of training data? “Greedy” approaches 2.13 Programming Basics: Functions Syntax of functions: name &lt;- function(arg1, arg2, ...) { body of function. Can use arg1, arg2, etc. } typically you will return a value. The value calculated by the last command line in the body is what’s returned. Or you can use return() at any point in the function. Often functions are designed to produce “side effects”, e.g. graphics. Scope: what happens in functions stays in functions. Create a plotting frame: plot() Write a function that makes this more convenient to use. What features would you like. blank_frame &lt;- function(xlim, ylim) { } Write a function to draw a circle. What do you want the interface to look like? What arguments are essential? What options are nice to have? 2.14 In-class programming activity Histogram and density functions set.seed(101) n = 20 X &lt;- data.frame(vals = runif(n), group = as.character((1:n) %% 2)) ggplot(head(X, 6), aes(x = vals)) + geom_density(bw = 1, position = &quot;stack&quot;, aes(color = group, fill = group)) + geom_rug(aes(color = group)) + # facet_grid( . ~ group) + xlim(-0.5, 1.5) Day 6 activity 2.15 Review of Day 7 We finished reviewing adjusted R^2 and ANOVA. Started talking about linear algebra. We only got through the first few elements in our review of linear algebra. Let’s go through them again 2.16 Using predict() to calculate precision confidence intervals prediction intervals 2.17 Conclusion This wraps up our look at linear regression. Main points: model output is a linear combination of the inputs. lm() finds the “best” linear combination. rich theory relating to precision of coefficients and the residuals. traditional ways of applying that theory: F tests and t tests. "],
["foundations-linear-algebra-likelihood-and-bayes-rule.html", "Topic 3 Foundations: linear algebra, likelihood and Bayes’ rule 3.1 Linear Algebra 3.2 Arithmetic of linear algebra operations 3.3 The geometry of fitting 3.4 Precision of the coefficients 3.5 Likelihood and Bayes 3.6 Summary of Day 8 3.7 Day 9 Announcements 3.8 Conditional probability 3.9 Inverting conditional probabilities 3.10 Summary of Day 9 3.11 Likelihood example 3.12 Exponential probability density 3.13 California earthquake warning, reprise 3.14 The Price is Right! 3.15 From likelihood to Bayes 3.16 Choosing models using maximum likelihood 3.17 Day 9 Review 3.18 Reading: What is Bayesian Statistics 3.19 Programming Basics: Conditionals 3.20 ifelse() examples 3.21 if … else … examples 3.22 Simple 3.23 Blood testing 3.24 The (hyper)-volume of the hypersphere. 3.25 Find the surface area, \\(D_n r^{n-1}\\). 3.26 In-class programming activity", " Topic 3 Foundations: linear algebra, likelihood and Bayes’ rule The topics in this section — linear algebra, Bayes’ rule, and likelihood — underlie many of the machine-learning techniques we will be studying later in the course. Bayes’ rule is a way to flip conditional probabilities. Among other things it allows you to interpret data in the light of previous knowledge and belief (which to be fancy, we can call “theory”). Likelihood is a unifying principle for using data to estimate model parameters and is fundamental in statistical theory. It’s also an essential part of Bayes’ rule. And linear algebra is used throughout statistics and machine learning. Among other things, it’s at work behind the motivation and calculations of regression. 3.1 Linear Algebra The idea here is not to teach you linear algebra, but to expose you to some of the terminology and operations of linear algebra, so that when you see it again later in the course you’ll have a good start. A vector — a column of numbers. The dimension is the count of numbers in the vector. A space: the set of all possible vectors of a given dimension. Scalar multiplication Vector addition: walk the first vector, then the second. Linear combination: do scalar multiplication on each vector, then add. A matrix — a collection of vectors (all the same dimension). Dot product: a basic calculations on vectors: the length (via Pythagorus) the angle between two vectors orthogonality: when two vectors are perpendicular, their dot product is zero. Matrix operation: Linear combination. Take a linear combination of the vectors in a matrix. Analogous to taking a trip. Result: a vector representing the end-point of the trip. The subspace spanned by the matrix: the set of all possible points you can get to with a linear combination. Matrix operation: Orthogonalization — Find perpendicular vectors that span the same subspace as a matrix. Example, draw the picture for two vectors \\(\\vec{a}\\) and \\(\\vec{b}\\). Matrix operation: Projection Given a matrix M and a vector V, find the closest point in the subspace of M to the vector V. How? Orthogonalize matrix M, then for each vector in orthogonalized M, subtract out the part of \\(V\\) aligned with that vector. Matrix operation: inversion — the inverse operation to linear combination. given an end-point in the space spanned by M, figure out a linear combination that will get you there. Vector to vector operation: Outer product. col vector \\(\\times\\) row vector. Can generalize to operations other than \\(\\times\\). For linear algebra folks: Projection is the Q part of QR decomposition. R is the solve part. - In economics, they write things in an older style: Solve \\(M b = y\\) for \\(b\\). But \\(M\\) may not be square, so no regular inverse. - Pre-multiply by \\(M^T\\) to get \\(M^T M b = M^T y\\) - Invert to get \\(b = (M^T M)^{-1} M^T y\\). The matrix \\((M^T M)^{-1} M^T\\) is called the pseudo inverse. 3.2 Arithmetic of linear algebra operations Addition comes for free. Confirm this. Scalar multiplication comes for free. Confirm this. Write a function for the dot product. vdot &lt;- function(v, w) { sum(v * w) } Write a function for the vector length. vlength &lt;- function(v) { sqrt(vdot(v, v)) } Write a function for the cosine of the angle between two vectors. vangle &lt;- function(v, w, degrees = FALSE) { theta &lt;- acos(vdot(v, w) / (vlength(v) * vlength(w))) if (degrees) theta * (180 / pi ) else theta } Write a function to project vector \\(\\vec{a}\\) onto \\(\\vec{b}\\). Subtracting the result from \\(\\vec{a}\\) will give the component of \\(\\vec{a}\\) orthogonal to \\(\\vec{b}\\). So we can decompose \\(\\vec{a}\\) into two components relative to \\(\\vec{b}\\). Show that the supposedly ortogonal component is really orthogonal to \\(b\\) — that is, the dot product is 0. vproject &lt;- function(v, onto) { # the red thing onto * vlength(v) * cos(vangle(v, onto)) / vlength(onto) } vresid &lt;- function(v, onto) { v - vproject(v, onto) } Generalization: Write a function to orthogonalize a matrix M. Generalization: Write a function to calculate the projection of V onto M. vdot &lt;- function(a, b) { sum(a * b) } vlen &lt;- function(a) sqrt(vdot(a, a)) vcos &lt;- function(a, b) { vdot(a, b) / (vlen(a) * vlen(b)) } vangle &lt;- function(a, b, degrees = FALSE) { res &lt;- vcos(a, b) if (degrees) res &lt;- res * 180 / pi res } vproj &lt;- function(a, onto = b) { vlen(a) * vcos(a, onto) * onto } 3.3 The geometry of fitting Data tables: cases and variables. Case space (the rows of the matrix) and variable space (the columns). A quantitative variable is a vector. A categorical variable can be encoded as a set of “dummy” vectors. Response variable and explanatory variable The linear projection problem: find the point spanned by the explanatory variables that’s closest to the response. That linear combination is the best-fitting model. One explanatory and the response Two explanatory on board and the response on the board (perfect, but meaningless fit) Two explanatory in three-space and the response (residual likely) 3.4 Precision of the coefficients \\[ \\mbox{standard error of B coef.} = | \\mbox{residuals} | \\frac{1}{| \\mbox{B} |}\\ \\frac{1}{\\sin( \\theta )}\\ \\frac{1}{\\sqrt{n}}\\ \\sqrt{\\frac{n}{n-m}}\\] \\(m\\) — degrees of freedom in model \\(\\theta\\) — angle between this model vector and the space spanned by the others B — this model vector residuals — the residual vector 3.5 Likelihood and Bayes We accept that our models won’t produce an \\(\\hat{f}(x)\\) that always matches \\(y\\). There is the irreducible error \\(\\epsilon\\), in addition to variance and bias. Variance: a measure of how far off our \\(\\hat{f}()\\) is from that we would have been able to construct with an infinite amount of data: \\(\\hat{f}_\\infty()\\). Bias: a measure of how far off \\(\\hat{f}_\\infty()\\) is from \\(f()\\). We’re using mean square error or sum of square errors as a measure of how far \\(\\hat{f}(x_i)\\) is from the actual result \\(y_i\\). Now we’re going to look at the difference in terms of probabilities: what would be the probability of any particular \\(\\hat{y}_i\\) given our \\(\\hat{f}(x_i)\\). Let’s quantify probability. 3.6 Summary of Day 8 We finished up our brief introduction to linear algebra and started discussing probability. I suggested the rather broad definition of a probability as a number between zero and one. 3.7 Day 9 Announcements Make sure you’ve accepted the invitation to the discussion group. Reading for Thursday: “What is Bayesian statistics and why everything else is wrong” 3.7.1 What’s a probability? Chances of something happening Frequentist: Number of “favorable events” / number of events Bayesian. Number between 0 and 1. $ p( | , , )$ densities cumulative — this is really what probability refers to. discrete events joint events conditional events relating joint and conditional: p(A &amp; X) = p(A | X) p(X) = p(X | A) p(A) Bayes rule p(A | X) = p(X | A) p(A) / p(X) 3.8 Conditional probability The probability of an event in a given state of the world. That state of the world might have been set up by another event having occurred. 3.9 Inverting conditional probabilities What we want is \\(p(\\mbox{state of world} | \\mbox{observations})\\). I’ll write this \\(p(\\theta | {\\cal O})\\) Tree with cancer (benign or malignant) and cell shape (round, elongated, ruffled) SPACE FOR THE TREE SEE PAPER NOTES. (remember to transcribe them here) , e.g. observe ruffled, what is the chance that the tumor is malignant. Of the 10000 people in the study, * 7000 had benign tumors of which 10% or 700 had ruffled cells * 3000 had malignant tumors of whom 60% or 1800 had ruffled cells So, of the 2500 people with ruffled cells, 1800 had malignant tumors. \\(p( \\theta | {\\cal O} )\\) 3.10 Summary of Day 9 Announcement: Read “What is Bayesian Statistics and why everything else is wrong.” We derived Bayes’ rule (simple!) from fundamental principles of probability. There are three components of the formula that have individual names: The prior. What you know before you examine the data. The likelihood. A conditional probability. Given a theory (that is, a way that you think the world works), what is the probability of the data you have observed. Typically a likelihood is built around a model that has both a deterministic and a random component. For instance, the output \\(y_i\\) depends on an input \\(x_i\\) according to \\(y_i = a + b x + \\epsilon\\). We specify the properties (e.g., the variance) of the random component as part of the theory. For instance, we might specify that \\(\\epsilon\\) is drawn from a normal distribution with mean zero and variance \\(c^2\\). The likelihood can be calculated by multiplying together the probabilities of each of values the data indicates. Continuing with the example in the previous points, \\(\\epsilon_i = y_i - (a + b x_i)\\). The calculation would be likelihood \\(= \\prod_i\\)dnorm(\\(\\epsilon_i\\), mean = 0, sd = c). In order to have acceptable performance in computer arithmetic, we generally calculate the log likelihood. This allows us to turn the product in the above point into a sum. The posterior. Our updated beliefs given the data. Controversy! Many people believe that performing calculations using a prior is unscientific. This is because the prior reflects the views of the researcher, rather than a solid fact of reality. Nonetheless, everyone agrees that the likelihood is meaningful. Many of the estimation problems of statistics amount to finding parameters that maximize the likelihood. (People who think that using a prior is a reasonable way of doing business, like the author, point out that the model itself is a subjective choice.) The in-class programming task involved calculating a likelihood with a model that involved the exponential probability distribution. In that task, we were not doing the full Bayesian calculation. Instead, we took as our point estimate of the parameter the argmax, that is, the parameter value that produces the highest likelihood. 3.11 Likelihood example Consider a mechanism like that behind Figure 3.1 in ISLR. Figure 3.1 from ISRL There seems to be a straight-line relationship between Sales and TV. You can also see that the variance of the residuals is bigger for larger values of TV. The usual least-squares estimator is based on maximizing the likelihood of a model like \\(\\mbox{Sales} = a + b \\mbox{TV} + \\epsilon\\), where \\(\\epsilon\\) is “iid normal”. But we know the \\(\\epsilon\\) estimated from the data won’t be iid. Our model is therefore wrong. That’s not necessarily a problem, since all models are wrong (they are models after all!) but some models are useful. So the iid model might be useful. Let’s make some similar data on which we can demonstrate a likelihood calculation. TV &lt;- runif(500) Sales &lt;- 3.5 + 12 * TV + (3* TV + 0.75) * rnorm(length(TV)) plot(TV, Sales, ylim = c(0, 25)) Here’s a simple calculation with a “wrong” model: lm(Sales ~ TV) ## ## Call: ## lm(formula = Sales ~ TV) ## ## Coefficients: ## (Intercept) TV ## 3.391 11.910 If we like, however, we can do the calculation with a “better” model, say, taking the following as the model behind our likelihood. Sales_likelihood &lt;- function(params) { a &lt;- params[1] b &lt;- params[2] c &lt;- params[3] d &lt;- params[4] # Negate so the the minimization routine will produce the maximum likelihood -sum(log(dnorm(Sales - (a + b * TV), mean = 0, sd = c + d * TV ))) } Sales_likelihood(c(5, 10, 1, 1)) ## [1] 1263.285 nlm(Sales_likelihood, c(5, 10, 1, 1)) ## $minimum ## [1] 1040.835 ## ## $estimate ## [1] 3.4181833 11.8437802 0.6808023 2.8594718 ## ## $gradient ## [1] 1.303770e-05 1.746993e-06 -1.318767e-05 8.905789e-06 ## ## $code ## [1] 1 ## ## $iterations ## [1] 23 3.12 Exponential probability density What’s the time between random events, e.g. 500-year storms or earthquakes in a region that has a big one roughly every 100 years? Earthquake warning in Southern California, late Sept. 2016 But over the last week, anxieties were particularly heightened, and the natural denial that is part of living in earthquake country was harder to pull off. A swarm of seismic activity at the Salton Sea that began a week ago prompted scientists to say there was an elevated risk for a big San Andreas fault earthquake. By Monday [Oct 3, 2016], that risk had lessened. But the impact of that warning was still being felt. For some, it meant checking quake safety lists. Others looked at preparing for the Big One, such as bolting bookshelves to walls, installing safety latches on kitchen cabinets and strapping down televisions. Why has the risk gotten smaller? How much smaller? 3.12.1 Meanwhile, further north … From *The Really Big One, an article in the New Yorker about discoveries in the last few decades that established a high risk in the Pacific Northwest for an earthquake of magnitude 9. We now know that the Pacific Northwest has experienced forty-one subduction-zone earthquakes in the past ten thousand years. If you divide ten thousand by forty-one, you get two hundred and forty-three, which is Cascadia’s recurrence interval: the average amount of time that elapses between earthquakes. That timespan is dangerous both because it is too long—long enough for us to unwittingly build an entire civilization on top of our continent’s worst fault line—and because it is not long enough. Counting from the earthquake of 1700, we are now three hundred and fifteen years into a two-hundred-and-forty-three-year cycle. It is possible to quibble with that number. Recurrence intervals are averages, and averages are tricky: ten is the average of nine and eleven, but also of eighteen and two. It is not possible, however, to dispute the scale of the problem. The last paragraph … All day long, just out of sight, the ocean rises up and collapses, spilling foamy overlapping ovals onto the shore. Eighty miles farther out, ten thousand feet below the surface of the sea, the hand of a geological clock is somewhere in its slow sweep. All across the region, seismologists are looking at their watches, wondering how long we have, and what we will do, before geological time catches up to our own. Have students propose distributions and justify them. 3.13 California earthquake warning, reprise The Salton Sea earthquake happens. Our prior on large \\(\\lambda\\) immediately surges, so there is a significant probability of a quake in the next hours. But as more time goes by, that probability goes down. We’re interested in \\(\\lambda\\) in the exponential distribution \\(\\lambda \\exp(-lambda t)\\) . This has a cumulative \\(1 - \\exp(-\\lambda t)\\). Observation: Earthquake hasn’t occurred after D days. Likelihood is 1 minus the cumulative, or \\(\\exp(-\\lambda D)\\). Prior: a mix of the conventional (very small lambda) and some small probability of very high lambda. Plot out the posterior for different values of D: D = 0.1 two hours after the quake. D = 1 a day after the quake D = 3 three days after the quake The area to the right of 5 days (in expected time to the next quake) is the conventional model. Plot this out as a function of \\(1/\\lambda\\), so we need to adjust the density by \\(| df/d\\lambda | = | d \\frac{1}{\\lambda} / d\\lambda| = \\lambda^2\\) D &lt;- 30 lambda &lt;- 100/(1:5000) # prior: proportional to lambda: # small lambda unlikely, so short time to next earthquake prior &lt;- function(lambda) (ifelse(lambda &lt; .2, 25, 1/lambda)) plot(lambda, ( prior(lambda)), type = &quot;l&quot;, xlim = c(0,5)) plot(1/lambda, exp( - lambda * D) * prior(lambda) * (lambda^2), type = &quot;l&quot;, xlab = &quot;Expected time to the big one, days.&quot;, xlim = c(0, 10)) lines(1/lambda, lambda*.005/prior(lambda), col = &quot;red&quot;) For small D, the “urgency” part of the prior overwhelms the likelihood. As D gets bigger, we revert to the standard model. 3.14 The Price is Right! The Price is Right is a game show in which contestants compete to guess the price of a prize. The winner is the person whose guess is closest to the actual price considering just those contestants who guesses a price less than or equal to the actual price. Strategy: First person to guess: an honest guess, hedged on the low side. Second person: bias guess to be far from the first person’s guess. Third person: Fourth person: Zero, or just above one of the other guesses. Play this game. Call down 4 contestants. What’s the price of this yacht? Now, suppose rather than being a strategic game biased toward the last guesser, we wanted to evaluate political prognosticators. The winner should be the person who makes the best prediction rather than the best guess. Game: Predict the number of electoral college votes for Donald Trump. Game: Predict the results of the Ukrainian Parliament’s vote of no confidence in Prime Minister Arseniy Yatsenyuk. How many votes for no confidence were there.1 Play this game asking people to draw the probability distribution of their prediction. Suppose you know something about the contestants. David Moore from International Studies Gary Krueger from Economics Sybill Trelawney from Divination Science Jesse Ventura from Political Science You’ve been asked to assign a probability to each contestant. You’ll use this probability to weight each of their future predictions. Have the contestants keep their identity secret at first. Draw a density on the board. Give them a vertical scale for density, insisting that each of their densities has area one. The Likelihood Game: Who won? How to evaluate the predictions? The Bayesian Game: The contestants reveal their identity What’s your posterior probability on each of them. 3.15 From likelihood to Bayes Multiply likelihood by prior probability. Normalize so that total probability is 1. 3.16 Choosing models using maximum likelihood We model the error as random, with a probability distribution we choose. Often this distribution has parameters. To find the error, we need to make an assumption of what the parameters of the deterministic model are. Make that assumption. Make a similar assumption for the parameters of the probability distribution. Find the errors. Calculate the probability of those errors given the probability distribution that we choose. That’s the likelihood for the assumed parameters. Repeat in order to modify the assumptions to increase the likelihood. Straight line model: Gaussian errors: \\[f(x \\; | \\; \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi} } \\; e^{ -\\frac{(x-\\mu)^2}{2\\sigma^2} }\\] What happens when you take the log … why it’s sum of squares. Question: What about minimizing the absolute value of the residuals, rather than the square? - Corresponds to a two-sided exponential distribution like \\(\\frac{\\lambda}{2} \\exp(-\\lambda |x|)\\) 3.17 Day 9 Review Likelihood. Choose the “best” of a set of competing models. Often the set is parameterized by quantities such as slope \\(m\\), intercept \\(b\\), rate , mean, standard deviation, … “Figure of merit” for each model is the probability of the data given the model. Often, models have a deterministic part (e.g. \\(m x + b\\)) and a random part \\(\\epsilon\\). Part of the model is our choice for the distribution of \\(\\epsilon\\). Given that distribution, and treating each error \\(\\epsilon_i\\) as random, to calculate the likelihood we find the probability of each \\(\\epsilon_i\\) and multiply them together. For practical reasons (both algebraic and computational) we work with the log-likelihood. Example: Mean and standard deviation data_vals &lt;- runif(10000, min = 20, max = 50) # data_vals &lt;- rexp(10000, rate = 1) mean(data_vals) ## [1] 35.07677 median(data_vals) ## [1] 35.01521 sd(data_vals) ## [1] 8.670784 IQR(data_vals) ## [1] 15.02683 LL_gaussian &lt;- function(params) { center = params[1] spread = params[2] sum(log(dnorm(data_vals, mean = center, sd = spread))) } optim(par = c(25, 10), LL_gaussian, control = list(fnscale = -1)) ## $par ## [1] 35.077971 8.671052 ## ## $value ## [1] -35788.48 ## ## $counts ## function gradient ## 53 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL LL_exp &lt;- function(params) { center = params[1] spread = params[2] sum(log(dexp(abs(data_vals - center), rate = 1/spread))) } optim(par = c(25, 10), LL_exp, control = list(fnscale = -1)) ## $par ## [1] 35.015632 7.515695 ## ## $value ## [1] -30171.14 ## ## $counts ## function gradient ## 55 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL 3.18 Reading: What is Bayesian Statistics Link to “What is Bayesian Statistics and why everything else is wrong.” Go through sections 1, 2, and 4 in class: about the likelihood calculation, the p-value calculation, and the philosophical criticism of p-values. Find the likelihood of observing \\(k\\) cases out of \\(E\\) employees with a rate of \\(\\theta\\). E &lt;- 145 k &lt;- 8 theta &lt;- .01 L &lt;- function(theta) dbinom(k, size = E, prob = theta) Making the plot of likelihood versus \\(\\theta\\) x &lt;- seq(0,.10, length = 100) y &lt;- L(x) plot(x, y, type = &quot;l&quot;, xlab = &quot;theta&quot;, ylab = &quot;likelihood&quot;) Emphasize the choice of what detail of the sampling model to use. Just this school in isolation? Suppose we consider that there are 1000 schools near high-tension lines. Our school is presumably one of the highest rates, since other schools who had bigger numbers would come forward. Let’s imagine that our school is in the top 10%. This is like calculating that of 10 schools, the 8 cancer cases we observed are the most of any of those 10. What does this give for the likelihood of theta? nschools &lt;- 2 Lschools &lt;- function(theta) { prob_of_k &lt;- dbinom(k, size = E, prob = theta) less_than_k &lt;- pbinom(k - 0.5, size = E, prob = theta)^(nschools - 1) prob_of_k * less_than_k } x &lt;- seq(0,.05, length = 100) y &lt;- Lschools(x) plot(x, y, type = &quot;l&quot;, xlab = &quot;theta&quot;, ylab = &quot;likelihood&quot;) 3.19 Programming Basics: Conditionals a function: ifelse(condition, yes_value, no_value) Carries out the test for each element in the vector. a special form: if (condition) {statements} else {statements} x &lt;- rnorm(10) table(x &lt; 0) ## ## FALSE TRUE ## 5 5 ifelse(x &gt;= 0, sqrt(x), &quot;bogus&quot;) ## Warning in sqrt(x): NaNs produced ## [1] &quot;0.415098097926984&quot; &quot;bogus&quot; &quot;0.78650648752133&quot; ## [4] &quot;bogus&quot; &quot;0.539146279368726&quot; &quot;1.72054962703838&quot; ## [7] &quot;0.889947136565264&quot; &quot;bogus&quot; &quot;bogus&quot; ## [10] &quot;bogus&quot; Both of these can be nested. 3.20 ifelse() examples library(ISLR) ## ## Attaching package: &#39;ISLR&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## College data(Default) # loan balances and defaults. head(Default) ## default student balance income ## 1 No No 729.5265 44361.625 ## 2 No Yes 817.1804 12106.135 ## 3 No No 1073.5492 31767.139 ## 4 No No 529.2506 35704.494 ## 5 No No 785.6559 38463.496 ## 6 No Yes 919.5885 7491.559 you_pay &lt;- with(Default, ifelse(balance / 10 &lt; income, 0.10 * balance, 0.05 * income)) sum(you_pay) ## [1] 835374.9 Determine annual payment amount for student loans. E.g. If it’s a student, no payment, otherwise $100 If it’s a student, no payment. Otherwise 10% of the balance. If the balance is less than 10 times the income, 10% of the balance, otherwise 5% of income. For those in default, nothing. For the others, any of the above three schemes. 3.21 if … else … examples Calculate the median of a set of values. if an odd number in the set, then the middle value if an even number in the set, the mean of the two values in the middle. my_median &lt;- function(x) { if (length(x) %% 2 == 1) { # odd length sort(x)[ceiling(length(x)/2)] } else { # even length inds &lt;- length(x)/2 + c(0, 1) mean(sort(x)[inds]) } } my_median(1:11) ## [1] 6 median(1:11) ## [1] 6 pmax() is a function that takes the case-by-case maximum of each of two input vectors. We’re going to add some error checking. my_pmax &lt;- function(v1, v2) ifelse(v1 &gt; v2, v1, v2) my_pmax3 &lt;- function(v1, v2, v3){ ifelse(v1 &gt;= v2, my_pmax(v1, v3), my_pmax(v2, v3) ) } my_pmax4 &lt;- function(v1, v2, v3, v4) { ifelse(v1 &gt;= v2, my_pmax3(v1, v3, v4), my_pmax3(v2, v3, v4)) } my_pmax_any &lt;- function(...) { vectors &lt;- list(...) if (length(vectors) == 2) my_pmax(...) if (length(vectors) == 3) my_pmax3(...) if (length(vectors) == 4) my_pmax4(...) } my_pmax(rnorm(10), rnorm(10)) ## [1] 1.05113667 0.42541644 0.98409896 0.98163834 -0.37525939 ## [6] 0.60408141 0.06943719 1.50049940 1.49312466 1.17298846 Unless the vectors are either numeric or character, and of the same type, throw an error. Add an argument handle_na which, if TRUE replaces NA with -Inf for the purposes of the comparison. Add an argument na_rid= if “either”, throw away the cases where either of the values is NA if “both”, throw away cases where both are NA and otherwise handle NA as -Inf if “neither”, keep all cases. 3.22 Simple Write functions that return, case by case, the maximum of two vectors. three vectors four vectors five vectors Write a supervisory function that does it for 1 to 5 vectors. Use ... max_in_parallel &lt;- function(...) { Vecs &lt;- list(...) } Write a supervisory function that will handle more than 5 vectors. 3.23 Blood testing It is known that 5% of the members of a population have disease X, which can be discovered by a blood test (that is assumed to perfectly identify both diseased and nondiseased populations). Suppose that N people are to be tested, and the cost of the test is nontrivial. The testing can be done in two ways: Everyone can be tested separately; or the blood samples of k people are pooled to be analyzed. Assume that N = nk with n being an integer. If the test is negative, all the people tested are healthy (that is, just this one test is needed). If the test result is positive, each of the k people must be tested separately (that is, a total of k + 1 tests are needed for that group). For fixed k what is the expected number of tests needed in (B)? Find the k that will minimize the expected number of tests in (B). Using the k that minimizes the number of tests, on average how many tests does (B) save in comparison with (A)? Be sure to check your answer using an empirical simulation. ntests &lt;- function(p = 0.05, npools = 500, pool_size = 10, nsims=1000) { # generate the number of infected people in each pool infected_in_pool &lt;- rbinom(npools, p = p, size = pool_size) # if one or more in a pool is infected, pool_size+1 tests, # otherwise 1 test tests_in_each_pool &lt;- ifelse(infected_in_pool &gt; 0, pool_size + 1, 1) # total across all pools sum(tests_in_each_pool) } Can we do this recursively to get more savings? people &lt;- runif(100000) &lt; .05 ntests &lt;- function(population) { if ( (! any(population)) || length(population) == 1) { # we&#39;re done! total_tests &lt;- 1 } else { # Split into two groups and test again split_point &lt;- round(length(population)/2) group1 &lt;- population[1:split_point] group2 &lt;- population[(split_point + 1) : length(population)] total_tests &lt;- ntests(group1) + ntests(group2) + 1 # + 1 for the test that said we need to divide the groups } total_tests } How many tests needed for a population of 10,000 with a prevalence of 1%? library(mosaic) do(10) * ntests(runif(10000) &lt; 0.01) ## ntests ## 1 1373 ## 2 1475 ## 3 1497 ## 4 1273 ## 5 1217 ## 6 1247 ## 7 1499 ## 8 1321 ## 9 1253 ## 10 1411 Try other prevalences to find prevalence at which it’s no longer worthwhile to pool the samples. 3.24 The (hyper)-volume of the hypersphere. A framework for the volumes: \\(C_n r^n\\). \\(n=1\\) — the line segment of length \\(2r\\). Volume is \\(2r^1\\) so \\(C_n = 2\\). \\(n=2\\) — the circle of radius \\(r\\): volume is \\(\\pi r^2\\), so \\(C_2 = \\pi\\) \\(n=3\\) — the sphere of radius \\(r\\): volume is \\(\\frac{4}{3}\\pi r^3\\), so \\(C_3 = \\frac{4}{3} \\pi\\) TASK: Find \\(C_4\\), \\(C_5\\), … , \\(C_8\\). Programming approach: Write the logic. Give explicit names to all quantities that you might want to change later. dim &lt;- 3 # might vary npts &lt;- 1000 # might vary pts &lt;- matrix(runif(dim * npts, min=0, max=1), ncol=dim) dists &lt;- rowSums( pts^2 ) (2^dim) * sum(dists &lt;= 1) / length(dists) ## [1] 4.312 Make the “might vary” quantities the arguments to a function that encapsulates the rest of the logic. sphere_volume &lt;- function(dim=3, npts=1000000) { pts &lt;- matrix(runif(dim * npts, min=0, max=1), ncol=dim) dists &lt;- rowSums( pts^2 ) (2^dim) * sum(dists &lt;= 1) / length(dists) } sapply(1:20, FUN = sphere_volume) ## [1] 2.000000 3.143072 4.192328 4.929392 5.285536 5.171712 4.689920 ## [8] 4.050944 3.212800 2.506752 1.961984 1.253376 0.892928 0.540672 ## [15] 0.360448 0.327680 0.393216 0.000000 0.000000 0.000000 Volume of the unit hyper-cube: 2 ^ (1:20) ## [1] 2 4 8 16 32 64 128 256 ## [9] 512 1024 2048 4096 8192 16384 32768 65536 ## [17] 131072 262144 524288 1048576 So the volume of the encompassed hyper-sphere goes to zero percent of the volume of the encompassing hyper-cube. Theoretical formula: \\(V_n(R) = \\frac{\\pi^{n/2}}{\\Gamma(\\frac{n}{2} + 1)}R^n\\) sapply(1:20, FUN=function(n) (pi^(n/2) / gamma(n/2 + 1)) ) ## [1] 2.00000000 3.14159265 4.18879020 4.93480220 5.26378901 5.16771278 ## [7] 4.72476597 4.05871213 3.29850890 2.55016404 1.88410388 1.33526277 ## [13] 0.91062875 0.59926453 0.38144328 0.23533063 0.14098111 0.08214589 ## [19] 0.04662160 0.02580689 3.25 Find the surface area, \\(D_n r^{n-1}\\). \\(D_1 = 0\\) \\(D_2 = 2 \\pi\\) \\(D_3 = 4 \\pi\\) Find \\(D_4\\), \\(D_5\\), …, \\(D_7\\) Two ways: Take the derivative wrt \\(r\\) of the volume. Find the fraction of points in a narrow shell between distance 0.99 and 1.01 from the origin. 3.26 In-class programming activity Explanation of draw poker cards: ranks and suits hands: royal flush, straight-flush, Actual result for you to compare your prediction to: one-hundred ninety-four out of three-hundred thirty-nine.↩ "],
["classifiers.html", "Topic 4 Classifiers 4.1 Classification overview 4.2 Day 10 preview", " Topic 4 Classifiers 4.1 Classification overview Response variable: categorical. Typically just a few levels: 2 or 3. Two types of outputs from classification models: The predicted category given the inputs Probability of each category given the inputs Type (2) can be fitted with maximum likelihood. Trade-offs: Flexibility vs interpretability Accuracy vs bias Four model architectures Logistic regression. Especially important for interpretability. Linear discriminant analysis Quadratic discriminant analysis K nearest neighbors 4.2 Day 10 preview Probability and odds Theme Song Making book Multivariate gaussians (Maybe) Programming activity: Poker hands "],
["logistic-regression.html", "Topic 5 Logistic Regression 5.1 Probability and odds 5.2 Log Odds 5.3 Why use odds? 5.4 Use of glm() 5.5 Interpretation of coefficients 5.6 Example: Logistic regression of default", " Topic 5 Logistic Regression 5.1 Probability and odds Probability \\(p(event)\\) is a number between zero and one. Simple way to make a probability model for yes/no variable: encode outcome as zero and one, use regression. Whickham$alive &lt;- as.numeric(with(Whickham, outcome == &quot;Alive&quot;)) Model of mortality in Whickham res &lt;- mean( alive ~ smoker, data=Whickham) res ## No Yes ## 0.6857923 0.7611684 res / (1-res) ## No Yes ## 2.182609 3.187050 mod2 &lt;- glm(alive ~ age, data=Whickham, family = &quot;binomial&quot;) f &lt;- makeFun(mod2) plotFun(f(age) ~ age, age.lim = c(20,100)) plotPoints(jitter(alive) ~ age, data=Whickham, add=TRUE, pch=20, alpha=.3) If we’re going to use likelihood to fit, the estimated probability can’t be \\(\\leq 0\\). 5.2 Log Odds Gerolamo Cardano (1501-1576) defined odds as the ratio of favorable to unfavorable outcomes. For an event whose probability is \\(p\\), it’s odds are \\(w = \\frac{p}{1-p}\\). A probability is a number between 0 and one. An odds is a ratio of two positive numbers. 5:9, 9:5, etc. “Odds are against it,” could be taken to mean that the odds is less than 1. More unfavorable outcomes than favorable ones. Given odds \\(w\\), the probability is \\(p = \\frac{w}{1+w}\\). There’s a one-to-one correspondence between probability and odds. The log odds is a number between \\(-\\infty\\) and \\(\\infty\\). 5.3 Why use odds? Making Book Several horses in a race. People bet on each one amounts \\(H_i\\). What should be the winnings when horse \\(j\\) wins? Payoff means you get your original stake back plus your winnings. If it’s arranged to pay winnings of \\(\\sum{i \\neq j} \\frac{H_i}{H_j}\\) + the amount \\(H_j\\) the net income will be zero for the bookie. Shaving the odds means to pay less than the zero-net-income winnings. Link function You can build a linear regression to predict the log odds, \\(\\ln w\\). The output of the linear regression is free to range from \\(-\\infty\\) to \\(\\infty\\). Then, to measure likelihood, unlog to get odds \\(w\\), then \\(p = \\frac{w}{1+w}\\). 5.4 Use of glm() Response should be 0 or 1. We don’t take the log odds of the response. Instead, the likelihood is - \\(p\\) if the outcome is 1 - \\(1-p\\) if the outcome is 0 Multiply these together of all the cases to get the total likelihood. 5.5 Interpretation of coefficients Each adds to the log odds in the normal, linear regression way. Negative means less likely; positive more likely. 5.6 Example: Logistic regression of default names(Default) ## [1] &quot;default&quot; &quot;student&quot; &quot;balance&quot; &quot;income&quot; ggplot(Default, aes(x = income, y = balance, alpha = default, color = default)) + geom_point() #+ facet_wrap( ~ student) model_of_default &lt;- glm(default == &quot;Yes&quot; ~ balance + income, data = Default, family = &quot;binomial&quot;) f &lt;- makeFun(model_of_default) plotFun(f(income=income, balance=balance) ~ income + balance, income.lim &lt;- c(0,70000), balance.lim = c(0, 3000)) summary(model_of_default) ## ## Call: ## glm(formula = default == &quot;Yes&quot; ~ balance + income, family = &quot;binomial&quot;, ## data = Default) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4725 -0.1444 -0.0574 -0.0211 3.7245 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.154e+01 4.348e-01 -26.545 &lt; 2e-16 *** ## balance 5.647e-03 2.274e-04 24.836 &lt; 2e-16 *** ## income 2.081e-05 4.985e-06 4.174 2.99e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2920.6 on 9999 degrees of freedom ## Residual deviance: 1579.0 on 9997 degrees of freedom ## AIC: 1585 ## ## Number of Fisher Scoring iterations: 8 logodds &lt;- predict(model_of_default, newdata = list(balance = 1000, income = 40000)) #, # type = &quot;response&quot;) logodds ## 1 ## -5.061006 odds &lt;- exp(logodds) odds / (1 + odds) ## 1 ## 0.006299244 logistic &lt;- function(x) {exp(x) / (1 + exp(x))} logistic(-3.36) ## [1] 0.03356922 table(Default$default) ## ## No Yes ## 9667 333 "],
["linear-and-quadratic-discriminant-analysis.html", "Topic 6 Linear and Quadratic Discriminant Analysis 6.1 Example: Default on student loans 6.2 A Bayes’ Rule approach 6.3 Univariate Gaussian 6.4 Uncorrelated bivariate gaussian 6.5 Bivariate normal distribution with correlations 6.6 Shape of multivariate gaussian 6.7 Generating bivariate normal from independent 6.8 Independent variables \\(x_i\\) 6.9 Re-explaining \\(\\boldsymbol\\Sigma\\) 6.10 LDA 6.11 QDA 6.12 Error test rates on various classifiers 6.13 Error rates 6.14 Receiver operating curves", " Topic 6 Linear and Quadratic Discriminant Analysis 6.1 Example: Default on student loans model_of_default2 &lt;- lda(default ~ balance + income, data = Default) model_of_default2 ## Call: ## lda(default ~ balance + income, data = Default) ## ## Prior probabilities of groups: ## No Yes ## 0.9667 0.0333 ## ## Group means: ## balance income ## No 803.9438 33566.17 ## Yes 1747.8217 32089.15 ## ## Coefficients of linear discriminants: ## LD1 ## balance 2.230835e-03 ## income 7.793355e-06 predict(model_of_default2, newdata = list(balance = 3000, income = 40000)) ## $class ## [1] Yes ## Levels: No Yes ## ## $posterior ## No Yes ## 1 0.008136798 0.9918632 ## ## $x ## LD1 ## 1 4.879445 sector_mod &lt;- lda(sector ~ wage + educ, data = CPS85) sector_mod ## Call: ## lda(sector ~ wage + educ, data = CPS85) ## ## Prior probabilities of groups: ## clerical const manag manuf other prof ## 0.18164794 0.03745318 0.10299625 0.12734082 0.12734082 0.19662921 ## sales service ## 0.07116105 0.15543071 ## ## Group means: ## wage educ ## clerical 7.422577 12.93814 ## const 9.502000 11.15000 ## manag 12.704000 14.58182 ## manuf 8.036029 11.19118 ## other 8.500588 11.82353 ## prof 11.947429 15.63810 ## sales 7.592632 13.21053 ## service 6.537470 11.60241 ## ## Coefficients of linear discriminants: ## LD1 LD2 ## wage 0.06196785 -0.2108914 ## educ 0.43349567 0.2480535 ## ## Proportion of trace: ## LD1 LD2 ## 0.9043 0.0957 predict(sector_mod, newdata = list(wage = 10, educ = 16)) ## $class ## [1] prof ## Levels: clerical const manag manuf other prof sales service ## ## $posterior ## clerical const manag manuf other prof ## 1 0.1619905 0.005807399 0.1680084 0.02274195 0.04429026 0.4781032 ## sales service ## 1 0.07668742 0.04237084 ## ## $x ## LD1 LD2 ## 1 1.352846 0.5336987 all_combos &lt;- expand.grid(wage = seq(0,20,length=100), educ = seq(5,16, length = 100)) res &lt;- predict(sector_mod, newdata = all_combos)$class all_combos$predicted &lt;- res ggplot(all_combos, aes(x = wage, y = educ, color = predicted)) + geom_point() 6.2 A Bayes’ Rule approach Suppose we have \\(K\\) classes, \\(A_1, A_2, \\ldots, A_K\\). We also have a set of inputs \\(x_1, x_2, \\ldots, x_p \\equiv {\\mathbf x}\\). We observe \\({\\mathbf x}\\) and we want to know \\(p(A_j \\downarrow {\\mathbf x})\\). This is a posterior probability. Per usual, the quantities we can get from our training data are in the form of a likelihood: \\(p({\\mathbf x} \\downarrow A_j)\\). Given a prior \\(p(A_j)\\) for all K classes, we can flip the likelihood into a posterior. In order to define our likelihood \\(p({\\mathbf x} \\downarrow A_j)\\), we need both the training data and a model form for the probability \\(p()\\). A standard (but not necessarily good) model of a distribution is a multivariate Gaussian. LDA and QDA are based on a multi-variable Gaussian. 6.3 Univariate Gaussian \\[p(x) = \\underbrace{\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}}_{Normalization} \\underbrace{\\exp(- \\frac{(x-m)^2}{2 \\sigma^2})}_{Shape}\\] Imagine that we have another variable \\(z = x/3\\). Geometrically, \\(z\\) is a stretched out version of \\(x\\), stretched by a factor of 3 around the mean. The distribution is \\[p(z) = \\underbrace{\\frac{1}{\\sqrt{2 \\pi (3\\sigma)^2}}}_{Normalization}\\ \\underbrace{\\exp(- \\frac{(x-m)^2}{2 (3\\sigma)^2})}_{Shape}\\] Note how the normalization changes. \\(p(z)\\) is broader than \\(p(x)\\), so it must also be shorter. The R function dnorm() calculates \\(p(x)\\) for a univariate Gaussian. 6.4 Uncorrelated bivariate gaussian For independent RVs x and y, p(xy) = p(x)p(y). Show that the normalization is \\(\\frac{1}{2 \\pi \\sigma_x \\sigma_y}\\). The sigmas multiply in the normalization, like the area of something being stretched out in two orthogonal directions. 6.5 Bivariate normal distribution with correlations \\[f(x,y) = \\frac{1}{2 \\pi \\sigma_X \\sigma_Y \\sqrt{1-\\rho^2}} \\exp\\left( -\\frac{1}{2(1-\\rho^2)}\\left[ \\frac{(x-\\mu_X)^2}{\\sigma_X^2} + \\frac{(y-\\mu_Y)^2}{\\sigma_Y^2} - \\frac{2\\rho(x-\\mu_X)(y-\\mu_Y)}{\\sigma_X \\sigma_Y} \\right] \\right)\\] If \\(\\rho &gt; 0\\) and \\(x\\) and \\(y\\) are both above their respective means, the correlation term makes the result less surprising: a larger probability. Another way of writing this same formula is using a covariate matrix \\({\\boldsymbol\\Sigma}\\). Or, in matrix form \\[(2\\pi)^{-\\frac{k}{2}}|\\boldsymbol\\Sigma|^{-\\frac{1}{2}}\\, \\exp\\left( -\\frac{1}{2}(\\mathbf{x}-\\boldsymbol\\mu)&#39;\\boldsymbol\\Sigma^{-1}(\\mathbf{x}-\\boldsymbol\\mu) \\right)\\] where \\[\\boldsymbol \\Sigma \\equiv \\left(\\begin{array}{cc}\\sigma_x^2 &amp; \\rho \\sigma_x \\sigma_y\\\\\\rho\\sigma_x\\sigma_y&amp; \\sigma_y^2\\end{array} \\right)\\] Therefore \\[\\boldsymbol \\Sigma^{-1} \\equiv \\frac{1}{\\sigma_x^2 \\sigma_y^2 (1 - \\rho^2)} \\left(\\begin{array}{cc}\\sigma_y^2 &amp; - \\rho \\sigma_x \\sigma_y\\\\ - \\rho\\sigma_x\\sigma_y&amp; \\sigma_x^2\\end{array} \\right)\\] 6.6 Shape of multivariate gaussian As an amplitude plot Showing marginals and 3-\\(\\sigma\\) contour 6.7 Generating bivariate normal from independent We want to find a matrix, M, by which to multiply iid Z to get correlated X with specified \\(\\sigma_x, \\sigma_y, \\rho\\). The covariance matrix will be # parameters sigma_x &lt;- 3 sigma_y &lt;- 1 rho &lt;- 0.5 Sigma &lt;- matrix(c(sigma_x^2, rho * sigma_x * sigma_y, rho * sigma_x * sigma_y, sigma_y^2), nrow = 2) n &lt;- 5000 # number of simulated cases # iid base Z &lt;- cbind(rnorm(n), rnorm(n)) M &lt;- matrix(c(sigma_x, 0, rho * sigma_y, sqrt(1-rho^2)* sigma_y), nrow = 2) X &lt;- Z %*% M cov(X) ## [,1] [,2] ## [1,] 8.667761 1.4328505 ## [2,] 1.432851 0.9602763 M transforms from iid to correlated. In formula, we transform from correlated X to iid, so use M\\(^{-1}\\). 6.8 Independent variables \\(x_i\\) Describing dependence x1 = runif(1000) x2 = rnorm(1000, mean=3*x1+2, sd=x1) plot(x1, x2) Linear correlations and the Gaussian Remember the univariate Gaussian with parameters \\(\\mu\\) and \\(\\sigma^2\\): \\[\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left(-\\frac{(x - \\mu)^2}{2 \\sigma^2}\\right)\\] Situation: Build a classifier. We measure some features and want to say which group a case refers to. Specific example: Based on the ISLR::Default data, find the probability of a person defaulting on a loan given their income and balance. names(Default) ## [1] &quot;default&quot; &quot;student&quot; &quot;balance&quot; &quot;income&quot; ggplot(Default, aes(x = income, y = balance, alpha = default, color = default)) + geom_point() We were looking at the likelihood: prob(observation | class) Note: Likelihood itself won’t do a very good job, since defaults are relatively uncommon. That is, p(default) \\(\\ll\\) p(not). 6.9 Re-explaining \\(\\boldsymbol\\Sigma\\) Figure 6.1: Figure 4.8 from ISL Imagine two zero-mean variables, \\(x_i\\) and \\(x_j\\), e.g. education and age, and suppose that \\(v_i = x_i - \\mu_i\\) and \\(v_j = x_j - \\mu_j\\). I’ll write these in data table format, where each column is a variable and each row is a case and denote this by \\(\\left(\\begin{array}{c}v_i\\\\\\downarrow\\end{array}\\right)\\) and \\(\\left(\\begin{array}{c}v_j\\\\\\downarrow\\end{array}\\right)\\) Correlations between random variables \\((v_i, v_j)\\) are created by overlapping sums of zero-mean iid random variables \\((z_i, z_j)\\), \\[\\left(\\begin{array}{cc}v_i &amp;v_j\\\\\\downarrow &amp; \\downarrow\\end{array}\\right) = \\left(\\begin{array}{cc}z_i &amp; z_j \\\\ \\downarrow &amp; \\downarrow\\end{array}\\right) \\left(\\begin{array}{cc}a &amp; b\\\\0 &amp; c\\end{array}\\right) \\equiv \\left(\\begin{array}{cc}z_i &amp; z_j\\\\\\downarrow &amp; \\downarrow\\end{array}\\right) {\\mathbf A} \\] and add in a possibly non-zero mean to form each \\(x\\). \\[\\left(\\begin{array}{cc}x_i &amp;x_j\\\\\\downarrow&amp;\\downarrow\\end{array}\\right) = \\left(\\begin{array}{cc}v_i &amp; v_j\\\\\\downarrow&amp;\\downarrow\\end{array}\\right) + \\left(\\begin{array}{cc}m_i &amp; m_j\\\\\\downarrow&amp;\\downarrow\\end{array}\\right) \\] where each of \\(\\left(\\begin{array}{c}m_i\\\\|\\end{array}\\right)\\) and \\(\\left(\\begin{array}{c}m_i\\\\|\\end{array}\\right)\\) have every row the same. The covariance matrix \\(\\boldsymbol\\Sigma\\) is \\[{\\boldsymbol \\Sigma} \\equiv \\frac{1}{n} \\left(\\begin{array}{cc}v_i &amp; v_j\\\\\\downarrow&amp;\\downarrow\\end{array}\\right)^T \\left(\\begin{array}{cc}v_i &amp; v_j\\\\\\downarrow&amp;\\downarrow\\end{array}\\right) = \\frac{1}{n} \\left(\\begin{array}{c}v_i \\longrightarrow\\\\v_j \\longrightarrow\\end{array}\\right) \\left(\\begin{array}{cc}v_i &amp; v_j\\\\\\downarrow&amp;\\downarrow\\end{array}\\right) \\] Substituting in \\[ \\left(\\begin{array}{cc}v_i &amp;v_j\\\\\\downarrow &amp; \\downarrow\\end{array}\\right) = \\left(\\begin{array}{cc}z_i &amp; z_j\\\\\\downarrow &amp; \\downarrow\\end{array}\\right) {\\mathbf A} \\] we get \\[{\\boldsymbol \\Sigma} = \\frac{1}{n} \\left[\\left(\\begin{array}{cc}z_i &amp; z_j\\\\\\downarrow &amp; \\downarrow\\end{array}\\right) {\\mathbf A} \\right]^T \\left(\\begin{array}{cc}z_i &amp; z_j\\\\\\downarrow &amp; \\downarrow\\end{array}\\right) {\\mathbf A} = \\frac{1}{n} {\\mathbf A}^T \\left(\\begin{array}{c}z_i \\longrightarrow\\\\z_j \\longrightarrow\\end{array}\\right) \\left(\\begin{array}{cc}z_i &amp; z_j\\\\\\downarrow &amp; \\downarrow\\end{array}\\right) {\\mathbf A}\\] Now \\(\\left(\\begin{array}{cc}z_i &amp; z_j\\\\\\downarrow &amp; \\downarrow\\end{array}\\right)\\) are iid with zero mean and unit variance, so \\[\\left(\\begin{array}{c}z_i \\longrightarrow\\\\z_j \\longrightarrow\\end{array}\\right) \\left(\\begin{array}{cc}z_i &amp; z_j\\\\\\downarrow &amp; \\downarrow\\end{array}\\right) = \\left(\\begin{array}{cc}1 &amp; 0\\\\0 &amp; 1\\end{array}\\right)\\] so \\({\\boldsymbol \\Sigma} = {\\boldsymbol A}^T {\\boldsymbol A}\\). In other words, \\(\\boldsymbol A\\) is the Choleski decomposition of \\(\\boldsymbol \\Sigma\\). Operationalizing this in R Find \\(\\boldsymbol \\Sigma\\) from data: cov(data), e.g. library(dplyr) Sigma &lt;- cov(ISLR::Default %&gt;% dplyr::select(balance, income)) Sigma ## balance income ## balance 233980.2 -982142.3 ## income -982142.3 177865954.8 Find \\(\\boldsymbol A\\) from \\(\\boldsymbol \\Sigma\\) A &lt;- chol(Sigma) A ## balance income ## balance 483.715 -2030.415 ## income 0.000 13181.175 Generate iid \\(\\left(\\begin{array}{cc}z_i&amp;z_j\\\\\\downarrow&amp;\\downarrow\\end{array}\\right)\\) with unit variance n &lt;- 100 # say Z &lt;- cbind(rnorm(n), rnorm(n)) Create the correlated \\(\\left(\\begin{array}{cc}v_i&amp;v_j\\\\\\downarrow&amp;\\downarrow\\end{array}\\right)\\) from \\(\\left(\\begin{array}{cc}z_i&amp;z_j\\\\\\downarrow&amp;\\downarrow\\end{array}\\right)\\) V &lt;- Z %*% A Create a set of means \\(\\left(\\begin{array}{cc}m_i&amp;m_j\\\\\\downarrow&amp;\\downarrow\\end{array}\\right)\\) to add on to \\(\\left(\\begin{array}{cc}v_i&amp;v_j\\\\\\downarrow&amp;\\downarrow\\end{array}\\right)\\) M &lt;- cbind(rep(3, n), rep(-2, n)) head(M) ## [,1] [,2] ## [1,] 3 -2 ## [2,] 3 -2 ## [3,] 3 -2 ## [4,] 3 -2 ## [5,] 3 -2 ## [6,] 3 -2 Add \\(\\left(\\begin{array}{cc}m_i&amp;m_j\\\\\\downarrow&amp;\\downarrow\\end{array}\\right)\\) to \\(\\left(\\begin{array}{cc}v_i&amp;v_j\\\\\\downarrow&amp;\\downarrow\\end{array}\\right)\\) to create \\(\\left(\\begin{array}{cc}x_i&amp;x_j\\\\\\downarrow&amp;\\downarrow\\end{array}\\right)\\) X &lt;- V + M Find the covariance matrix for \\(\\left(\\begin{array}{cc}x_i&amp;x_j\\\\\\downarrow&amp;\\downarrow\\end{array}\\right)\\) cov(X) ## balance income ## balance 208366.0 -861153.5 ## income -861153.5 160224764.8 Why isn’t this exactly the same as the covariance matrix \\(\\boldsymbol \\Sigma\\) that we were aiming at? Because of random fluctuations in the \\(\\left(\\begin{array}{cc}z_i&amp;z_j\\\\\\downarrow&amp;\\downarrow\\end{array}\\right)\\). You can reduce the impact of those fluctuations by making \\(n\\) bigger. Notice that \\({\\boldsymbol A}\\) transforms from uncorrelated \\(\\left(\\begin{array}{cc}z_i&amp;z_j\\\\\\downarrow&amp;\\downarrow\\end{array}\\right)\\) to correlated \\(\\left(\\begin{array}{cc}v_i&amp;v_j\\\\\\downarrow&amp;\\downarrow\\end{array}\\right)\\). If we have \\(\\left(\\begin{array}{cc}x_i&amp;x_j\\\\\\downarrow&amp;\\downarrow\\end{array}\\right)\\), we can create the uncorrelated \\(\\left(\\begin{array}{cc}z_i&amp;z_j\\\\\\downarrow&amp;\\downarrow\\end{array}\\right)\\) with \\[\\left(\\begin{array}{cc}z_i&amp;z_j\\\\\\downarrow&amp;\\downarrow\\end{array}\\right) = \\left[\\left(\\begin{array}{cc}x_i&amp;x_j\\\\\\downarrow&amp;\\downarrow\\end{array}\\right) - \\left(\\begin{array}{cc}m_i&amp;m_j\\\\\\downarrow&amp;\\downarrow\\end{array}\\right)\\right] {\\boldsymbol A}^{-1}\\]. Recall \\[(2\\pi)^{-\\frac{k}{2}}|\\boldsymbol\\Sigma|^{-\\frac{1}{2}}\\, \\exp\\left( -\\frac{1}{2}(\\mathbf{x}-\\boldsymbol\\mu)^T\\boldsymbol\\Sigma^{-1}(\\mathbf{x}-\\boldsymbol\\mu) \\right)\\]. Since \\(\\boldsymbol\\Sigma = \\boldsymbol A^T \\boldsymbol A\\) and \\(x_i - m = v_i\\), this formula is equivalent to \\[(2\\pi)^{-\\frac{k}{2}}|\\boldsymbol\\Sigma|^{-\\frac{1}{2}}\\, \\exp\\left( -\\frac{1}{2}(\\mathbf{v}^{T}\\boldsymbol A^{-T} \\boldsymbol A^{-1} \\mathbf v) \\right) = (2\\pi)^{-\\frac{k}{2}}|\\boldsymbol\\Sigma|^{-\\frac{1}{2}}\\, \\exp\\left( -\\frac{1}{2}(\\mathbf{z}^{T}\\mathbf z) \\right)\\] Now for a pair of values like \\(\\mathbf x = (x_1 \\ \\ x_2)\\) finding the probability of \\(\\mathbf x\\) corresponds to finding the corresponding \\(\\mathbf z^T = (z_i\\ \\ z_j)\\), where \\(z_i\\) and \\(z_j\\) are each a random scalars, and \\((\\mathbf{z}^{T}\\mathbf z) = z_i^2 + z_2^2\\), so the probability is \\[(2\\pi)^{-\\frac{k}{2}}|\\boldsymbol\\Sigma|^{-\\frac{1}{2}}\\, \\exp\\left( -\\frac{1}{2}(z_1^2 + z_2^2 )\\right) = (2\\pi)^{-\\frac{k}{2}}|\\boldsymbol\\Sigma|^{-\\frac{1}{2}}\\, \\exp(-\\frac{z_1^2}{2}) \\exp(-\\frac{z_2^2}{2})\\] Look at the stretching that goes on due to a matrix: figure source The stretching is due to the matrix \\(\\boldsymbol A\\). So we should divide by the determinant of \\(\\boldsymbol A\\), that is, \\(|\\boldsymbol A|\\). The nature of the Cholesky decomposition is that \\(|\\boldsymbol A| = \\sqrt{|\\boldsymbol\\Sigma|}\\). Note in the formula for the Gaussian that the normalizing constant involves \\(\\sqrt{|\\boldsymbol\\Sigma|}\\). 6.10 LDA All classes are treated as having the same \\({\\mathbf \\Sigma}\\). 6.11 QDA Classes are treated with different \\({\\mathbf \\Sigma}_i\\). knitr::include_graphics(&quot;Images/Chapter-4/4.9.png&quot;) Figure 6.2: Figure 4.9 from ISL. Left: Bayes (purple dashed), LDA (black dotted), and QDA (green solid)} decision boundaries for a two-class problem with \\({\\mathbf \\Sigma}_1 = {\\mathbf \\Sigma}_2\\). Right: QDA 6.12 Error test rates on various classifiers Figure 6.3: Figure 4.10 from ISL Scenarios: In all, class means are different. Each class is two uncorrelated Gaussian random vars. Both classes had a correlation of \\(-0.5\\) Uncorrelated, like (1), but the distribution is t(df=?): long tailed to right. Figure 6.4: Figure 4.11 from ISL Like (2), but one class has \\(\\rho = 0.5\\) and the other \\(\\rho = -0.5\\) A nonlinear predictor with \\(X_1^2\\), \\(X_2^2\\), \\(X_1 \\times X_2\\) giving a quadratic decision boundary A decision boundary more complicated than a quadratic. 6.13 Error rates Ways to measure the performance of a classifier. Examples: Two classifiers of employment type. data(CPS85, package = &quot;mosaicData&quot;) classifier_1 &lt;- lda(sector ~ wage + educ + age, data = CPS85) classifier_2 &lt;- qda(sector ~ wage + educ + age, data = CPS85) Confusion matrix table(CPS85$sector, predict(classifier_1)$class) ## ## clerical const manag manuf other prof sales service ## clerical 60 0 0 1 17 17 0 2 ## const 7 0 2 4 5 0 0 2 ## manag 15 0 5 1 3 31 0 0 ## manuf 22 0 4 13 14 7 0 8 ## other 21 0 2 6 24 5 0 10 ## prof 15 0 2 0 6 81 0 1 ## sales 25 0 1 0 2 8 0 2 ## service 38 0 1 9 12 7 0 16 Rates for right-vs-wrong. Accuracy. Total error rate. Not generally useful, because there are two ways to be wrong. table(predict(classifier_1)$class == CPS85$sector) / nrow(CPS85) ## ## FALSE TRUE ## 0.6273408 0.3726592 Sensitivity: If the real class is X, the probability that the classifier will produce an output of X. We need to choose the output we care about. Let’s use prof. is_prof &lt;- CPS85$sector == &quot;prof&quot; predicts_prof &lt;- predict(classifier_1)$class == &quot;prof&quot; table(is_prof, predicts_prof) ## predicts_prof ## is_prof FALSE TRUE ## FALSE 354 75 ## TRUE 24 81 105 actual prof, of which 81 were correctly classified. So, sensitivity is 81/105. Specificity: If the real class is not X, the probability that the classifier will output not X. See above table. 429 not prof, of which 354 were correctly classified. So specificity is 354/429. Loss functions Social awkwardness of thinking someone is in the wrong profession. 6.14 Receiver operating curves There’s always one or more parameters that can be set in a classifier. This might be as simple as the prior probability. As this parameter is changed, typically sensitivity will go up at the cost of specificity, or vice versa. President Garfield’s assassination. "],
["cross-validation-and-bootstrapping.html", "Topic 7 Cross-Validation and Bootstrapping 7.1 Philosophical approaches 7.2 Operationalizing model choice 7.3 Some definitions of “better” 7.4 Training and Testing 7.5 Trade-off 7.6 Classical theory interlude 7.7 Bootstrapping", " Topic 7 Cross-Validation and Bootstrapping In making a statistical model (including with machine-learning techniques) we have an overall goal: Make model error small for the ways the model will actually be applied. Model error has three components: irreducible error (\\(\\epsilon\\)) error due to bias error due to variance Bias refers to the ways that \\(\\hat{f}({\\mathbf X})\\) is systematically different from the idealized \\(f({\\mathbf X})\\). Variance refers to the ways that \\(\\hat{f}_i({\\mathbf X})\\) varies with the particular sample \\(i\\) used for training. The overall model error is the sum of squares of these three components. There’s nothing to be done about irreducible error. we want to do two things that can conflict with one another: make bias small. flexible models QDA rather than LDA include lots of explanatory variables make variance small lots of data few degrees of freedom (vs. flexibility or QDA) avoid collinearity and weak/irrelevant explanatory variables 7.1 Philosophical approaches Often, these are just drawing attention to the problem; they don’t actually offer a solution. 7.1.1 Occam’s Razor: A heuristic Non sunt multiplicanda entia sine necessitate. Entities must not be multiplied beyond necessity. But what’s “beyond necessity?” 7.1.2 Einstein’s proverb Man muß die Dinge so einfach wie möglich machen. Aber nicht einfacher. Make things as simple as possible, but not one bit simpler. But what’s “too simple?” Basic modeling-building question: Does adding this [variable, model term, potential flexibility] help? For reducing bias: yes For reducing variance: maybe (if it eats up residual variance) For reducing error: maybe 7.2 Operationalizing model choice We need to be able to measure two aspects of a model: bias: theory is difficult, since we don’t know the “true” system variance: theory is straightforward Bias in practice: Have a good measure of variance. Add in new model terms, flexibility, etc. See if the variance goes down. Think in terms of comparing two models to select which one is better. “Better” needs to be transitive: if A is better than B, and B is better than C, then A is better than C. Then, any number of models can be compared to find the one (or more) that is the best. 7.3 Some definitions of “better” Larger likelihood (non-iid Gaussian error models) Smaller mean square prediction error (same as larger likelihood with iid Gaussian error model) Classification error rate is smaller Loss function is smaller 7.4 Training and Testing Evaluation of performance using training data is biased to give larger likelihood (smaller MSE or classification error or loss error). Unbiased evaluation is done on separate testing data. 7.5 Trade-off Need large testing dataset for good estimate of performance Need large training dataset for reducing variance in fit. How to get both: Collect a huge amount of data. When this works, go for it! K-fold cross validation Pull out 1/K part of the data for performance testing. Fit to the other (K-1)/K part of the data. Repeat K times and average the prediction results over the K trials. Once you’ve found the best form of model, fit it to the whole data set. That’s your model. 7.6 Classical theory interlude M-estimators and robust estimation. 7.7 Bootstrapping Figure 7.1: ISLR Figure 5.10 In-class demonstration. How many cases don’t get used in a typical resample? Hint: It will suffice to work with indices, so the numbers 1:n for a sample of size n. cases &lt;- 1:10000 ntrials &lt;- 50 result &lt;- numeric(ntrials) for (k in 1:50) { bootstrap_sample &lt;- sample(cases, size=length(cases), replace = TRUE) result[k] &lt;- length(setdiff(cases, unique(bootstrap_sample))) / length(cases) } result ## [1] 0.3647 0.3686 0.3633 0.3693 0.3661 0.3742 0.3680 0.3651 0.3671 0.3696 ## [11] 0.3671 0.3687 0.3663 0.3719 0.3678 0.3622 0.3744 0.3691 0.3703 0.3678 ## [21] 0.3677 0.3673 0.3639 0.3677 0.3685 0.3664 0.3709 0.3695 0.3688 0.3680 ## [31] 0.3638 0.3665 0.3714 0.3735 0.3638 0.3656 0.3676 0.3688 0.3783 0.3663 ## [41] 0.3678 0.3628 0.3731 0.3735 0.3741 0.3729 0.3592 0.3732 0.3662 0.3679 ## Or ... (1 - 1/length(cases))^length(cases) ## [1] 0.367861 library(mosaic) do(100) * { n &lt;- 100000 s &lt;- sample(1:n, size = n, replace = TRUE) n - length(unique(s)) } ## result ## 1 36798 ## 2 36631 ## 3 36705 ## 4 36724 ## 5 36846 ## 6 36784 ## 7 36725 ## 8 36742 ## 9 36825 ## 10 36768 ## 11 36646 ## 12 36839 ## 13 36965 ## 14 36608 ## 15 36789 ## 16 36748 ## 17 36728 ## 18 36849 ## 19 36640 ## 20 36888 ## 21 36846 ## 22 36882 ## 23 36794 ## 24 36731 ## 25 36875 ## 26 36754 ## 27 36793 ## 28 36720 ## 29 36556 ## 30 36679 ## 31 36787 ## 32 36801 ## 33 36644 ## 34 36852 ## 35 36713 ## 36 36649 ## 37 36707 ## 38 36615 ## 39 36753 ## 40 36555 ## 41 36776 ## 42 36900 ## 43 36829 ## 44 36977 ## 45 36787 ## 46 36610 ## 47 36952 ## 48 36853 ## 49 36698 ## 50 36927 ## 51 37008 ## 52 36688 ## 53 36734 ## 54 36848 ## 55 36780 ## 56 36872 ## 57 36796 ## 58 36785 ## 59 36680 ## 60 36813 ## 61 36646 ## 62 36816 ## 63 36696 ## 64 36813 ## 65 36904 ## 66 36689 ## 67 36690 ## 68 36760 ## 69 36807 ## 70 36748 ## 71 36672 ## 72 36866 ## 73 36801 ## 74 36760 ## 75 36672 ## 76 36767 ## 77 36945 ## 78 36713 ## 79 36765 ## 80 36584 ## 81 36802 ## 82 36912 ## 83 36711 ## 84 36876 ## 85 36682 ## 86 36704 ## 87 36824 ## 88 36728 ## 89 36752 ## 90 36844 ## 91 36740 ## 92 36816 ## 93 36797 ## 94 36823 ## 95 36975 ## 96 36734 ## 97 36845 ## 98 36694 ## 99 36771 ## 100 36714 "],
["regularization-shrinkage-and-dimension-reduction.html", "Topic 8 Regularization, shrinkage and dimension reduction 8.1 Best subset selection 8.2 Approximation to best subset selection 8.3 Classical theory of best model choice 8.4 Optimization 8.5 Shrinkage methods 8.6 LASSO 8.7 Review 8.8 Multi-collinearity 8.9 Creating correlations 8.10 Rank 1 Matrices 8.11 Idea of singular values. 8.12 Dimension reduction", " Topic 8 Regularization, shrinkage and dimension reduction 8.1 Best subset selection Algorithm 6.1 from ISLR Let \\(M_0\\) denote the null model, which contains no predictors. This model simply predicts the sample mean for each observation. For k in 1,2,…p: Fit all p models that contain exactly k predictors. \\(\\left(\\begin{array}{c}k\\\\p\\end{array}\\right)\\) Pick the best among these k models, and call it \\(M_k\\). Here best is defined as having the smallest RSS, or equivalently largest R\\(^2\\). Select a single best model from among \\(M_0, \\ldots , M_p\\) using cross- validated prediction error, C\\(_p\\), AIC, BIC, or adjusted \\(R^2\\). Figure 8.1: ISLR Figure 6.1. The models from best subset selection 8.2 Approximation to best subset selection For large \\(p\\), there are too many possible models to fit all of them: \\(2^p\\). So, some heuristics. There are only \\(p\\) models with just one term: \\(d = 1\\). So easy to try all of them. There are only \\(p(p-1)/2\\) models with just two terms \\(d = 2\\). Again, easy to try all of them. Starting out from \\(M_1\\) or \\(M_2\\), keep all of those terms and look for the best individual term to add. There will be only \\(p-(d-1)\\) of them. We presume that one of these will be near the frontier of the best possible model with \\(d\\) terms. Repeat the process \\(k \\gg p\\) times, moving forward and back randomly, adding a term or deleting a term. This will take roughly \\(k p\\) fits \\(\\alpha p^2\\) fits, where \\(\\alpha\\) is a constant \\(k/p\\), say 10. Compare this to \\(2^p\\). Setting \\(\\alpha = 10\\), find the ratio \\(\\frac{2^p}{\\alpha p^2}\\) for \\(p = 5, \\ldots, 40\\). p &lt;- 1:40 (2^p) / (10 * p^2) ## [1] 2.000000e-01 1.000000e-01 8.888889e-02 1.000000e-01 1.280000e-01 ## [6] 1.777778e-01 2.612245e-01 4.000000e-01 6.320988e-01 1.024000e+00 ## [11] 1.692562e+00 2.844444e+00 4.847337e+00 8.359184e+00 1.456356e+01 ## [16] 2.560000e+01 4.535363e+01 8.090864e+01 1.452321e+02 2.621440e+02 ## [21] 4.755447e+02 8.665917e+02 1.585748e+03 2.912711e+03 5.368709e+03 ## [26] 9.927347e+03 1.841121e+04 3.423922e+04 6.383721e+04 1.193046e+05 ## [31] 2.234634e+05 4.194304e+05 7.887911e+05 1.486148e+06 2.804877e+06 ## [36] 5.302429e+06 1.003937e+07 1.903587e+07 3.614437e+07 6.871948e+07 Exhaustion seems find up through about \\(d = 20\\) — only 100 times more expensive than the random search. 8.3 Classical theory of best model choice We punish models with lots of terms. In-sample Adjusted Out-of-sample \\(\\frac{1}{n}\\)RSS \\(C_p = \\frac{1}{n}(\\mbox{RSS} + 2 d \\hat{\\sigma}^2)\\) cross-validated prediction error . \\(\\mbox{AIC} = -2 \\ln {\\cal L} - 2 d\\) . . \\(\\mbox{AIC}_{ls} = \\frac{1}{\\hat{\\sigma}^2}C_p\\) . . \\(\\mbox{BIC} = \\frac{1}{n} (\\mbox{RSS} + \\ln(n) d \\hat{\\sigma}^2)\\) . R\\(^2\\) Adjusted R\\(^2\\) ??? \\(\\mbox{Adjusted R}^2 = 1 - \\frac{\\mbox{RSS}/(n-d-1)}{\\mbox{TSS}{(n-1)}}\\) Figure 8.2: ISLR Figure 6.2. Note that the values on the vertical axis are the best for that “number of predictors. Uncertainty Repeat the analysis for different test sets or using different folds in k-fold cross validation. At each value of “Number of Predictors”, there will be a distribution. One-standard-error rule: select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve. 8.4 Optimization 8.4.1 What are we optimizing over? Choose the best set of columns from the model matrix. Small &lt;- mosaic::sample(mosaicData::KidsFeet, size = 5) row.names(Small) &lt;- NULL M1 &lt;- model.matrix(width ~ length + sex, data = Small) M2 &lt;- model.matrix(width ~ length + sex*biggerfoot, data = Small) M3 &lt;- model.matrix(width ~ length*domhand*sex + sex*biggerfoot, data = Small) 8.5 Shrinkage methods Example, a roughly quadratic cloud of points. Better to fit it with a constant, straight line, a quadratic? Depends on the amount of data. What if you have only n=3 or 4? Constant will have the least sampling variation but the most bias. Quadratic will have the least bias but the most sampling variation. Shrinkage idea: Make a linear combination of the constant model with the “full” model. 8.5.1 Ridge regression The objective function: minimize \\[\\sum_{i=1}^n \\left(y_i - \\beta_0 - \\sum_{j = 1}^p \\beta_j x_{ij}\\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\\] Figure 8.3: ISLR Figure 6.4. Note the y-axis label: “Standardized Coefficients.” Remember that coefficients have units that depend on the response variable and the explanatory variable(s) participating in the coefficient. Those coefficients will typically be different from model term to model term. Meaningless to add up numbers with different physical dimension. … So, standardize the explanatory variables. Another perspective on the reason to standardize: Some of the coefficients might be huge numerically, others small. In such a situation, the huge coefficients will dominate; the small ones will have little influence on the shrinkage. data(CPS85, package = &quot;mosaic&quot;) ## Warning in data(CPS85, package = &quot;mosaic&quot;): data set &#39;CPS85&#39; not found x &lt;- model.matrix(wage ~ age + educ + sex + union + married, data = CPS85) foo &lt;- glmnet(x, CPS85$wage, alpha = 0) foo$lambda.min ## NULL predict(foo,s=foo$lambda.min,exact=T,type=&quot;coefficients&quot;) ## 7 x 100 sparse Matrix of class &quot;dgCMatrix&quot; ## [[ suppressing 100 column names &#39;s0&#39;, &#39;s1&#39;, &#39;s2&#39; ... ]] ## ## (Intercept) 9.024064e+00 8.9845500258 8.9807084640 8.9764945908 ## (Intercept) . . . . ## age 7.833801e-38 0.0002223183 0.0002439361 0.0002676496 ## educ 7.580412e-37 0.0021509145 0.0023600259 0.0025894031 ## sexM 2.137431e-36 0.0060625175 0.0066516621 0.0072978516 ## unionUnion 2.184744e-36 0.0061916460 0.0067927982 0.0074520479 ## marriedSingle -1.097614e-36 -0.0031093285 -0.0034110717 -0.0037419469 ## ## (Intercept) 8.971872561 8.9668031280 8.9612433278 8.9551461420 ## (Intercept) . . . . ## age 0.000293661 0.0003221914 0.0003534828 0.0003878003 ## educ 0.002840997 0.0031169438 0.0034195819 0.0037514703 ## sexM 0.008006567 0.0087838090 0.0096361411 0.0105707449 ## unionUnion 0.008174955 0.0089676003 0.0098366324 0.0107893174 ## marriedSingle -0.004104736 -0.0045024806 -0.0049385043 -0.0054164378 ## ## (Intercept) 8.9484601281 8.9411290199 8.933091297 8.9242797166 ## (Intercept) . . . . ## age 0.0004254338 0.0004667008 0.000511948 0.0005615549 ## educ 0.0041154088 0.0045144596 0.004951971 0.0054316019 ## sexM 0.0115954738 0.0127189125 0.013950443 0.0153003109 ## unionUnion 0.0118335936 0.0129781289 0.014232384 0.0156066793 ## marriedSingle -0.0059402456 -0.0065142539 -0.007143182 -0.0078321724 ## ## (Intercept) 8.9146208149 8.9040343629 8.8924327880 8.8797205546 ## (Intercept) . . . . ## age 0.0006159361 0.0006755443 0.0007408743 0.0008124655 ## educ 0.0059573508 0.0065335834 0.0071650649 0.0078569941 ## sexM 0.0167797050 0.0184008330 0.0201770102 0.0221227462 ## unionUnion 0.0171122647 0.0187613975 0.0205674209 0.0225448498 ## marriedSingle -0.0085868290 -0.0094132503 -0.0103180689 -0.0113084920 ## ## (Intercept) 8.8657934983 8.8505381222 8.833830848 8.81553723 ## (Intercept) . . . . ## age 0.0008909068 0.0009768399 0.001070964 0.00117404 ## educ 0.0086150384 0.0094453731 0.010354721 0.01135040 ## sexM 0.0242538476 0.0265875170 0.029142461 0.03193901 ## unionUnion 0.0247094586 0.0270783749 0.029670175 0.03250498 ## marriedSingle -0.0123923429 -0.0135781042 -0.014874962 -0.01629285 ## ## (Intercept) 8.795511112 8.773593781 8.749613049 8.723382330 ## (Intercept) . . . . ## age 0.001286896 0.001410431 0.001545623 0.001693531 ## educ 0.012440352 0.013633217 0.014938358 0.016365920 ## sexM 0.034999211 0.038346996 0.042008261 0.046011017 ## unionUnion 0.035604563 0.038992442 0.042693981 0.046736492 ## marriedSingle -0.017842503 -0.019535481 -0.021384232 -0.023402122 ## ## (Intercept) 8.694699697 8.66334692 8.629088521 8.591670836 ## (Intercept) . . . . ## age 0.001855303 0.00203218 0.002225506 0.002436727 ## educ 0.017926885 0.01963311 0.021497409 0.023533552 ## sexM 0.050385511 0.05516435 0.060382633 0.066078045 ## unionUnion 0.051149320 0.05596393 0.061213970 0.066935341 ## marriedSingle -0.025603471 -0.02800358 -0.030618769 -0.033466351 ## ## (Intercept) 8.550821131 8.506246777 8.457634524 8.404649905 ## (Intercept) . . . . ## age 0.002667402 0.002919208 0.003193942 0.003493528 ## educ 0.025756363 0.028181734 0.030826673 0.033709334 ## sexM 0.072290980 0.079064621 0.086445009 0.094481084 ## unionUnion 0.073166217 0.079947059 0.087320592 0.095331734 ## marriedSingle -0.036564669 -0.039933055 -0.043591796 -0.047562072 ## ## (Intercept) 8.346936807 8.284117277 8.21579158 8.141538616 ## (Intercept) . . . . ## age 0.003820021 0.004175608 0.00456261 0.004983484 ## educ 0.036849035 0.040266269 0.04398269 0.048021111 ## sexM 0.103224702 0.112730595 0.12305630 0.134262014 ## unionUnion 0.104027489 0.113456777 0.12367019 0.134719696 ## marriedSingle -0.051865865 -0.056525832 -0.06156514 -0.067007279 ## ## (Intercept) 8.060916717 7.973464940 7.878704903 7.776143265 ## (Intercept) . . . . ## age 0.005440816 0.005937322 0.006475838 0.007059311 ## educ 0.052405411 0.057160500 0.062312194 0.067887072 ## sexM 0.146410399 0.159566289 0.173796323 0.189168465 ## unionUnion 0.146658213 0.159539140 0.173415734 0.188340405 ## marriedSingle -0.072875762 -0.079193859 -0.085984206 -0.093268375 ## ## (Intercept) 7.665274929 7.54558705 7.416563927 7.27769283 7.1284709 ## (Intercept) . . . . . ## age 0.007690785 0.00837338 0.009110276 0.00990468 0.0107598 ## educ 0.073912294 0.08041537 0.087423891 0.09496517 0.1030659 ## sexM 0.205751424 0.22361395 0.242824003 0.26344778 0.2855486 ## unionUnion 0.204363867 0.22153418 0.239895671 0.25948770 0.2803434 ## marriedSingle -0.101066377 -0.10939610 -0.118272683 -0.12770782 -0.1377091 ## ## (Intercept) 6.96841279 6.79705996 6.61399012 6.41882823 6.2112579 ## (Intercept) . . . . . ## age 0.01167879 0.01266472 0.01372055 0.01484901 0.0160526 ## educ 0.11175163 0.12104636 0.13097192 0.14154739 0.1527884 ## sexM 0.30918583 0.33441321 0.36127774 0.38981798 0.4200625 ## unionUnion 0.30248814 0.32593832 0.35069962 0.37676565 0.4041166 ## marriedSingle -0.14827895 -0.15941437 -0.17110566 -0.18333586 -0.1960801 ## ## (Intercept) 5.9910334 5.7579921 5.51206649 5.25329617 4.98183860 ## (Intercept) . . . . . ## age 0.0173335 0.0186935 0.02013392 0.02165556 0.02325861 ## educ 0.1647066 0.1773089 0.19059658 0.20456512 0.21920324 ## sexM 0.4520282 0.4857189 0.52112390 0.55821629 0.59695227 ## unionUnion 0.4327179 0.4625192 0.49345370 0.52543758 0.55837010 ## marriedSingle -0.2093050 -0.2229683 -0.23701831 -0.25139453 -0.26602738 ## ## (Intercept) 4.69797856 4.40213581 4.09487021 3.77688286 3.44901969 ## (Intercept) . . . . . ## age 0.02494258 0.02670626 0.02854762 0.03046385 0.03245118 ## educ 0.23449256 0.25040721 0.26691358 0.28397034 0.30152843 ## sexM 0.63727009 0.67908961 0.72231213 0.76682031 0.81248048 ## unionUnion 0.59213404 0.62659667 0.66161129 0.69701946 0.73265266 ## marriedSingle -0.28083903 -0.29574427 -0.31065174 -0.32546539 -0.34008662 ## ## (Intercept) 3.11226065 2.76771551 2.41661109 2.06027583 1.70012110 ## (Intercept) . . . . . ## age 0.03450504 0.03661995 0.03878962 0.04100694 0.04326407 ## educ 0.31953158 0.33791673 0.35661486 0.37555188 0.39464982 ## sexM 0.85914169 0.90663875 0.95479401 1.00341984 1.05232141 ## unionUnion 0.76833644 0.80389287 0.83914426 0.87391666 0.90804331 ## marriedSingle -0.35441592 -0.36835537 -0.38181085 -0.39469431 -0.40692595 ## ## (Intercept) 1.33761998 0.97428376 0.61163733 0.25119401 -0.10556916 ## (Intercept) . . . . . ## age 0.04555258 0.04786348 0.05018744 0.05251487 0.05483612 ## educ 0.41382804 0.43300458 0.45209753 0.47102648 0.48971376 ## sexM 1.10129972 1.15015470 1.19868837 1.24670797 1.29402881 ## unionUnion 0.94136790 0.97374751 1.00505497 1.03518081 1.06403449 ## marriedSingle -0.41843607 -0.42916675 -0.43907299 -0.44812347 -0.45630085 ## ## (Intercept) -0.45723496 -0.80246738 -1.14003052 -1.46880417 -1.78779598 ## (Intercept) . . . . . ## age 0.05714158 0.05942192 0.06166817 0.06387188 0.06602526 ## educ 0.50808576 0.52607395 0.54361585 0.56065574 0.57714518 ## sexM 1.34047696 1.38589155 1.43012672 1.47305307 1.51455874 ## unionUnion 1.09154508 1.11766129 1.14235091 1.16559986 1.18741066 ## marriedSingle -0.46360157 -0.47003518 -0.47562331 -0.48039836 -0.48440181 ## ## (Intercept) -2.09614984 -2.39315067 -2.6782256 -2.95094196 -3.21100222 ## (Intercept) . . . . . ## age 0.06812123 0.07015356 0.0721169 0.07400677 0.07581963 ## educ 0.59304334 0.60831709 0.6229410 0.63689689 0.65017384 ## sexM 1.55455004 1.59295153 1.6297059 1.66477322 1.69813027 ## unionUnion 1.20780080 1.22680071 1.2444518 1.26080428 1.27591536 ## marriedSingle -0.48768253 -0.49029496 -0.4922973 -0.49374981 -0.49471329 ## ## (Intercept) -3.45823680 -3.69259470 -3.91413280 -4.12300419 -4.31944604 ## (Intercept) . . . . . ## age 0.07755284 0.07920461 0.08077398 0.08226072 0.08366532 ## educ 0.66276734 0.67467888 0.68591532 0.69648818 0.70641303 ## sexM 1.72976918 1.75969624 1.78793039 1.81450170 1.83944980 ## unionUnion 1.28984720 1.30266526 1.31443681 1.32522960 1.33511079 ## marriedSingle -0.49524770 -0.49541101 -0.49525830 -0.49484107 -0.49420673 ## ## (Intercept) -4.50376732 -4.67633677 -4.83757141 -4.98792574 -5.12788178 ## (Intercept) . . . . . ## age 0.08498886 0.08623295 0.08739966 0.08849141 0.08951095 ## educ 0.71570878 0.72439708 0.73250174 0.74004811 0.74706270 ## sexM 1.86282238 1.88467367 1.90506303 1.92405372 1.94171161 ## unionUnion 1.34414606 1.35239889 1.35993009 1.36679741 1.37305531 ## marriedSingle -0.49339833 -0.49245445 -0.49140918 -0.49029227 -0.48912933 ## ## (Intercept) -5.25794016 -5.3786121 -5.49041268 -5.59385451 -5.6894432 ## (Intercept) . . . . . ## age 0.09046123 0.0913454 0.09216668 0.09292837 0.0936338 ## educ 0.75357262 0.7596053 0.76518802 0.77034787 0.7751113 ## sexM 1.95810424 1.9732998 1.98736634 2.00037119 2.0123803 ## unionUnion 1.37875481 1.3839435 1.38866554 1.39296178 1.3968698 ## marriedSingle -0.48794212 -0.4867489 -0.48556453 -0.48440129 -0.4832687 ## ## (Intercept) -5.77767290 -5.85902333 -5.93390991 -6.00287296 -6.06628568 ## (Intercept) . . . . . ## age 0.09428624 0.09488895 0.09544397 0.09595664 0.09642875 ## educ 0.77950393 0.78355064 0.78727494 0.79070015 0.79384759 ## sexM 2.02345769 2.03366541 2.04306574 2.05170968 2.05965435 ## unionUnion 1.40042429 1.40365684 1.40658991 1.40926359 1.41169502 ## marriedSingle -0.48217421 -0.48112327 -0.48012751 -0.47917354 -0.47827092 For Credit dataset: http://www-bcf.usc.edu/~gareth/ISL/Credit.csv 8.6 LASSO The objective function: minimize \\[\\sum_{i=1}^n \\left(y_i - \\beta_0 - \\sum_{j = 1}^p \\beta_j x_{ij}\\right)^2 + \\lambda \\sum_{j=1}^p |\\beta_i|\\] Almost exactly the same as ridge regression. But the small change makes a big difference. Figure 8.4: ISLR Figure 6.7. x &lt;- model.matrix(wage ~ age * educ * sex * union * married, data = CPS85) foo &lt;- cv.glmnet(x, CPS85$wage, alpha = 1) foo$lambda.min ## [1] 0.1263735 predict(foo,s=foo$lambda.min,exact=T,type=&quot;coefficients&quot;) ## 33 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) -1.9708098459 ## (Intercept) . ## age . ## educ 0.5482123152 ## sexM . ## unionUnion 1.1057230664 ## marriedSingle . ## age:educ 0.0054884080 ## age:sexM 0.0559924024 ## educ:sexM . ## age:unionUnion . ## educ:unionUnion . ## sexM:unionUnion . ## age:marriedSingle . ## educ:marriedSingle . ## sexM:marriedSingle . ## unionUnion:marriedSingle . ## age:educ:sexM 0.0003101264 ## age:educ:unionUnion . ## age:sexM:unionUnion . ## educ:sexM:unionUnion . ## age:educ:marriedSingle . ## age:sexM:marriedSingle . ## educ:sexM:marriedSingle -0.0453647898 ## age:unionUnion:marriedSingle . ## educ:unionUnion:marriedSingle . ## sexM:unionUnion:marriedSingle . ## age:educ:sexM:unionUnion . ## age:educ:sexM:marriedSingle . ## age:educ:unionUnion:marriedSingle -0.0002003640 ## age:sexM:unionUnion:marriedSingle . ## educ:sexM:unionUnion:marriedSingle . ## age:educ:sexM:unionUnion:marriedSingle . 8.7 Review We started by considering one mathematical representation of the problem: Choose \\(\\beta_1, \\beta_2, \\ldots, \\beta_p\\) to minimize this objective function \\[\\sum_{i=1}^n \\left(y_i - \\beta_0 - \\sum_{j = 1}^p \\beta_j x_{ij}\\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\\] The result depends on the value of \\(\\lambda\\), which is often called the Lagrange multiplier. The motivation for this form of the objective function is rooted in two different and conflicting goals: Get the model as close as possible to the data: minimize the sum of square errors. Keep the \\(\\beta\\)’s small: minimize the model variance. Reductio ad absurdum argument. Suppose \\(\\lambda \\rightarrow \\infty\\). Then the best \\(\\beta\\)’s would be zeros. Such a model has zero model variance, since we’ll get exactly the same result on every data set used to train the model. Another way to think about multi-objective optimization … We have several goals as a function of the adjustable parameters. Let’s denote them by \\(g_1(x), g_2(x), g_3(x), \\ldots\\). For instance, suppose \\(g_1(x)\\) is the appeal of a meal as a function of the ingredients, \\(g_2(x)\\) is the vitamin content, \\(g_3(x)\\) is the protein content, \\(g_4(x)\\) is the amount of salt, and so on. We want to find the parameters \\(x\\) that minimize some mixture of the objectives, e.g. \\[T(x; \\lambda_1, \\lambda_2, \\ldots) \\equiv \\lambda_1 g_1(x) + \\lambda_2 g_2(x) + \\ldots\\] We don’t know what the \\(\\lambda\\)’s are; they reflect the relative importance of the different components of the objective function. We could, of course, make them up. But instead … Let’s choose one of the components to be our sole objective. It can be any of the components, the end result will be the same regardless. The others we will consider as constraints. For instance, we might take taste (\\(g_1(x)\\)) as our objective but impose constraints: calories should be between 800 and 1200, salt should be below 1200 mg, protein should be above 50 grams. Consider all the values of \\(x\\) that satisfy the constraints. Search over these to find the one that maximizes our selected objective component. This is called constrained optimization. Consider each of the constraints. Find out how much the selected objective will improve if we relax that constraint a bit, say allowing salt to be 1300 mg. The ratio is the Lagrange multiplier \\(\\lambda_i\\). \\[\\frac{\\mbox{change in taste}}{\\mbox{change in constraint}_i} \\equiv \\lambda_i\\] Now ask questions about priorities: If I were to ease up on the constraint, would I get enough change in the objective to be worthwhile? Or, if I were to tighten the constraint, would the resulting change in objective be acceptable? Change the constraints accordingly until you no longer see a net gain by changing the constraints. Let’s translate this to ridge and lasso regression. The selected component of the overall objective: Minimize the in-sample sum of square error. The constraint: how big are we willing to make the coefficients? (Remember, by standardizing the variables we make it possible to calculate a total size.) Evaluating the trade-offs between constraints and objectives: does the out-of-sample prediction error get smaller. The picture: the red shows the objective function the blue shows the permitted region for the coefficients to satisfy the constraint. Figure 8.5: ISLR Figure 6.7. We can imagine increasing or decreasing the constraints, and will get a different optimum at each level of the constraint. Why the circle and diamond? Three different “metrics,” that is three different ways of combining parts to get an overall size. \\(L^2\\) — square-root of the sum of squares. The usual Euclidean distance. Circle \\(L^1\\) — sum of the absolute values. Sometimes called the Manhattan metric. Diamond \\(L^\\infty\\) — the biggest individual value. Square. Predicting Salary in the ISLR::Hitters data: Without_NA &lt;- na.omit(ISLR::Hitters) inds &lt;- sample(nrow(Without_NA), size = nrow(Without_NA)/2) Train &lt;- Without_NA[inds,] Test &lt;- Without_NA[-inds,] y_all &lt;- Without_NA$Salary x_all &lt;- model.matrix(Salary ~ ., data=Without_NA) y_train &lt;- Train$Salary x_train &lt;- model.matrix(Salary ~ ., data=Train) y_test &lt;- Test$Salary x_test &lt;- model.matrix(Salary ~ ., data=Test) ridge_mod &lt;- cv.glmnet(x_train, y_train, alpha = 0) ridge_mod$lambda.min ridge_pred &lt;- predict(ridge_mod, s=0, newx = x_test, exact=TRUE) mean((ridge_pred - y_test)^2) final &lt;- glmnet(x_all, y_all, alpha=0) predict(final, type=&quot;coefficients&quot;, s=ridge_mod$lambda.min) Lasso: Do we really need all of those variables? lasso_mod &lt;- cv.glmnet(x_train, y_train, alpha = 1) lasso_mod$lambda.min lasso_pred &lt;- predict(lasso_mod, s=0, newx = x_test, exact=TRUE) mean((lasso_pred - y_test)^2) final &lt;- glmnet(x_all, y_all, alpha=1) predict(final, type=&quot;coefficients&quot;, s=lasso_mod$lambda.min) Figure 8.6: ISLR Figure 6.4 Figure 8.7: ISLR Figure 6.7 8.8 Multi-collinearity The SAT story. summary(lm(sat ~ expend, data=mosaicData::SAT))$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1089.29372 44.389950 24.539197 8.168276e-29 ## expend -20.89217 7.328209 -2.850925 6.407965e-03 summary(lm(sat ~ expend + ratio, data=mosaicData::SAT))$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1136.335547 107.803485 10.5408053 5.693212e-14 ## expend -22.307944 7.955544 -2.8040751 7.313013e-03 ## ratio -2.294539 4.783836 -0.4796442 6.337049e-01 summary(lm(sat ~ expend + ratio + salary, data=mosaicData::SAT))$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1069.234168 110.924940 9.6392585 1.292219e-12 ## expend 16.468866 22.049899 0.7468907 4.589302e-01 ## ratio 6.330267 6.542052 0.9676272 3.382908e-01 ## salary -8.822632 4.696794 -1.8784372 6.666771e-02 mosaic::rsquared(lm(expend ~ ratio + salary, data=mosaicData::SAT)) ## [1] 0.893476 Figure 8.8: Confidence interval explanation Figure 8.9: Confidence interval explanation, part 2 load(&quot;Images/mona.rda&quot;) rankMatrix(mona) ## [1] 191 ## attr(,&quot;method&quot;) ## [1] &quot;tolNorm2&quot; ## attr(,&quot;useGrad&quot;) ## [1] FALSE ## attr(,&quot;tol&quot;) ## [1] 5.551115e-14 # pick a vector at random; column 151 versus the first 131 columns mosaic::rsquared(lm(t(mona)[,151] ~ t(mona)[,-151])) ## [1] 0.9999636 Variance inflation factor \\(\\mbox{VIF}(\\beta_j) = \\frac{1}{1 - R^2_{x_j|x_{-j}}}\\) Getting rid of vectors that correlate substantially with one another can reduce the variance inflation factor. 8.9 Creating correlations Generate points on circles of radius 1, 2, 3, … make_circles &lt;- function(radii = 1:2, nangs = 30) { theta = seq(0, 2*pi, length = nangs) x &lt;- rep(cos(theta), length(radii)) y &lt;- rep(sin(theta), length(radii)) r &lt;- rep(radii, each = nangs) col &lt;- rep(rainbow(nangs), length(radii)) data.frame(x = x * r, y = y * r, r = r, col = col) } transform_circles &lt;- function(M, circles = NULL) { if (is.null(circles)) circles &lt;- make_circles() XY &lt;- rbind(circles$x, circles$y) new &lt;- M %*% XY circles$x = new[1, ] circles$y = new[2, ] circles } Trans &lt;- matrix(c(1, 2, -3, -1), nrow = 2, byrow = TRUE) After_trans &lt;- transform_circles(Trans) plot(y ~ x, data = After_trans, col = (After_trans$col), asp = 1, pch = 20) svals &lt;- svd(Trans) Start &lt;- make_circles() plot(y ~ x, data = Start, col = Start$col, asp = 1, pch = 20) After_V &lt;- transform_circles(t(svals$v), Start) plot(y ~ x, data = After_V, col = After_V$col, asp = 1, pch = 20) After_V_lambda &lt;- transform_circles(diag(svals$d), After_V) plot(y ~ x, data &lt;- After_V_lambda, col = After_V_lambda$col, asp = 1, pch = 20) After_V_lambda_U &lt;- transform_circles(svals$u, After_V_lambda) plot(y ~ x, data = After_V, col = After_V_lambda_U$col, asp = 1, pch = 20) 8.10 Rank 1 Matrices Suppose all the columns in a matrix are simple multiples of the first column. All of the columns would be exactly collinear, so lm() will produce NA for all but one of the coefficients (putting the intercept aside). Lasso would zero out all but one of the coefficients, ridge would … set.seed(101) one &lt;- outer(rnorm(100), rnorm(10)) two &lt;- outer(rnorm(100), rnorm(10)) three &lt;- outer(rnorm(100), rnorm(10)) y &lt;- rnorm(100) + 7 * one[, 1] - 2 * two[, 1] x &lt;- one + two dim(one) ## [1] 100 10 lm(y ~ x) ## ## Call: ## lm(formula = y ~ x) ## ## Coefficients: ## (Intercept) x1 x2 x3 x4 ## -0.06554 -17.08557 -10.65980 NA NA ## x5 x6 x7 x8 x9 ## NA NA NA NA NA ## x10 ## NA lasso_mod &lt;- cv.glmnet(x, y, alpha = 1) lasso_mod$lambda.min ## [1] 0.02322399 predict(lasso_mod, type=&quot;coefficients&quot;, s=lasso_mod$lambda.min) ## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) -0.06467677 ## V1 . ## V2 . ## V3 . ## V4 . ## V5 . ## V6 -3.00961477 ## V7 . ## V8 . ## V9 -2.19570742 ## V10 . ridge_mod &lt;- cv.glmnet(one, y, alpha = 0) ridge_mod$lambda.min ## [1] 2.046288 predict(ridge_mod, type=&quot;coefficients&quot;, s=ridge_mod$lambda.min) ## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 0.12330285 ## V1 0.52231401 ## V2 -0.23757058 ## V3 0.06598347 ## V4 0.11986807 ## V5 0.18788200 ## V6 -0.60749645 ## V7 1.59250740 ## V8 -0.06389472 ## V9 -0.29849103 ## V10 0.08247090 8.11 Idea of singular values. Find orthogonal vectors to describe the ellipsoidal cloud. The singular value describes “how long” each ellipsoidal axis is. Correlation \\(R^2_{x_j | x_{-j}}\\) gets increased for each direction that overlaps between \\(x_j\\) and \\(x_{-j}\\) — it doesn’t matter how big the singular value is in that direction. Only by throwing out directions can we reduce \\(R^2_{x_j | x_{-j}}\\) Nice illustration: Source 8.12 Dimension reduction Re-arrange the variables to squeeze the juice out of them. Matrix Approximate matrix in a least squares sense. If that approximation includes the same column or more, we can discard the repeats. Outer product Rank-1 matrix constructed by creating multiples of one column. Create another vector and another rank-1 matrix. Add it up and we get closer to the target. Creating those singular vectors: singular value decomposition \\({\\mathbf D}\\) gives information on how big they are orthogonal to one another cumulative sum of \\({\\mathbf D}\\) components gives the amount of variance in the approximation. res &lt;- svd(mona) approx &lt;- 0 for (i in 1:10) { approx &lt;- approx + outer(res$u[,i], res$v[,i]) * res$d[i] } image(approx, asp=1) Picture in terms of gaussian cloud. The covariance matrix tells all that you need. Using pcr() to fit models, interpreting the output. library(pls) ## ## Attaching package: &#39;pls&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## loadings pcr.fit &lt;- pcr(Salary ~ ., data = ISLR::Hitters, scale=TRUE, validation=&quot;CV&quot;) summary(pcr.fit) ## Data: X dimension: 263 19 ## Y dimension: 263 1 ## Fit method: svdpc ## Number of components considered: 19 ## ## VALIDATION: RMSEP ## Cross-validated using 10 random segments. ## (Intercept) 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps ## CV 452 354.3 353.2 353.7 351.6 348.7 345.2 ## adjCV 452 353.9 352.7 353.1 350.9 348.0 344.3 ## 7 comps 8 comps 9 comps 10 comps 11 comps 12 comps 13 comps ## CV 345.8 349.3 351.5 357.6 358.9 361.6 362.3 ## adjCV 344.9 348.2 350.3 356.0 357.1 359.8 360.4 ## 14 comps 15 comps 16 comps 17 comps 18 comps 19 comps ## CV 355.4 355 346.7 346.4 346.0 349.4 ## adjCV 353.3 353 344.6 344.3 343.7 346.9 ## ## TRAINING: % variance explained ## 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps 7 comps ## X 38.31 60.16 70.84 79.03 84.29 88.63 92.26 ## Salary 40.63 41.58 42.17 43.22 44.90 46.48 46.69 ## 8 comps 9 comps 10 comps 11 comps 12 comps 13 comps 14 comps ## X 94.96 96.28 97.26 97.98 98.65 99.15 99.47 ## Salary 46.75 46.86 47.76 47.82 47.85 48.10 50.40 ## 15 comps 16 comps 17 comps 18 comps 19 comps ## X 99.75 99.89 99.97 99.99 100.00 ## Salary 50.55 53.01 53.85 54.61 54.61 validationplot(pcr.fit, val.type = &quot;MSEP&quot;) "],
["nonlinearity-in-linear-models.html", "Topic 9 Nonlinearity in linear models 9.1 Smoothers 9.2 Steps 9.3 Other functions 9.4 Holes in the data 9.5 Bootstrapping 9.6 Normal theory confidence bands 9.7 Splines 9.8 GAMS", " Topic 9 Nonlinearity in linear models Basic idea: Use linear regression to construct models \\(f(x)\\) that are nonlinear in \\(x\\). 9.1 Smoothers Sometimes the model form we want to impose is described by broad properties: The “smoothness” of the model. The generalizability of the model, e.g. extrapolation outside the range of the inputs The broad class of model forms used here are called smoothers. They are a linear combination of a set of functions, called basis functions, that have nice properties. 9.1.1 Ideas of smoothness Continuity: Is the \\(n\\)th derivative continuous? The higher is \\(n\\), the smoother is the function. Bumpiness: The integral of the absolute value of the 2nd derivative. Aside: Is continuity always good? Regression discontinuity video 9.1.2 Polynomials Polynomials have been central to math education for a long time, and there has been a rich theory of them since around the 13th century. For instance: The fundamental theorem of algebra states that every non-constant single-variable polynomial with complex coefficients has at least one complex root. This includes polynomials with real coefficients, since every real number is a complex number with an imaginary part equal to zero. Polynomials are completely smooth in the sense of continuity: all derivatives are continuous. But they may be bumpy And their behavior for large and small \\(x\\) is crazy. 9.1.3 The model matrix The set of basis functions evaluated at the inputs \\(x\\). make_model_matrix &lt;- function(x, basis_funs) { MM &lt;- matrix(0, nrow=length(x), ncol=length(basis_funs)) for (i in 1:length(basis_funs)) { MM[,i] = basis_funs[[i]](x) } return(MM) } Polynomial basis functions: polynomial_basis_3 &lt;- list( function(x) 1, function(x) x, function(x) x^2, function(x) x^3 ) monomial &lt;- function(k) function(x) x^k make_polynomial_basis &lt;- function(p) { lapply(0:p, FUN=monomial) } show_smoother(basis=make_polynomial_basis(10), data=sample(CPS85, size=100), bootstrap=10, confidence=TRUE) ## Warning in sqrt(rowSums(MMM %*% vcov(mod) * MMM)): NaNs produced 9.1.4 Sigmoidal Functions sigmoidal_basis &lt;- list( function(x) 1, function(x) dnorm(x, mean=25, sd = 10), function(x) dnorm(x, mean=40, sd = 15), function(x) dnorm(x, mean=55, sd=10) ) 9.1.5 Hat functions hat &lt;- function(from, width) function(x) {ifelse(x&gt;from &amp; x&lt;(from+width), 1, 0)} hat_basis &lt;- list( function(x) 1, hat(20, 10), hat(30, 10), hat(40, 10), hat(43, 17), hat(22, 6), hat(50, 10) ) 9.1.6 Fourier analysis E.g. taking a function apart into Fourier components — sines and cosines fourier &lt;- function(fun, period) { function(x) fun(2*pi*x / period) } fourier_set &lt;- list( function(x) 1, # fourier(cos, 5), # fourier(cos, 10), fourier(cos, 20), fourier(cos, 30), # fourier(sin, 5), # fourier(sin, 10), fourier(sin, 20), fourier(sin, 30)) show_smoother(basis=fourier_set, data=mosaic::sample(Wage, size=100), bootstrap=20, confidence=TRUE) show_smoother(basis=hat_basis, data=mosaic::sample(Wage, size=100), bootstrap=20, confidence=FALSE) 9.2 Steps step_fun &lt;- function(where) { function(x) ifelse(x &gt; where, 1, 0)} step_basis &lt;- list( function(x) 1, step_fun(30), step_fun(35), step_fun(50), step_fun(55) ) show_smoother(basis=step_basis, data=mosaic::sample(Wage, size=100), bootstrap=0, confidence=FALSE) 9.3 Other functions triangles gaussian: dnorm() sigmoidals: pnorm() spline basis: 1, \\(x\\), \\(x^2\\), \\(x^3\\), \\((x-\\xi_j)^3_+\\) 9.4 Holes in the data Leave out the middle of the data 9.5 Bootstrapping 9.6 Normal theory confidence bands covariance matrix for model coefficients rowSums(MM %*% cov * MM) data(SwimRecords, package = &quot;mosaicData&quot;) model_formula &lt;- time ~ year MM &lt;- model.matrix(model_formula, data=SwimRecords) mod &lt;- lm(model_formula, data = SwimRecords) SE &lt;- rowSums(MM %*% vcov(mod) * MM) top &lt;- fitted(mod) + 2*SE bottom &lt;- fitted(mod) - 2*SE #if(!is.null(knitr::current_input() )) plot(time ~ year, data=SwimRecords) points(MM[,2], top, pch=20) points(MM[,2], bottom, pch=20) 9.7 Splines 9.7.1 B-splines library(splines) x &lt;- seq(0, 100, by = 0.1) funs &lt;- splines::bs(x, df = 15) plot(x, funs[,1], type=&quot;l&quot;) for (k in 1:ncol(funs)) lines(x, funs[,k], col=topo.colors(ncol(funs))[k]) mod &lt;- lm(time ~ sex * splines::bs(year, df=5), data=SwimRecords) plot(time ~ year, data=SwimRecords) with(SwimRecords, points(year, fitted(mod), pch=20)) For_plotting &lt;- expand.grid(sex = c(&quot;F&quot;, &quot;M&quot;), year = x) preds &lt;- predict(mod, newdata = For_plotting) plot(preds ~ year, col = sex, pch=20, data = For_plotting) with(SwimRecords, points(year, time)) 9.7.2 Natural splines funs &lt;- splines::ns(x, df = 15) plot(x, funs[,1], type=&quot;l&quot;) for (k in 1:ncol(funs)) lines(x, funs[,k], col=topo.colors(ncol(funs))[k]) mod &lt;- lm(time ~ sex * splines::bs(year, df=5), data=SwimRecords) plot(time ~ year, data=SwimRecords) with(SwimRecords, points(year, fitted(mod), pch=20)) For_plotting &lt;- expand.grid(sex = c(&quot;F&quot;, &quot;M&quot;), year = x) preds &lt;- predict(mod, newdata = For_plotting) plot(preds ~ year, col = sex, pch=20, data = For_plotting) with(SwimRecords, points(year, time)) 9.7.3 Smoothing splines mod &lt;- with(SwimRecords, smooth.spline(year, time, df = 5)) multiples &lt;- duplicated( SwimRecords$time ) mod2 &lt;- with(SwimRecords, smooth.spline(year, time, cv = TRUE)) ## Warning in smooth.spline(year, time, cv = TRUE): cross-validation with non- ## unique &#39;x&#39; values seems doubtful mod2$df ## [1] 3.47454 plot(time ~ year, data=SwimRecords) lines(mod, col=&quot;red&quot;, lwd=2) lines(mod2, col=&quot;blue&quot;, lwd=3) 9.7.4 Smoothers in k dimensions 9.8 GAMS SOMETHING’S WRONG library(gam) mod &lt;- gam(time ~ sex * s(year, 2), data=SwimRecords) preds &lt;- predict(mod, newdata=X) plot(X$year, preds) "],
["programming-activity.html", "Topic 10 Programming Activity", " Topic 10 Programming Activity "],
["where-to-place-knots.html", "Topic 11 Where to place knots?", " Topic 11 Where to place knots? Show the ecdf() of some data. Invert this to get the quantiles. Show mosaic::qdata() and mosaic::pdata() as inverses. quantile() is the original version of this. "],
["trees-for-regression-and-classification.html", "Topic 12 Trees for Regression and Classification 12.1 Splitting Criteria for Classification Trees 12.2 Variable importance 12.3 Avoiding overfitting 12.4 Pruning 12.5 Averaging 12.6 Shrinking (“Boosting”)", " Topic 12 Trees for Regression and Classification We’re working on a straightforward framework for regression and classification modeling: tree-based models. Let’s work with two examples: one a regression model y ~ x and the other a classification model class ~ x. We’ll use these training data. x y class 1 2 A 2 5 B 3 1 A 4 3 A 5 8 B 6 5 B 7 4 A 8 6 B First, the regression model. The data can be split by \\(x\\) in 7 different ways: 1 vs 2:8 1:2 vs 3:8 and so on, up to 1:7 vs 8 For each of these 7 possible splits, find the mean \\(y\\) of the left and right groups, and calculate the sum of square residuals from the mean of the points in that each group. Add these to get a total sum of squares. We’ll use this to pick the “best” split. Then repeat for the left group and for the right group until the sum of square errors for the groups is zero. A tree! A branch point (fork? bifurcation?) divides the data under consideration into two branches based on a single explanatory variable. The division is chosen, by exhaustion, to decrease the deviance of the response variable. For regression, the model is a simple mean2 and the deviance is equivalent to the sum of squared residuals. For classification, the model is a vector of observed probabilities and the deviance is proportional to minus 2 times the log-likelihood of the response variable given those probabilities. Figure 12.1: ISLR Figure 8.8 from ISLR Branches are followed by additional branch points or by a single leaf. A leaf contains a single case3 from the original data (which, by necessity, cannot be further divided into cases). The process of branching is applied recursively until there is a leaf for every case in the original data. I call this a pure tree; the log likelihood is as large as possible: 0. Deviance (which amounts to sum of square residuals in a regression setting), is a value that results from summing over every case. The data cases and model values leading into each branch point produce a given deviance. Similarly, each of the branches stemming from the branch point has a deviance. The sum of the branches’ deviances is always less than the deviance of the input to the branch point. That reduction in deviance can be ascribed to the variable used in the branch point. By summing over all the branch points using a particular variable, the total reduction in deviance due to that variable can be calculated. This can be a useful measure of the importance of each variable in contributing to the “explanation” of the response variable. It’s analogous to the sum of squares reported in ANOVA for each model term. But, unlike ANOVA, there is no fixed order of which term comes first. (This is good in some ways and bad in others.) 12.1 Splitting Criteria for Classification Trees ISLR considers three measures to determine which of all the possible splits is best. Let \\(m\\) be a region and \\(k\\) a class. Also, let \\(\\hat{p}_{mk}\\) be the proportion of the training observations in region \\(m\\) of class \\(k\\). Finally, let \\(\\hat{n}_{mk}\\) be the count of cases of class \\(k\\) in region \\(m\\). Thus \\[\\hat{p}_{mk} \\equiv \\frac{\\hat{n}_{mk}}{\\sum_k \\hat{n}_{mk}}\\] Classification error rate: \\[E = 1 - \\max_k(\\hat{p}_{mk}).\\] This is the fraction of cases that were not in the most popular class. The book describes this as too coarse a measure to be effective for tree-growing. The “Gini index”: \\[G = \\sum_{k=1}^K \\hat{p}_{mk}(1 - \\hat{p}_mk)\\] The “cross entropy”: \\[D = - \\sum_{k=1}^K \\hat{p}_{mk}\\ln(\\hat{p}_{mk})\\] Note that (2) and (3) are very similar for small \\(\\hat{p}_{mk}\\), since a first-order Taylor expansion gives \\[\\ln(x) \\approx \\ln(x)|_{x = 1} + \\frac{d\\ln}{dx}|_{x = 1} (x - 1) = 0 + \\frac{1}{1}(x - 1) = x - 1\\] So \\(-\\ln(x)\\) which appears in \\(D\\) is \\(\\approx 1 - x\\). I find it easier to think about the deviance, or, more simply, the negative log likelihood (which is \\(-2\\) times the deviance). This is the sum of all cases of the negative log likelihood of each class, or \\[{\\cal L} \\equiv - \\sum_k \\left[\\hat{n}_{mk} \\ln( \\hat{p}_{mk})\\right]\\] This is \\(\\sum_k \\hat{n}_{mk}\\) times the cross entropy. I prefer \\({\\cal L}\\) this because it is an extensive quantity and so we can consider the whole as the sum of the parts. Example from physics: temperature is intensive, energy is extensive. Perform the same exercise as in constructing the model y ~ x but with class ~ x. Use \\({\\cal L}\\) as the figure of merit in determining which is the “best” split at each stage. 12.2 Variable importance The tree construction allows a simple measure of how important each variable is in the model. At each fork, there is one variable being split up. That splits results in a reduction of \\({\\cal L}\\) which can be ascribed to that variable. Across all of the forks, add up the \\({\\cal L}\\) reductions for each variable. That gives the relative importance of each variable. Another possibility would be to use an ANOVA type strategy of nested models, but the order of explanatory variables would, as in ANOVA on linear models, play a role in the results. Figure 12.2: ISLR Figure 8.9 from ISLR 12.3 Avoiding overfitting The model-fitting method we’ve explored will produce a model whose leaves have deviance of zero. Of course, the pure tree is likely to be an overfit. The out-of-sample prediction error is almost certain to be larger than the in-sample error. There are several ways to extend the process to produce better out-of-sample performance. Prune the tree. The controlling parameter is the number of leaves in the pruned tree. Average over bootstrap replications. Don’t prune. Shrink the tree to a very simple structure (e.g. 2-4 leaves) and fit successive trees to the residual. Heavy pruning for each tree. library(ISLR) library(tree) Heart &lt;- na.omit(read.csv(&quot;Heart.csv&quot;)) Heart$X &lt;- NULL mod1 &lt;- tree(ChestPain ~ ., data = Heart) in_sample &lt;- predict(mod1, data = Heart, type= &quot;class&quot;, split=TRUE) table(Heart$ChestPain, in_sample) ## in_sample ## asymptomatic nonanginal nontypical typical ## asymptomatic 122 13 3 4 ## nonanginal 30 42 8 3 ## nontypical 7 11 28 3 ## typical 5 4 1 13 12.4 Pruning prune1 &lt;- prune.misclass(mod1, best=8) in_sample &lt;- predict(prune1, data = Heart, type= &quot;class&quot;, split=TRUE) table(Heart$ChestPain, in_sample) ## in_sample ## asymptomatic nonanginal nontypical typical ## asymptomatic 125 14 0 3 ## nonanginal 30 47 0 6 ## nontypical 12 28 7 2 ## typical 8 6 0 9 12.5 Averaging Each tree produces sharp division points. The precise division point has high variance. So bootstrap to produce a cohort of trees and average them. library(randomForest) mod2 &lt;- randomForest(ChestPain ~ ., data = Heart, importance = TRUE) In random forests, we also “bootstrap” the available variables for each branching point. This produces trees that are uncorrelated with one another, thereby giving additional opportunities to create variance and average over it. mod3 &lt;- randomForest(ChestPain ~ ., data = Heart, mtry=5, importance = TRUE) 12.6 Shrinking (“Boosting”) library(gbm) ## Loading required package: survival ## Loading required package: parallel ## Loaded gbm 2.1.1 Heart$pain &lt;- ifelse(Heart$ChestPain == &quot;asymptomatic&quot;, 0, 1) mod4 &lt;- gbm(pain ~ ., data=Heart, distribution=&quot;bernoulli&quot;, n.trees=5000, interaction.depth = 4) Slow down the learning process to avoid the pitfalls of greedy optimization. This importance is analogous to the mean square Similarity to stepwise selection in linear regression. which is itself a maximum-likelihood estimator↩ or by multiple cases with the same value for the response variable↩ "],
["support-vector-classifiers.html", "Topic 13 Support Vector Classifiers 13.1 Lines, planes, and hyperplanes 13.2 Optimizing within the constraint 13.3 Allowing violations of the boundary 13.4 Nonlinear Boundaries 13.5 Support Vector Machine 13.6 Kernels 13.7 SVM versus logistic regression", " Topic 13 Support Vector Classifiers The methods we have studied so far use all the data to fit the model. But for classification, it may be that only the cases that are intermixed are telling for distinguishing one group from the other. The bulk of cases, clustered happily in their home country, may not be so useful. 13.1 Lines, planes, and hyperplanes \\(y = m x + b = \\beta_0 + \\beta_1 x\\) \\(z = a + b x + c y = \\beta_0 + \\beta_1 x + \\beta_2 y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\) \\(\\beta_1\\) and \\(\\beta_2\\) give the partial derivatives w.r.t. \\(x_1\\) and \\(x_2\\). But what if the plane is intended to be parallel to the z-axis? Partials are infinite. We would write the formula with \\(x_1\\) or \\(x_2\\) on the left-hand side. \\(0 = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\) \\(\\beta_0\\) sets the distance of the plane from the origin. \\(\\beta_1, \\beta_2, ...\\) set the orientation of the plane. Another perspective. \\(d\\) points in \\(d\\)-dimensional space define a hyperplane. The hyperplane is a linear combination of the \\(d-1\\) vectors \\(x_2 - x_1\\), \\(x_3 - x_1\\), \\(x_4 - x_1\\), … The unique normal to the place is the vector not in the \\(d-1\\) dimensional subspace. Plane 13.1.1 Rescaling X It’s the relative size of the \\(\\beta_j\\) that’s important. So let’s agree to scale them so that \\(\\sum_{j=1}^p \\beta_j^2 = 1\\). 13.1.2 Impose an absolute constraint Formalism: \\(y_i = 1\\) means class A \\(y_i = -1\\) means class B Constraints: \\(y_i ( \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}) &gt; 0\\) Says, “You must be on the right side of the boundary.” Alternative: \\(y_i ( \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}) &gt; M &gt; 0\\) Says, “You must be on the right side of the boundary and at least \\(M\\) units away from the boundary.” Like, “You must produce \\(M\\) units less than your permits allow, or less.” Figure 13.1: ISLR Figure 9.2 from ISLR There may be no feasible solution, or there may be many. This formulation provides no way to deal with the no-solution situation, and no way to choose the best of the many solutions. Figure 13.2: ISLR Figure 9.3 from ISLR 13.2 Optimizing within the constraint maximize over \\(\\beta_0\\), \\(\\beta_1\\), … the quantity \\(M\\) constraints that \\(y_i ( \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}) &gt; M\\) and remember that \\(\\sum_{j=1}^{p} \\beta_j^2 = 1\\). Says, “You must be on the right side of the boundary and as far from it as possible.” \\(M\\) measures how far from boundary. Figure 13.3: ISLR Figure 9.5 from ISLR 13.3 Allowing violations of the boundary maximize over \\(\\beta_0\\), \\(\\beta_1\\), … the quantity \\(M\\) constraints that \\(y_i ( \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}) &gt; M (1 - \\epsilon_i)\\) \\(\\epsilon_i \\geq 0\\) — Once you’re on the good side of the margin, you don’t get any credit that could be used to increase \\(M\\)! \\(\\sum_{i=1}^n \\epsilon_i &lt; C\\) — but let’s keep the epsilons small. We’re willing to trade off between epsilons. \\(C\\) is the budget for violations, a tuning parameter. For a given \\(C\\), there’s no guarantee that we can meet the constraints. (That is, no guarantee that there’s a feasible solution.) But there will be some \\(C\\) for which there is a feasible solution. and remember that \\(\\sum_{j=1}^{p} \\beta_j^2 = 1\\). Figure 13.4: ISLR Figure 9.6 from ISLR FIGURE 9.6. Left: A support vector classifier was fit to a small data set. The hyperplane is shown as a solid line and the margins are shown as dashed lines. Purple observations: Observations 3,4,5, and 6 are on the correct side of the margin, observation 2 is on the margin, and observation 1 is on the wrong side of the margin. Blue observations: Observations 7 and 10 are on the correct side of the margin, observation 9 is on the margin, and observation 8 is on the wrong side of the margin. No observations are on the wrong side of the hyperplane. Right: Same as left panel with two additional points, 11 and 12. These two observations are on the wrong side of the hyperplane and the wrong side of the margin. Figure 13.5: ISLR Figure 9.7 from ISLR Figure 9.7 from ISL. A support vector classifier was fit using four different values of the tuning parameter \\(C\\) in (9.12)–(9.15). The largest value of \\(C\\) was used in the top left panel, and smaller values were used in the top right, bottom left, and bottom right panels. When \\(C\\) is large, then there is a high tolerance for observations being on the wrong side of the margin, and so the margin will be large. As \\(C\\) decreases, the tolerance for observations being on the wrong side of the margin decreases, and the margin narrows. 13.4 Nonlinear Boundaries The support vector “classifier” builds a boundary based on a linear combination of linear function. One can add in nonlinear transformations of the predictors as well as interactions. This increases the dimension of the predictor space. The resulting boundary will still be linear in the values of the predictors, but when considered in the original predictor space, can be nonlinear. Example: A linear function of \\(x\\), \\(y\\), \\(x^2\\), \\(y^2\\), and \\(xy\\) require(mosaic) f &lt;- function(x,y){3*x - 2*y + x^2 + 2*y^2 - x*y} plotFun(f(x,y) ~ x &amp; y) Let’s consider \\(f(x,y)\\) to be the classifier function. \\(f(x,y) = 0\\) is the boundary between positive and negative predictions. 13.5 Support Vector Machine Consider a classification function of this form: \\(f(x) = \\beta_0 + \\sum_{i=1}^{n} \\alpha_i &lt; x, x_i &gt;\\) There are \\(n\\) training cases \\(x_i\\). The physicists will recognize \\(&lt;x, y&gt;\\) as bra-ket notation for an inner product. In a support vector machine, only those cases that are on or across the margin will contribute. \\(\\alpha_i = 0\\) for any \\(x_i\\) on the proper side of the boundary. 13.6 Kernels The function \\(&lt; x, x_i &gt;\\) could be replaced with any other function of \\(x\\) and \\(x_i\\). Such functions are called kernels. Explanation of kernels in terms of: high and low-pass filters point-spread functions for images Examples: Remember: \\(i\\) refers to the case. \\(j\\) refers to the variable. Polynomial kernel \\[K(x, x_i) = (1 + \\sum_{j=1}^p x_{ij} x_{i&#39;j})^d\\] (Bad notation when it depends on a superscript to a subscript!) Radial kernel: \\[K(x, x_i) = \\exp\\left( - \\gamma \\sum_{j=1}^p (x_{ij} - x_{i&#39;j}) \\right)\\] Figure 13.6: Figure 9.9 from ISL. Left: A SVM with a polynomial kernel of degree 3. Right: Using a radial kernel. load(&quot;Blood-Cell-data.rda&quot;) training_inds &lt;- sample(1:nrow(Cells1), size=5000) Training &lt;- Cells1[training_inds,] Testing &lt;- Cells1[ - training_inds,] ggplot(Training, aes(x=x1, y=x2, color=class)) + geom_point() library(e1071) mod &lt;- svm(as.factor(class) ~ x1 + x2, data = Training, kernel = &quot;radial&quot;, cost = 10) In sample error: preds &lt;- predict(mod, newdata = Training) table(Training$class, preds) ## preds ## A B C D F ## A 697 9 0 0 16 ## B 9 337 46 0 0 ## C 0 21 3439 5 0 ## D 0 0 52 83 1 ## F 37 1 0 0 247 GotWrong &lt;- Training %&gt;% filter(preds != class) ggplot(Training, aes(x=x1, y=x2, color=class)) + geom_point() + geom_point(data=GotWrong, aes(x=x1, y=x2, color=NA, shape=class)) Out-of-sample error: preds &lt;- predict(mod, newdata = Testing) table(Testing$class, preds) ## preds ## A B C D F ## A 4430 56 0 0 84 ## B 89 2067 299 0 11 ## C 0 118 20769 48 3 ## D 0 0 279 438 2 ## F 254 7 10 14 1555 GotWrong &lt;- Testing %&gt;% filter(preds != class) ggplot(Testing, aes(x=x1, y=x2, color=class)) + geom_point(alpha = .1) + geom_point(data=GotWrong, aes(x=x1, y=x2, color=NA, shape=class)) What could we do to improve things? Is there a loss function? Bayesian prior? Optimize parameters (e.g. cost, radial kernel \\(\\gamma\\)) by cross validation Build specialized classifiers to resolve particular sources of mis-classification, e.g. D versus C. Actual problem: Build a classifier that they can implement in a small amount of code that will work in real time, and where they can demonstrate to regulators that the code does what it says it does. In other words, everything must be done from scratch with simple mathematics: addition, multiplication, exponentiation. They asked for 80% accuracy for distinguishing certain pairs of cell types. I got &gt; 90% out of the box. At this point, I presented my results rather than trying to optimize the classifier parameters. I wanted to make sure that what I was giving them a solution to their problem rather than to the problem as I conceived it. Is there a clinical loss function? How will the results of the classification be used? Then they asked for 95% accuracy. This wasn’t hard to accomplish by some optimization of parameters Then they gave me some “bad runs,” where the human “gold-standard” classifier had a hard time. Performace was worse on these. mis-alignment of optics? Patient-specific properties of cells? non-Bayesian classification by human “gold standard?” Building a model of human mis-classification is too hard a problem for me. Instead, I gave them a classifier for whether a given sample was “reliable” or “unreliable.” With this you can ring a bell for unreliable runs and have it done again. 13.7 SVM versus logistic regression Recall the shape of the logistic function that transforms the link function value to likelihood. For points far from the transition, a change in the function has practically no impact on the likelihood function. Thus, logistic regression also discounts “mainstream” cases. Could use radial or polynomial kernels in logistic regression as well, but we’d have to decide where to place them. Figure 13.7: Figure 9.12 from ISL: SVM and logistic loss functions. SVM has zero loss on the correct side of the boundary: it’s harder than logistic. Machine learning technique like bagging, boosting, and random forests are designed to soften the boundaries of tree-based models. "],
["programming-basics.html", "Topic 14 Programming Basics 14.1 Programming Basics I: Names, classes, and objects {progbasics1} 14.2 Programming basics: Linear Models 14.3 K-nearest neighbors 14.4 Loops/Iteration 14.5 Parts of a loop 14.6 Trivial examples 14.7 Bootstrapping 14.8 Leave-one-out cross-validation. 14.9 Building a package", " Topic 14 Programming Basics 14.1 Programming Basics I: Names, classes, and objects {progbasics1} 14.1.1 Names Composed of letters, numbers, _ and .. - Don’t use . — it’s a bad habit. But plenty of people do. - Can’t lead with a number. - Capitalization counts. - Unquoted (… almost always) 14.1.2 Objects Information (bits) in a particular format. - Different formats for different purposes. - The format is the class() or mode(). mode is more basic than class. Assignment: Give a name to an object 14.1.3 Vectors 1-dimensional homogeneous collections of numbers, character strings, booleans/logicals, etc. 14.1.3.1 Must Know! There are some basic types of vectors. The most common are: numeric, e.g. 3, 3.14159, 6.023e26,6.626196e-34 character, e.g. &quot;hello&quot;, `“When in the course of human events …” logical (or “booleans”). The only allowable values: TRUE and FALSE Much of the software you’ll use in this course will work with an alternative to character strings called “factors.” factor, an encoded representation of levels of a categorical variable. Some operations you will use when dealing with categorical variables are as.character() and (for older software) as.factor(). Vectors are “1-dimensional” collections. That is, you need only one index to refer to a specific element. my_vector &lt;- c(&quot;apple&quot;, &quot;berry&quot;, &quot;cherry&quot;) my_vector[2] ## [1] &quot;berry&quot; my_vector[c(3, 1, 2, 2)] ## [1] &quot;cherry&quot; &quot;apple&quot; &quot;berry&quot; &quot;berry&quot; my_vector[4] &lt;- &quot;durian&quot; Boolean indexing my_vector[my_vector &gt; &quot;b&quot;] ## [1] &quot;berry&quot; &quot;cherry&quot; &quot;durian&quot; If you want, you can convert the boolean style to a number style with which(). Other important functions: length(), to say how many elements there are in the vector. The length can be zero. Arithmetic operations, e.g. sum(), mean(), max(), cumsum(), and other functions (e.g. sqrt(), log(), sin()), … Logical operations, that is, operations that transform vectors into a logical vector. Examples: Comparison: &gt;, ==, &gt;=, !=, &lt;, &lt;= Boolean operations: !, |, &amp;. (Note, these are single characters.) Categorical operations, e.g. table() funny &lt;- ceiling(length(my_vector)*runif(10)) my_vector[funny] ## [1] &quot;durian&quot; &quot;apple&quot; &quot;apple&quot; &quot;berry&quot; &quot;berry&quot; &quot;berry&quot; &quot;durian&quot; &quot;apple&quot; &quot;durian&quot; &quot;durian&quot; length(my_vector) ## [1] 4 dim(my_vector) ## NULL 14.1.4 Matrices 2-dimensional homogeneous collections of numbers or of character strings. These collections are called matrices 2-dimensional means you need two indices to refer to a specific element. dim() Operations, e.g., t(), colSums(), rowSums(), %*%, … Index matrices with [ , ]. You need to give two 14.1.5 Lists 1-dimensional heterogeneous collections create with list() my_list &lt;- list(magic = 1:10, greeting = &quot;hello&quot;, is_wednesday = FALSE) my_list[c(&quot;greeting&quot;, &quot;magic&quot;, &quot;magic&quot;)] ## $greeting ## [1] &quot;hello&quot; ## ## $magic ## [1] 1 2 3 4 5 6 7 8 9 10 ## ## $magic ## [1] 1 2 3 4 5 6 7 8 9 10 my_list$magic ## [1] 1 2 3 4 5 6 7 8 9 10 index with [[ ]] or [ ]. The first is for an individual item which you want in the form of that item itself. The second is to create a subset of the list which will be a list itself. you can name items in lists and refer to them by name. [[&quot;name&quot;]] To set names of list items, use named arguments to list() or use the names() function on the left-hand side of the &lt;- operation. (The left-hand side of &lt;- is called an “assignable.” These can be names, but they can be other things as well such as indexed arrays, names(), etc.) Data frames A list of vectors. Each component in a given vector must be the same kind of thing as the other components. (Special cases: NA for missing data. Two more special things for numerical data.NaN, &amp; Inf) Important functions: nrow(), names(), $. 14.1.6 Functions Take inputs and produce an output (and maybe a side-effect). - Make them with function(){ }, a special form. (It’s not actually a function!) - Try this class(sin) class(3) class(&quot;a string&quot;) class(function) ## Error: &lt;text&gt;:4:15: unexpected &#39;)&#39; ## 3: class(&quot;a string&quot;) ## 4: class(function) ## ^ 14.2 Programming basics: Linear Models Syntactic element: formulas. Formulas provide a way of using variables “symbolically.” This is useful, for instance, in depicting the desired relationship among variables. Two forms: y ~ x two-sided ~ x one-sided NOT ALLOWED, y ~ Important functions: lm(), predict() , anova(), summary(). For later: solve(), model.matrix(). From 155: coef(), fitted(), resid() Training with lm(): Specify formula Y ~ X1 + X2 … data(College, package = &quot;ISLR&quot;) mod &lt;- lm(Outstate ~ Enroll + Accept + perc.alumni, data = College ) coef(mod) ## (Intercept) Enroll Accept perc.alumni ## 6387.368606 -2.888077 1.101019 179.528731 What kind of thing is mod? Model output with predict() predict(mod, newdata = data.frame(Enroll = 100, Accept = 1000, perc.alumni = 25)) ## 1 ## 11687.8 What kind of thing is the output of predict()? predict(mod, interval = &quot;confidence&quot;, newdata = data.frame(Enroll = 100, Accept = 1000, perc.alumni = 25)) ## fit lwr upr ## 1 11687.8 11383.57 11992.03 predict(mod, interval = &quot;prediction&quot;, newdata = data.frame(Enroll = 100, Accept = 1000, perc.alumni = 25)) ## fit lwr upr ## 1 11687.8 5548.956 17826.64 Why is the “confidence interval” so much narrower than the “prediction interval?” Inference with anova() and summary(). This is all “in-sample” inference, not cross-validated. M &lt;- model.matrix(~ Enroll + Accept * perc.alumni, data = College) qr.solve(M, College$Outstate) ## (Intercept) Enroll Accept perc.alumni Accept:perc.alumni ## 7024.34843865 -3.00377409 0.78401053 147.31268325 0.02011475 For the people who have had linear algebra, why doesn’t this work? solve(M, College$Outstate) ## Error in solve.default(M, College$Outstate): &#39;a&#39; (777 x 5) must be square Indexing on data: training and testing data sets 14.2.1 Graphics basics API for graphics: plot(), points(), lines(), polygon(), text(), … 14.3 K-nearest neighbors K-nearest neighbors is a simple, general kind of function-building method. But some problems: Interpretability: but you can always take partial derivatives. When you have prediction (aka “explanatory”) variables in dollars and in miles, how do you calculate the distance between points? What are the dimensions of distance? Dimensionality refers to the physical feature, e.g. time, distance, area, volume, money, charge, luminance, mass, … Units are the ways in which dimensions are measured, e.g., cups, gallons, liters … all refer to volume Give some examples of units for each of the dimensions. Some everyday quantities are dimensionless, e.g. pure numbers. Give some examples: … (angles, percent, fractions, … but not ratios in general.) Regression fixes units automatically, since the coefficients themselves have dimensionality. They will adjust automatically to changes in units, so the model is the same regardless of whether we use miles, km, parsecs, … In KNN, to avoid dependence on units, need to do some standardization by dividing by something in the same units, e.g. sd. Curse of dimensionality. Let’s create 1000 randomly placed points in the unit square: rpts &lt;- matrix(runif(2*1000), ncol=2) What’s the distribution of distances from a single random point to the 1000 others: our_point &lt;- runif(2) The distance between our point and each of the others tmp &lt;- matrix(our_point, ncol=2, nrow=1000, byrow=TRUE) delta &lt;- sqrt(rowSums((rpts - tmp)^2)) How far away is a typical point? Write a function that takes the matrix of points and the “our point” and finds the distance from our point to each and every one of the points in the matrix. How far away is a typical point in 1-dimensional space? In 10-dimensional space? In 100-dimensional space? 14.4 Loops/Iteration Loops are the programming control structure that allows you to repeat the same commands many times. A definition of insanity: Doing something over and over again and expecting a different result. 14.5 Parts of a loop Preparation — creating a place to hold the results This is called the “accumulator.” Identify a set to loop over. Inside the loop, modify the accumulator When the loop is done, package up the results. 14.6 Trivial examples Find the sum of squares of a vector. (R sum(x^2)) Find the biggest element of a vector. (R max()) Generate \\(k\\) random numbers from the set 1:n (R sample()) with replacement: pre-allocate result, loop and select without replacement: shrink the set of possibilities each time. Find the \\(k\\)th Fibonacci number: with previous and before_that as the state with an array as the state: initialize to c(1, 1) with a global array as the state memoization. with an array created in the capturing environment. **In practice, we would use the already parallelized R functions. See also Vectorize(). 14.7 Bootstrapping Process Set up accumulator — what should we store? (all coefs) but we don’t know what this should look like until we’ve tried it out Loop: create a new random sample fit the model store away the results Post-process: Give std-err? Give covariance matrix? Give array? 14.8 Leave-one-out cross-validation. # preparation my_data &lt;- mosaicData::KidsFeet error &lt;- numeric(nrow(my_data)) # The looping set: each row in my_data for (k in 1:nrow(my_data)) { # the body of the loop mod &lt;- lm(width ~ length * sex, data = my_data[ -k, ]) mod_value &lt;- predict(mod, newdata = my_data[k, ]) error[k] &lt;- my_data$width[k] - mod_value } # packaging up the results result &lt;- sum(error^2) Look at the result: result ## [1] 6.304381 regular_model &lt;- lm(width ~ length * sex, data = my_data) sum(resid(regular_model)^2) ## [1] 5.329327 anova(regular_model) ## Analysis of Variance Table ## ## Response: width ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## length 1 4.0557 4.0557 26.6353 9.86e-06 *** ## sex 1 0.4790 0.4790 3.1456 0.08484 . ## length:sex 1 0.0037 0.0037 0.0246 0.87638 ## Residuals 35 5.3293 0.1523 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In ANOVA, we use a degrees of freedom to adjust for the under-estimate of residuals. sum(resid(regular_model)^2) / 35 ## [1] 0.1522665 In leave-one-out, we can simply average the errors: result / 38 ## [1] 0.1659048 14.9 Building a package Divide up into groups of two or three. Open a new project: choose “new directory”, “choose R package” Go to the “Build” tab, select More/Configure Build Tools and fill in the checkmark for Roxygen comments. Write functions for \\(C_p\\), \\(AIC\\), \\(BIC\\), and adjusted \\(R^2\\). The function should take a model as input. Document the functions. Compile the package. "],
["appendices.html", "Appendices", " Appendices "],
["connecting-rstudio-to-your-github-repository.html", "Connecting RStudio to your GitHub repository 14.10 Setting up RStudio 14.11 Setting up your Math 253 repository 14.12 Using your repository 14.13 Why are we doing this?", " Connecting RStudio to your GitHub repository 14.10 Setting up RStudio It’s likely that most students in the class will work with the Macalester RStudio server which you access at rstudio.macalester.edu. You can login with your usual Macalester email credentials. The server already has all the necessary software installed. (If you are a junior or senior, you may have an account on another server named www.macalester.edu/rstudio. Do not use that server. It’s obsolete.) Some students may wish to set up RStudio on their own laptop. This is a three step process: Install the latest version of R from this site Install the RStudio Preview Edition Install the git software. Git is open-source software for managing complex sets of documents. In contrast, GitHub, is a web service that provides a place for you to store, backup, and distribute your work. You use git commands for communicating with GitHub. 14.11 Setting up your Math 253 repository First, we have to set up communications between your RStudio system and GitHub. You need only do this once on each RStudio system. But if you want to work on, say, a new computer or new RStudio server, you’ll need to do it again on that computer. Make sure you have a GitHub account. You’ll need two pieces of information about your account: Your GitHub ID. For the instructions, I’ll call in brosenberg, but remember to use your own ID The email address you gave when you set up your account. I’ll use brosenberg@macalester.edu for the example. Start up RStudio and do the following: In the console, give these commands appropriately customized for your situation: system(&#39;git config --global user.name &quot;Brian MacAlistair&quot;&#39;) system(&#39;git config --global user.email &quot;brosenberg@macalester.edu&quot;&#39;) system(&#39;git config --global --list&#39;) Select the menu item Tools/Global Options/Git-SVN. You will see a button to “Create RSA key …”. If the box above that button is empty, press the button. Otherwise continue on …. Click “View public key” and copy the resulting displayed text to your clipboard. Go to your GitHub account. It will have an address like github.com/brosenberg. Press the “Edit Profile” button, then select “SHS and GPG keys.” Press the “New SSH key” button. Two text boxes will appear. In the smaller one, insert some description of your RStudio system, e.g. rstudio.macalester.edu or my own laptop. In the larger one, paste the text you copied to your keyboard in step 2c. Almost done … A GitHub repository has been set up for you. It will has a URL in this form: &lt;github.com/dtkaplan/math253-brosenberg&gt;. If that doesn’t work, ask for help. There might have been some mistake or delay in setting up your repository, and only dtkaplan can fix things. Direct your browser to the address of your repository. On the right side of the page, there is a green button: “Clone or download”. Press it. A small dialog panel will appear. It should say, “Clone with SSH”. (If not, press the small “Use SSH” link to the right.) Click on the small clipboard icon. This will copy an address to your clipboard. The address starts with git@github.com. Go back to your RStudio system. Select File/New Project/Version Control/GIT. A dialog box will appear. Paste the address from 4c into the topmost text-entry widget in the dialog box. Press “Create project.” Some stuff will happen. Ultimately, you should see RStudio restart and the name of your repository (e.g. math253-brosenberg) will be on the upper right corner of the RStudio window. (If you are using the server version of RStudio, you might have to refresh your browser to see the change.) 14.12 Using your repository You will be modifying files that are already in your repository, for instance 01-Programming.Rmd. Make sure that the project displayed in your RStudio is the right one, e.g. math253-brosenberg Open, edit, debug, and revise the files you are working on in the normal way. You should knit the file to HTML frequently. This helps you spot problems early. You’ll mainly be working with .Rmd files. When you’re satisfied with things, knit the file to produce HTML one final time. Ready to “hand in” your work? Go to the “Git” tab in RStudio. You should see at least two files listed: the file you edited and the corresponding HTML file. “Stage” the files by checking the little boxes. “Commit” the files by pressing the “commit” button. You will be asked to write a message. Choose something short and informative, e.g. “Day 1 project submission.” A dialog box will appear with some text indicating what was in the commit. Press the “Pull” arrow. Press the “Push” arrow. Assuming that no errors appear, you are all done! Well … actually you’re never “all done.” You will often want to revise your work. Follow the same steps as above. It is not cheating to revise your work, even after you hand it in. I encourage you to do this, be it the next day or several weeks after you handed it in. Git keeps a detailed history of all your commits, so I can always find any version of the file that I need. 14.13 Why are we doing this? We speak of “handing in” assignments, projects, and other work for a course. But just as we no longer “dial” a telephone, there is no longer a hand involved in handing in. With course support systems like Moodle, you “hand in” by uploading to a server a copy of the file containing your work. You have a working copy of your work and, when you’re finished with your work, you upload a final copy. Making a copy has implications that can get in the way of carrying out your work. The problem is that there is usually nothing in the copy that indicates unambiguously where it comes from. If you want to revise the document, should you be revising the working document or the final copy? The word “final” suggests the traditional approach to this question. Once the work is done and the document is finalized, you no longer make any changes. It turns out that “final” does not accord well with the way that people work in collaboration. For all the time that people work with a document, there is a continuing process of revision and refinement. You may think something is final, but in a collaboration that’s not entirely your call; you may find out that it isn’t yet finished. Insofar as your document contains web links to other documents, the situation becomes even more complex, since those other documents may change. An extreme instance of this lack of finality appears in software development. Software is complex and bugs are apt to be discovered even after the “final” release. Any piece software written by you or your team relies on and communicates with other software. Changes in that other software can imply a need to revise your own software. We’ve become familiar with the idea of versions of software: R 3.3.2, Word 2016, etc. There are advantages in thinking about documents in the same way we think about software.4 Or, put another way, there are advantages to thinking about working with documents using the same tools that computer programmers use to manage their own complex collaborations with other programmers and with other software. That’s what we’re going to do. There are two reasons: (1) it provides a superior way of managing communications and (2) it is the way things are heading in general, so the skills and concepts you develop will help you in your future work and career. To outline the components of the process … You are going to be writting documents using the .Rmd (R/Markdown) format. You’ll use this even if there happens not to be any R content in the document. Each document that you create as part of your work for this course will be located in an RStudio “project.” A project is a means of grouping together related files. The project and the files within it will be located on your own computer. This might be your own laptop or a server. Your project will be cloned to storage on other computers. In particular, there will be a clone on the instructor’s computer and another clone on a server in the cloud. The cloud server is maintained by GitHub.com. This is not the only such server, but it is the one we are going to use. Both GitHub.com and your computer (and any other computers the project is cloned on) communicate with a system called “git.” The git system provides facilities for keeping a thorough record of the changes you make in files contained in your project. You control how fine-grained you want this record to be. The means for doing this is to take a snapshot of the current state of your project. Such a snapshot is called a “commit.” You have control of which of the documents will have changes recorded in the history; directing git to include the changes to a particular file (or even its creation) is called “staging” the file. At times of your own choice, you can synchronize/update the clone of the web server to the most recent commit of your project. This is called “pushing” the commit. Also at times of your choosing, you can synchronize/update your own clone of the project to the one on the GitHub server. This is called “pulling” a commit. Pulling and pushing are the means by which you collaborate with others on the project. Just as you will push your changes to the server, others will be pushing their own changes. You pull to synchronize your clone of the project with the changes that have been pushed by others. In a typical work session, you will commit the current state of your project, then pull from the server, make the revisions you want in your clone. commit as often as you like within a session. Committing provides you with a marker. If something went wrong and you want to undo your revisions, it’s very easy to do so to the point where you last made a commit. Some people commit every paragraph. You decide. It takes very little time and costs nothing. If your file is intended to be translated to HTML, make sure to do this and to stage and commit the resulting version of the HTML file. When you are done with that session’s revisions, you — wait for it! — once again pull from the server. This lets you capture any changes that others made while you were revising your own clone of the project. It’s important because, sometimes, your changes will conflict with those made by others and pulling gives git a chance to bring any such conflicts to your attention. (Git will not let you push until all such conflicts have been resolved.) Finally, you push to the server, so that the clone on the server is synchronized to the changes you have made. At the instant after your push, you are now ready to continue at step (c). If you recommence work after enough time has elapsed that someone else might have pushed their own clone to the server, you should instead continue at step (a). You don’t need to be anxious about this; git will identify any conflicts that might have arisen due to another collaborator’s commits after step (f). It’s up to you to decide how often you should push. Pushing serves two purposes: It “backs up” your work to the cloud, so you won’t lose anything done up to the point of the push. It allows your collaborators to work with your revisions. You will “hand in” your work by Staging any files involved in the work. Committing the staged files. You might want to identify the commit with a message like “Handing in Assignment 2.” Pushing to the server. You’re work will be marked as “handed in” the moment you commit. But the instructor will only be able to see the files you’ve handed in after you push to the cloud server. If you decide to revise your work, simply revise, stage, commit and push again. The instructor will see the new version and will also have access to the earlier versions along with the time stamp made when they were committed. When in doubt about a deadline, push your work. You can always revise and push again if you discover that you have additional time until the deadline. What’s more, you can revise and push again after the deadline. Your instructor will be able to sort out which is the version to use for grading purposes. It is not cheating to revise after the deadline, because the instructor always knows which versions were submitted before the deadline. Indeed, I encourage you to revise your work even after it’s been submitted. That way you will have a copy that reflects your current understanding. By the way, the web site for this course is managed in exactly the same way. The only difference is that whatever clone the server has is made available to web browsers. Vocabulary: You should be able to give an accurate definition of each of these words, and say how they are connected to one another: git, GitHub, clone, stage, commit, push, pull, conflict DT Kaplan, Tue Feb 21 10:14:53 2017 Documents on a computer are always software: instructions to some display system about what image to paint on the computer screen or paper. To paraphrase the artist Magritte, “Ceçi n’est pas un document.” What you write is the software to create a document, just as Magritte’s famous painting of a pipe is a painting, not a pipe.↩ "],
["instructions-for-the-publishing-system-bookdown.html", "Instructions for the publishing system: Bookdown", " Instructions for the publishing system: Bookdown You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter ??. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 14.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 14.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 14.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 14.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2016) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). "]
]
